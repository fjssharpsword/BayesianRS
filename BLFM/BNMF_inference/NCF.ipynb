{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR@10:0.679635761589404, NDCG@10:0.4016749363446544, K:8\n",
      "HR@10:0.677317880794702, NDCG@10:0.4046946690581572, K:16\n",
      "HR@10:0.6855960264900662, NDCG@10:0.4137875869190139, K:32\n",
      "HR@10:0.6844370860927153, NDCG@10:0.40809027846034607, K:64\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8  \n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.08.02\n",
    "@function: baseline: NCF(Neural Collaborative Filtering)\n",
    "           paper: https://arxiv.org/pdf/1708.05031.pdf\n",
    "           Datatset: MovieLens-1m:https://grouplens.org/datasets/movielens/  \n",
    "           Evaluation: HR@10 NDCG@10\n",
    "'''\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import heapq\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "class NCFData(torch.utils.data.Dataset):#define the dataset\n",
    "    def __init__(self, features, num_item, train_mat=None, num_ng=0, is_training=None):\n",
    "        super(NCFData, self).__init__()\n",
    "        # Note that the labels are only useful when training, we thus add them in the ng_sample() function.\n",
    "        self.features_ps = features\n",
    "        self.num_item = num_item\n",
    "        self.train_mat = train_mat\n",
    "        self.num_ng = num_ng\n",
    "        self.is_training = is_training\n",
    "        self.labels = [0 for _ in range(len(features))]\n",
    "\n",
    "    def ng_sample(self):\n",
    "        assert self.is_training, 'no need to sampling when testing'\n",
    "        self.features_ng = []\n",
    "        for x in self.features_ps:\n",
    "            u = x[0]\n",
    "            for t in range(self.num_ng):\n",
    "                j = np.random.randint(self.num_item)\n",
    "                while (u, j) in self.train_mat:\n",
    "                    j = np.random.randint(self.num_item)\n",
    "                self.features_ng.append([u, j])\n",
    "        \n",
    "        labels_ps = [1 for _ in range(len(self.features_ps))]\n",
    "        labels_ng = [0 for _ in range(len(self.features_ng))]\n",
    "        \n",
    "        self.features_fill = self.features_ps + self.features_ng\n",
    "        self.labels_fill = labels_ps + labels_ng\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.num_ng + 1) * len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        if self.is_training:\n",
    "            self.ng_sample()\n",
    "            features = self.features_fill\n",
    "            labels = self.labels_fill\n",
    "        else:\n",
    "            features = self.features_ps\n",
    "            labels = self.labels\n",
    "        '''\n",
    "        features = self.features_fill if self.is_training else self.features_ps\n",
    "        labels = self.labels_fill if self.is_training else self.labels\n",
    "        \n",
    "        user = features[idx][0]\n",
    "        item = features[idx][1]\n",
    "        label = labels[idx]\n",
    "        return user, item ,label\n",
    "\n",
    "#define the NCF model, integrating the GMF and MLP\n",
    "class NCF(nn.Module):\n",
    "    def __init__(self, user_num, item_num, factor_num, num_layers, dropout):\n",
    "        super(NCF, self).__init__()\n",
    "        \"\"\"\n",
    "        user_num: number of users;\n",
    "        item_num: number of items;\n",
    "        factor_num: number of predictive factors;\n",
    "        num_layers: the number of layers in MLP model;\n",
    "        dropout: dropout rate between fully connected layers;\n",
    "        \"\"\"\n",
    "        self.embed_user_GMF = nn.Embedding(user_num, factor_num)\n",
    "        self.embed_item_GMF = nn.Embedding(item_num, factor_num)\n",
    "        self.embed_user_MLP = nn.Embedding(user_num, factor_num * (2 ** (num_layers - 1)))\n",
    "        self.embed_item_MLP = nn.Embedding(item_num, factor_num * (2 ** (num_layers - 1)))\n",
    "\n",
    "        MLP_modules = []\n",
    "        for i in range(num_layers):\n",
    "            input_size = factor_num * (2 ** (num_layers - i))\n",
    "            MLP_modules.append(nn.Dropout(p=dropout))\n",
    "            MLP_modules.append(nn.Linear(input_size, input_size//2))\n",
    "            MLP_modules.append(nn.ReLU())\n",
    "        self.MLP_layers = nn.Sequential(*MLP_modules)\n",
    "        \n",
    "        self.predict_layer = nn.Linear(factor_num * 2, 1)\n",
    "        \n",
    "        self._init_weight_()\n",
    "        \n",
    "    def _init_weight_(self):\n",
    "        \"\"\" We leave the weights initialization here. \"\"\"\n",
    "        nn.init.normal_(self.embed_user_GMF.weight, std=0.01)\n",
    "        nn.init.normal_(self.embed_user_MLP.weight, std=0.01)\n",
    "        nn.init.normal_(self.embed_item_GMF.weight, std=0.01)\n",
    "        nn.init.normal_(self.embed_item_MLP.weight, std=0.01)\n",
    "        \n",
    "        for m in self.MLP_layers:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.kaiming_uniform_(self.predict_layer.weight, a=1, nonlinearity='sigmoid')\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        \n",
    "        embed_user_GMF = self.embed_user_GMF(user)\n",
    "        embed_item_GMF = self.embed_item_GMF(item)\n",
    "        output_GMF = embed_user_GMF * embed_item_GMF\n",
    "        \n",
    "        embed_user_MLP = self.embed_user_MLP(user)\n",
    "        embed_item_MLP = self.embed_item_MLP(item)\n",
    "        interaction = torch.cat((embed_user_MLP, embed_item_MLP), -1)\n",
    "        output_MLP = self.MLP_layers(interaction)\n",
    "\n",
    "        concat = torch.cat((output_GMF, output_MLP), -1)\n",
    "\n",
    "        prediction = self.predict_layer(concat)\n",
    "        return prediction.view(-1)\n",
    "    \n",
    "#loading dataset function\n",
    "def load_dataset(test_num=100):\n",
    "    train_data = pd.read_csv(\"/data/fjsdata/BNMF/ml-1m.train.rating\", \\\n",
    "                             sep='\\t', header=None, names=['user', 'item'], \\\n",
    "                             usecols=[0, 1], dtype={0: np.int32, 1: np.int32})\n",
    "\n",
    "    user_num = train_data['user'].max() + 1\n",
    "    item_num = train_data['item'].max() + 1\n",
    "    \n",
    "    train_data = train_data.values.tolist()\n",
    "    \n",
    "    # load ratings as a dok matrix\n",
    "    train_mat = sp.dok_matrix((user_num, item_num), dtype=np.float32)\n",
    "    for x in train_data:\n",
    "        train_mat[x[0], x[1]] = 1.0\n",
    "\n",
    "    test_data = []\n",
    "    with open(\"/data/fjsdata/BNMF/ml-1m.test.negative\", 'r') as fd:\n",
    "        line = fd.readline()\n",
    "        while line != None and line != '':\n",
    "            arr = line.split('\\t')\n",
    "            u = eval(arr[0])[0]\n",
    "            test_data.append([u, eval(arr[0])[1]])#one postive item\n",
    "            for i in arr[1:]:\n",
    "                test_data.append([u, int(i)]) #99 negative items\n",
    "            line = fd.readline()\n",
    "    return train_data, test_data, user_num, item_num, train_mat\n",
    "\n",
    "#evaluate function\n",
    "def hit(gt_item, pred_items):\n",
    "    if gt_item in pred_items:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def ndcg(gt_item, pred_items):\n",
    "    if gt_item in pred_items:\n",
    "        index = pred_items.index(gt_item)\n",
    "        return np.reciprocal(np.log2(index+2))\n",
    "    return 0\n",
    "\n",
    "\n",
    "def metrics(model, test_loader, top_k):\n",
    "    HR, NDCG = [], []\n",
    "    \n",
    "    for user, item, label in test_loader:\n",
    "        user = user.cuda()\n",
    "        item = item.cuda()\n",
    "        \n",
    "        predictions = model(user, item)\n",
    "        _, indices = torch.topk(predictions, top_k)\n",
    "        recommends = torch.take(item, indices).cpu().numpy().tolist()\n",
    "        \n",
    "        gt_item = item[0].item()\n",
    "        HR.append(hit(gt_item, recommends))\n",
    "        NDCG.append(ndcg(gt_item, recommends))\n",
    "    return np.mean(HR), np.mean(NDCG)\n",
    "\n",
    "#Setting GPU Enviroment\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\" #using gpu \n",
    "cudnn.benchmark = True\n",
    "# construct the train and test datasets\n",
    "train_data, test_data, user_num ,item_num, train_mat = load_dataset()\n",
    "train_dataset = NCFData(train_data, item_num, train_mat, num_ng=2, is_training=True)#pos:neg=1:2\n",
    "test_dataset = NCFData(test_data, item_num, train_mat, num_ng=0, is_training=False)#100\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4)\n",
    "#every user have 99 negative items and one positive items，so batch_size=100\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=99+1, shuffle=False, num_workers=2)\n",
    "#training and evaluationg\n",
    "#Setting GPU Enviroment\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\" #using gpu \n",
    "cudnn.benchmark = True\n",
    "#training and evaluationg\n",
    "for K in [8,16,32,64]:#latent factors\n",
    "    model = NCF(int(user_num), int(item_num), factor_num=K, num_layers=3, dropout=0.0)\n",
    "    model.cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_function = nn.BCEWithLogitsLoss()\n",
    "    best_hr,best_ndcg = 0.0, 0.0\n",
    "    for epoch in range(20):#epochs=20\n",
    "        model.train()\n",
    "        train_loader.dataset.ng_sample()\n",
    "        for user, item, label in train_loader:\n",
    "            user = user.cuda()\n",
    "            item = item.cuda()\n",
    "            label = label.float().cuda()\n",
    "\n",
    "            model.zero_grad()\n",
    "            prediction = model(user, item)\n",
    "            loss = loss_function(prediction, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        HR, NDCG = metrics(model, test_loader, top_k=10)\n",
    "        if HR > best_hr: best_hr=HR\n",
    "        if NDCG > best_ndcg: best_ndcg=NDCG  \n",
    "    print(\"HR@10:{}, NDCG@10:{}, K:{}\".format(best_hr, best_ndcg, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR@10:0.5781456953642384, NDCG@10:0.32559769234087926, K:8\n",
      "HR@10:0.5915562913907285, NDCG@10:0.33110328294044583, K:16\n",
      "HR@10:0.6112582781456953, NDCG@10:0.34494348863020113, K:32\n",
      "HR@10:0.6142384105960265, NDCG@10:0.3536964208698385, K:64\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8  \n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.08.02\n",
    "@function: baseline: NCF(Neural Collaborative Filtering)\n",
    "           paper: https://arxiv.org/pdf/1708.05031.pdf\n",
    "           Datatset: MovieLens-1m:https://grouplens.org/datasets/movielens/  \n",
    "           Evaluation: HR@10 NDCG@10\n",
    "'''\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import heapq\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "class NCFData(torch.utils.data.Dataset):#define the dataset\n",
    "    def __init__(self, features, num_item, train_mat=None, num_ng=0, is_training=None):\n",
    "        super(NCFData, self).__init__()\n",
    "        # Note that the labels are only useful when training, we thus add them in the ng_sample() function.\n",
    "        self.features_ps = features\n",
    "        self.num_item = num_item\n",
    "        self.train_mat = train_mat\n",
    "        self.num_ng = num_ng\n",
    "        self.is_training = is_training\n",
    "        self.labels = [0 for _ in range(len(features))]\n",
    "\n",
    "    def ng_sample(self):\n",
    "        assert self.is_training, 'no need to sampling when testing'\n",
    "        self.features_ng = []\n",
    "        for x in self.features_ps:\n",
    "            u = x[0]\n",
    "            for t in range(self.num_ng):\n",
    "                j = np.random.randint(self.num_item)\n",
    "                while (u, j) in self.train_mat:\n",
    "                    j = np.random.randint(self.num_item)\n",
    "                self.features_ng.append([u, j])\n",
    "        \n",
    "        labels_ps = [1 for _ in range(len(self.features_ps))]\n",
    "        labels_ng = [0 for _ in range(len(self.features_ng))]\n",
    "        \n",
    "        self.features_fill = self.features_ps + self.features_ng\n",
    "        self.labels_fill = labels_ps + labels_ng\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.num_ng + 1) * len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        if self.is_training:\n",
    "            self.ng_sample()\n",
    "            features = self.features_fill\n",
    "            labels = self.labels_fill\n",
    "        else:\n",
    "            features = self.features_ps\n",
    "            labels = self.labels\n",
    "        '''\n",
    "        features = self.features_fill if self.is_training else self.features_ps\n",
    "        labels = self.labels_fill if self.is_training else self.labels\n",
    "        \n",
    "        user = features[idx][0]\n",
    "        item = features[idx][1]\n",
    "        label = labels[idx]\n",
    "        return user, item ,label\n",
    "\n",
    "#define the NCF model, integrating the GMF and MLP\n",
    "class NCF(nn.Module):\n",
    "    def __init__(self, user_num, item_num, factor_num, num_layers, dropout):\n",
    "        super(NCF, self).__init__()\n",
    "        \"\"\"\n",
    "        user_num: number of users;\n",
    "        item_num: number of items;\n",
    "        factor_num: number of predictive factors;\n",
    "        num_layers: the number of layers in MLP model;\n",
    "        dropout: dropout rate between fully connected layers;\n",
    "        \"\"\"\n",
    "        self.embed_user_GMF = nn.Embedding(user_num, factor_num)\n",
    "        self.embed_item_GMF = nn.Embedding(item_num, factor_num)\n",
    "        self.embed_user_MLP = nn.Embedding(user_num, factor_num * (2 ** (num_layers - 1)))\n",
    "        self.embed_item_MLP = nn.Embedding(item_num, factor_num * (2 ** (num_layers - 1)))\n",
    "\n",
    "        MLP_modules = []\n",
    "        for i in range(num_layers):\n",
    "            input_size = factor_num * (2 ** (num_layers - i))\n",
    "            MLP_modules.append(nn.Dropout(p=dropout))\n",
    "            MLP_modules.append(nn.Linear(input_size, input_size//2))\n",
    "            MLP_modules.append(nn.ReLU())\n",
    "        self.MLP_layers = nn.Sequential(*MLP_modules)\n",
    "        \n",
    "        self.predict_layer = nn.Linear(factor_num * 2, 1)\n",
    "        \n",
    "        self._init_weight_()\n",
    "        \n",
    "    def _init_weight_(self):\n",
    "        \"\"\" We leave the weights initialization here. \"\"\"\n",
    "        nn.init.normal_(self.embed_user_GMF.weight, std=0.01)\n",
    "        nn.init.normal_(self.embed_user_MLP.weight, std=0.01)\n",
    "        nn.init.normal_(self.embed_item_GMF.weight, std=0.01)\n",
    "        nn.init.normal_(self.embed_item_MLP.weight, std=0.01)\n",
    "        \n",
    "        for m in self.MLP_layers:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.kaiming_uniform_(self.predict_layer.weight, a=1, nonlinearity='sigmoid')\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        \n",
    "        embed_user_GMF = self.embed_user_GMF(user)\n",
    "        embed_item_GMF = self.embed_item_GMF(item)\n",
    "        output_GMF = embed_user_GMF * embed_item_GMF\n",
    "        \n",
    "        embed_user_MLP = self.embed_user_MLP(user)\n",
    "        embed_item_MLP = self.embed_item_MLP(item)\n",
    "        interaction = torch.cat((embed_user_MLP, embed_item_MLP), -1)\n",
    "        output_MLP = self.MLP_layers(interaction)\n",
    "\n",
    "        concat = torch.cat((output_GMF, output_MLP), -1)\n",
    "\n",
    "        prediction = self.predict_layer(concat)\n",
    "        return prediction.view(-1)\n",
    "    \n",
    "#loading dataset function\n",
    "def load_dataset(test_num=100):\n",
    "    train_data = pd.read_csv(\"/data/fjsdata/BNMF/ml-1m.train.rating\", \\\n",
    "                             sep='\\t', header=None, names=['user', 'item'], \\\n",
    "                             usecols=[0, 1], dtype={0: np.int32, 1: np.int32})\n",
    "\n",
    "    user_num = train_data['user'].max() + 1\n",
    "    item_num = train_data['item'].max() + 1\n",
    "    \n",
    "    train_data = train_data.values.tolist()\n",
    "    \n",
    "    # load ratings as a dok matrix\n",
    "    train_mat = sp.dok_matrix((user_num, item_num), dtype=np.float32)\n",
    "    for x in train_data:\n",
    "        train_mat[x[0], x[1]] = 1.0\n",
    "\n",
    "    test_data = []\n",
    "    with open(\"/data/fjsdata/BNMF/ml-1m.test.negative\", 'r') as fd:\n",
    "        line = fd.readline()\n",
    "        while line != None and line != '':\n",
    "            arr = line.split('\\t')\n",
    "            u = eval(arr[0])[0]\n",
    "            test_data.append([u, eval(arr[0])[1]])#one postive item\n",
    "            for i in arr[1:]:\n",
    "                test_data.append([u, int(i)]) #99 negative items\n",
    "            line = fd.readline()\n",
    "    return train_data, test_data, user_num, item_num, train_mat\n",
    "\n",
    "#evaluate function\n",
    "def hit(gt_item, pred_items):\n",
    "    if gt_item in pred_items:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def ndcg(gt_item, pred_items):\n",
    "    if gt_item in pred_items:\n",
    "        index = pred_items.index(gt_item)\n",
    "        return np.reciprocal(np.log2(index+2))\n",
    "    return 0\n",
    "\n",
    "\n",
    "def metrics(model, test_loader, top_k):\n",
    "    HR, NDCG = [], []\n",
    "    \n",
    "    for user, item, label in test_loader:\n",
    "        user = user.cuda()\n",
    "        item = item.cuda()\n",
    "        \n",
    "        predictions = model(user, item)\n",
    "        _, indices = torch.topk(predictions, top_k)\n",
    "        recommends = torch.take(item, indices).cpu().numpy().tolist()\n",
    "        \n",
    "        gt_item = item[0].item()\n",
    "        HR.append(hit(gt_item, recommends))\n",
    "        NDCG.append(ndcg(gt_item, recommends))\n",
    "    return np.mean(HR), np.mean(NDCG)\n",
    "\n",
    "#Setting GPU Enviroment\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\" #using gpu \n",
    "cudnn.benchmark = True\n",
    "# construct the train and test datasets\n",
    "train_data, test_data, user_num ,item_num, train_mat = load_dataset()\n",
    "train_dataset = NCFData(train_data, item_num, train_mat, num_ng=2, is_training=True)#pos:neg=1:2\n",
    "test_dataset = NCFData(test_data, item_num, train_mat, num_ng=0, is_training=False)#100\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4)\n",
    "#every user have 99 negative items and one positive items，so batch_size=100\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=99+1, shuffle=False, num_workers=2)\n",
    "#training and evaluationg\n",
    "#Setting GPU Enviroment\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\" #using gpu \n",
    "cudnn.benchmark = True\n",
    "#training and evaluationg\n",
    "for K in [8,16,32,64]:#latent factors\n",
    "    model = NCF(int(user_num), int(item_num), factor_num=K, num_layers=3, dropout=0.0)\n",
    "    model.cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_function = nn.BCEWithLogitsLoss()\n",
    "    best_hr,best_ndcg = 0.0, 0.0\n",
    "    for epoch in range(20):#epochs=20\n",
    "        model.train()\n",
    "        train_loader.dataset.ng_sample()\n",
    "        for user, item, label in train_loader:\n",
    "            user = user.cuda()\n",
    "            item = item.cuda()\n",
    "            label = label.float().cuda()\n",
    "\n",
    "            model.zero_grad()\n",
    "            prediction = model(user, item)\n",
    "            loss = loss_function(prediction, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        HR, NDCG = metrics(model, test_loader, top_k=10)\n",
    "        if HR > best_hr: best_hr=HR\n",
    "        if NDCG > best_ndcg: best_ndcg=NDCG  \n",
    "    print(\"HR@10:{}, NDCG@10:{}, K:{}\".format(best_hr, best_ndcg, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
