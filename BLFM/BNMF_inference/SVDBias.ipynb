{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Interaction = 994169, User = 6040, Item = 3706, Sparsity = 0.0444\n",
      "Iteration: 10 ; error = 437.6707\n",
      "Iteration: 20 ; error = 436.3256\n",
      "Iteration: 30 ; error = 435.6824\n",
      "Iteration: 40 ; error = 434.7616\n",
      "Iteration: 50 ; error = 433.1408\n",
      "HR@10:0.46655629139072846, NDCG@10:0.26113775091566355, K:8\n",
      "Iteration: 10 ; error = 437.1201\n",
      "Iteration: 20 ; error = 436.3696\n",
      "Iteration: 30 ; error = 435.8847\n",
      "Iteration: 40 ; error = 435.1103\n",
      "Iteration: 50 ; error = 433.1086\n",
      "HR@10:0.46572847682119206, NDCG@10:0.2616682762344062, K:16\n",
      "Iteration: 10 ; error = 437.1657\n",
      "Iteration: 20 ; error = 436.7812\n",
      "Iteration: 30 ; error = 436.3101\n",
      "Iteration: 40 ; error = 435.7479\n",
      "Iteration: 50 ; error = 434.3493\n",
      "HR@10:0.46572847682119206, NDCG@10:0.2624086717011347, K:32\n",
      "Iteration: 10 ; error = 437.0290\n",
      "Iteration: 20 ; error = 436.8687\n",
      "Iteration: 30 ; error = 436.7618\n",
      "Iteration: 40 ; error = 436.5100\n",
      "Iteration: 50 ; error = 435.9069\n",
      "HR@10:0.46208609271523177, NDCG@10:0.26079258349727474, K:64\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8  \n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.08.02\n",
    "@function: baseline: SVDBias\n",
    "           paper: https://dl.acm.org/citation.cfm?id=1401944\n",
    "           Datatset: MovieLens-1m:https://grouplens.org/datasets/movielens/  \n",
    "           Evaluation: HR@10 NDCG@10\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import heapq\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import tensorflow as tf\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self):\n",
    "        self.trainset, self.testset, self.maxu, self.maxi = self._getDataset_as_list()\n",
    "        \n",
    "    def _getDataset_as_list(self):\n",
    "        #trainset\n",
    "        filePath = \"/data/fjsdata/BNMF/ml-1m.train.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        data['rating']=data['rating'].apply(lambda x: 1.0 if float(x)>0.0 else 0.0)\n",
    "        maxu, maxi = data['user'].max()+1, data['item'].max()+1\n",
    "        print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "        trainset = data.values.tolist()\n",
    "        #testset\n",
    "        testset = []\n",
    "        filePath = \"/data/fjsdata/BNMF/ml-1m.test.negative\" \n",
    "        with open(filePath, 'r') as fd:\n",
    "            line = fd.readline()\n",
    "            while line != None and line != '':\n",
    "                arr = line.split('\\t')\n",
    "                u = eval(arr[0])[0]\n",
    "                testset.append([u, eval(arr[0])[1], 1.0])#first is one postive item\n",
    "                for i in arr[1:]:\n",
    "                    testset.append([u, int(i), 0.0]) #99 negative items\n",
    "                line = fd.readline()\n",
    "        return trainset, testset, maxu, maxi \n",
    "    \n",
    "    def list_to_matrix(self, dataset, maxu, maxi):              \n",
    "        dataMat = np.zeros([maxu, maxi], dtype=np.float32)\n",
    "        for u,i,r in dataset:\n",
    "            dataMat[int(u)][int(i)] = float(r)\n",
    "        return np.array(dataMat)\n",
    "    \n",
    "    def list_to_dict(self, dataset):\n",
    "        dataDict = {}\n",
    "        for u,i,r in dataset:\n",
    "            dataDict[int(u), int(i)] = float(r)\n",
    "        return dataDict\n",
    "    \n",
    "    def getHitRatio(self, ranklist, targetItem):\n",
    "        for item in ranklist:\n",
    "            if item == targetItem:\n",
    "                return 1\n",
    "        return 0\n",
    "    def getNDCG(self, ranklist, targetItem):\n",
    "        for i in range(len(ranklist)):\n",
    "            item = ranklist[i]\n",
    "            if item == targetItem:\n",
    "                return math.log(2) / math.log(i+2)\n",
    "        return 0\n",
    "\n",
    "class SVDBias():\n",
    "    \n",
    "    def __init__(self, R, num_ng):\n",
    "        \"\"\"\n",
    "        Perform matrix factorization to predict empty entries in a matrix.     \n",
    "        Arguments\n",
    "        - R (ndarray)   : user-item rating matrix\n",
    "        - num_ng (int)  : number of negative items\n",
    "        \"\"\"\n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.num_ng = num_ng\n",
    "        \n",
    "        # Create a list of training samples\n",
    "        pos_samples = [\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] > 0\n",
    "        ]\n",
    "        #smapling the negative items\n",
    "        neg_samples = random.sample([\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] == 0\n",
    "        ], len(pos_samples)*num_ng)\n",
    "        \n",
    "        self.samples = pos_samples + neg_samples\n",
    "        \n",
    "    def train(self, K, alpha=0.001, beta=0.01, epochs=20):\n",
    "        '''\n",
    "        - alpha (float) : learning rate\n",
    "        - beta (float)  : regularization parameter\n",
    "        - K (int)       : number of latent dimensions\n",
    "        -epochs(int)    : number of iterations\n",
    "        '''\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.epochs = epochs\n",
    "        # Initialize user and item latent feature matrice\n",
    "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
    "        \n",
    "        # Initialize the biases\n",
    "        self.b_u = np.zeros(self.num_users)\n",
    "        self.b_i = np.zeros(self.num_items)\n",
    "        self.b = np.mean(self.R[np.where(self.R != 0)])\n",
    "               \n",
    "        # Perform stochastic gradient descent for number of iterations\n",
    "        training_process = []\n",
    "        for i in range(self.epochs):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            if (i+1) % 10 == 0:\n",
    "                mse = self.mse()\n",
    "                print(\"Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "        \n",
    "        return self.full_matrix()\n",
    "\n",
    "    def mse(self):\n",
    "        \"\"\"\n",
    "        A function to compute the total mean square error\n",
    "        \"\"\"\n",
    "        xs, ys = self.R.nonzero()\n",
    "        predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        for x, y in zip(xs, ys):\n",
    "            error += pow(self.R[x, y] - predicted[x, y], 2)\n",
    "        return np.sqrt(error)\n",
    "\n",
    "    def sgd(self):\n",
    "        \"\"\"\n",
    "        Perform stochastic graident descent\n",
    "        \"\"\"\n",
    "        for i, j, r in self.samples:\n",
    "            # Computer prediction and error\n",
    "            prediction = self.get_rating(i, j)\n",
    "            e = (r - prediction)\n",
    "            \n",
    "            # Update biases\n",
    "            self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i])\n",
    "            self.b_i[j] += self.alpha * (e - self.beta * self.b_i[j])\n",
    "            \n",
    "            # Create copy of row of P since we need to update it but use older values for update on Q\n",
    "            P_i = self.P[i, :][:]\n",
    "            \n",
    "            # Update user and item latent feature matrices\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            self.Q[j, :] += self.alpha * (e * P_i - self.beta * self.Q[j,:])\n",
    "\n",
    "    def get_rating(self, i, j):\n",
    "        \"\"\"\n",
    "        Get the predicted rating of user i and item j\n",
    "        \"\"\"\n",
    "        prediction = self.b + self.b_u[i] + self.b_i[j] + self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "    \n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        Computer the full matrix using the resultant biases, P and Q\n",
    "        \"\"\"\n",
    "        return self.b + self.b_u[:,np.newaxis] + self.b_i[np.newaxis:,] + self.P.dot(self.Q.T)\n",
    "    \n",
    "if __name__ == \"__main__\": \n",
    "    ds = DataSet()#loading dataset\n",
    "    svdb = SVDBias(R=ds.list_to_matrix(ds.trainset,ds.maxu,ds.maxi), num_ng=2)#samples ratios is pos:neg = 1:2\n",
    "    for K in [8,16,32,64]:#latent factors\n",
    "        nR = svdb.train(K=K, alpha=0.001, beta=0.01, epochs=50)\n",
    "        hrs = []\n",
    "        ndcgs = []\n",
    "        num_batches = len(ds.testset) // 100\n",
    "        for i in range(num_batches):\n",
    "            test_batch = ds.testset[i*100: (i+1)*100]\n",
    "            targetItem = int(test_batch[0][1])#positive item\n",
    "            map_item_score = {}\n",
    "            for j in range(100):\n",
    "                tu ,ti = int(test_batch[j][0]), int(test_batch[j][1])\n",
    "                map_item_score[ti] = nR[tu,ti]\n",
    "            ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "            hrs.append(ds.getHitRatio(ranklist, targetItem))\n",
    "            ndcgs.append(ds.getNDCG(ranklist, targetItem))\n",
    "        hr, ndcg = np.array(hrs).mean(), np.array(ndcgs).mean()\n",
    "        print(\"HR@10:{}, NDCG@10:{}, K:{}\".format(hr, ndcg, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Interaction = 994169, User = 6040, Item = 3706, Sparsity = 0.0444\n",
      "HR@10:0.08874172185430464, NDCG@10:0.0408303303215734, K:8\n",
      "HR@10:0.0945364238410596, NDCG@10:0.042871776899174326, K:16\n",
      "HR@10:0.10066225165562914, NDCG@10:0.044435626680108996, K:32\n",
      "HR@10:0.10546357615894039, NDCG@10:0.047462935540000334, K:64\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8  \n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.08.02\n",
    "@function: baseline: SVDBias\n",
    "           paper: https://dl.acm.org/citation.cfm?id=1401944\n",
    "           Datatset: MovieLens-1m:https://grouplens.org/datasets/movielens/  \n",
    "           Evaluation: HR@10 NDCG@10\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import heapq\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import tensorflow as tf\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self):\n",
    "        self.trainset, self.testset, self.maxu, self.maxi = self._getDataset_as_list()\n",
    "        \n",
    "    def _getDataset_as_list(self):\n",
    "        #trainset\n",
    "        filePath = \"/data/fjsdata/BNMF/ml-1m.train.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        data['rating']=data['rating'].apply(lambda x: 1.0 if float(x)>0.0 else 0.0)\n",
    "        maxu, maxi = data['user'].max()+1, data['item'].max()+1\n",
    "        print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "        trainset = data.values.tolist()\n",
    "        #testset\n",
    "        testset = []\n",
    "        filePath = \"/data/fjsdata/BNMF/ml-1m.test.negative\" \n",
    "        with open(filePath, 'r') as fd:\n",
    "            line = fd.readline()\n",
    "            while line != None and line != '':\n",
    "                arr = line.split('\\t')\n",
    "                u = eval(arr[0])[0]\n",
    "                testset.append([u, eval(arr[0])[1], 1.0])#first is one postive item\n",
    "                for i in arr[1:]:\n",
    "                    testset.append([u, int(i), 0.0]) #99 negative items\n",
    "                line = fd.readline()\n",
    "        return trainset, testset, maxu, maxi \n",
    "    \n",
    "    def list_to_matrix(self, dataset, maxu, maxi):              \n",
    "        dataMat = np.zeros([maxu, maxi], dtype=np.float32)\n",
    "        for u,i,r in dataset:\n",
    "            dataMat[int(u)][int(i)] = float(r)\n",
    "        return np.array(dataMat)\n",
    "    \n",
    "    def list_to_dict(self, dataset):\n",
    "        dataDict = {}\n",
    "        for u,i,r in dataset:\n",
    "            dataDict[int(u), int(i)] = float(r)\n",
    "        return dataDict\n",
    "    \n",
    "    def getHitRatio(self, ranklist, targetItem):\n",
    "        for item in ranklist:\n",
    "            if item == targetItem:\n",
    "                return 1\n",
    "        return 0\n",
    "    def getNDCG(self, ranklist, targetItem):\n",
    "        for i in range(len(ranklist)):\n",
    "            item = ranklist[i]\n",
    "            if item == targetItem:\n",
    "                return math.log(2) / math.log(i+2)\n",
    "        return 0\n",
    "\n",
    "class SVDBias():\n",
    "    \n",
    "    def __init__(self, R, num_ng):\n",
    "        \"\"\"\n",
    "        Perform matrix factorization to predict empty entries in a matrix.     \n",
    "        Arguments\n",
    "        - R (ndarray)   : user-item rating matrix\n",
    "        - num_ng (int)  : number of negative items\n",
    "        \"\"\"\n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.num_ng = num_ng\n",
    "        \n",
    "        # Create a list of training samples\n",
    "        pos_samples = [\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] > 0\n",
    "        ]\n",
    "        #smapling the negative items\n",
    "        neg_samples = random.sample([\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] == 0\n",
    "        ], len(pos_samples)*num_ng)\n",
    "        \n",
    "        self.samples = pos_samples + neg_samples\n",
    "        \n",
    "    def train(self, K, alpha=0.001, beta=0.01, epochs=20):\n",
    "        '''\n",
    "        - alpha (float) : learning rate\n",
    "        - beta (float)  : regularization parameter\n",
    "        - K (int)       : number of latent dimensions\n",
    "        -epochs(int)    : number of iterations\n",
    "        '''\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.epochs = epochs\n",
    "        # Initialize user and item latent feature matrice\n",
    "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
    "        \n",
    "        # Initialize the biases\n",
    "        self.b_u = np.zeros(self.num_users)\n",
    "        self.b_i = np.zeros(self.num_items)\n",
    "        self.b = np.mean(self.R[np.where(self.R != 0)])\n",
    "               \n",
    "        # Perform stochastic gradient descent for number of iterations\n",
    "        training_process = []\n",
    "        for i in range(self.epochs):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            if (i+1) % 10 == 0:\n",
    "                mse = self.mse()\n",
    "                print(\"Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "        \n",
    "        return self.full_matrix()\n",
    "\n",
    "    def mse(self):\n",
    "        \"\"\"\n",
    "        A function to compute the total mean square error\n",
    "        \"\"\"\n",
    "        xs, ys = self.R.nonzero()\n",
    "        predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        for x, y in zip(xs, ys):\n",
    "            error += pow(self.R[x, y] - predicted[x, y], 2)\n",
    "        return np.sqrt(error)\n",
    "\n",
    "    def sgd(self):\n",
    "        \"\"\"\n",
    "        Perform stochastic graident descent\n",
    "        \"\"\"\n",
    "        for i, j, r in self.samples:\n",
    "            # Computer prediction and error\n",
    "            prediction = self.get_rating(i, j)\n",
    "            e = (r - prediction)\n",
    "            \n",
    "            # Update biases\n",
    "            self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i])\n",
    "            self.b_i[j] += self.alpha * (e - self.beta * self.b_i[j])\n",
    "            \n",
    "            # Create copy of row of P since we need to update it but use older values for update on Q\n",
    "            P_i = self.P[i, :][:]\n",
    "            \n",
    "            # Update user and item latent feature matrices\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            self.Q[j, :] += self.alpha * (e * P_i - self.beta * self.Q[j,:])\n",
    "\n",
    "    def get_rating(self, i, j):\n",
    "        \"\"\"\n",
    "        Get the predicted rating of user i and item j\n",
    "        \"\"\"\n",
    "        prediction = self.b + self.b_u[i] + self.b_i[j] + self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "    \n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        Computer the full matrix using the resultant biases, P and Q\n",
    "        \"\"\"\n",
    "        return self.b + self.b_u[:,np.newaxis] + self.b_i[np.newaxis:,] + self.P.dot(self.Q.T)\n",
    "    \n",
    "if __name__ == \"__main__\": \n",
    "    ds = DataSet()#loading dataset\n",
    "    svdb = SVDBias(R=ds.list_to_matrix(ds.trainset,ds.maxu,ds.maxi), num_ng=2)#samples ratios is pos:neg = 1:2\n",
    "    for K in [8,16,32,64]:#latent factors\n",
    "        nR = svdb.train(K=K, alpha=0.001, beta=0.01, epochs=50)\n",
    "        hrs = []\n",
    "        ndcgs = []\n",
    "        num_batches = len(ds.testset) // 100\n",
    "        for i in range(num_batches):\n",
    "            test_batch = ds.testset[i*100: (i+1)*100]\n",
    "            targetItem = int(test_batch[0][1])#positive item\n",
    "            map_item_score = {}\n",
    "            for j in range(100):\n",
    "                tu ,ti = int(test_batch[j][0]), int(test_batch[j][1])\n",
    "                map_item_score[ti] = nR[tu,ti]\n",
    "            ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "            hrs.append(ds.getHitRatio(ranklist, targetItem))\n",
    "            ndcgs.append(ds.getNDCG(ranklist, targetItem))\n",
    "        hr, ndcg = np.array(hrs).mean(), np.array(ndcgs).mean()\n",
    "        print(\"HR@10:{}, NDCG@10:{}, K:{}\".format(hr, ndcg, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
