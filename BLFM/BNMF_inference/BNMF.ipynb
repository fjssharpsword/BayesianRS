{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Interaction = 994169, User = 6040, Item = 3706, Sparsity = 0.0444\n",
      "start training the BNMF model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 3.9334e+10: 100%|██████████| 10000/10000 [15:04:37<00:00,  4.46s/it] \n",
      "Finished [100%]: Average Loss = 3.8741e+10\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0816 23:55:21.916945 139764077238016 inference.py:248] Finished [100%]: Average Loss = 3.8741e+10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "rng_mrg cpu-implementation does not support more than (2**31 -1) samples\nApply node that caused the error: mrg_uniform{TensorType(float64, vector),inplace}(<TensorType(int32, matrix)>, MakeVector{dtype='int64'}.0)\nToposort index: 11\nInputs types: [TensorType(int32, matrix), TensorType(int64, vector)]\nInputs shapes: [(15360, 6), (1,)]\nInputs strides: [(24, 4), (8,)]\nInputs values: ['not shown', array([19992576000])]\nOutputs clients: [['output'], [Shape_i{0}(mrg_uniform{TensorType(float64, vector),inplace}.1), Subtensor{int64::}(mrg_uniform{TensorType(float64, vector),inplace}.1, ScalarFromTensor.0), Subtensor{:int64:}(mrg_uniform{TensorType(float64, vector),inplace}.1, ScalarFromTensor.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: rng_mrg cpu-implementation does not support more than (2**31 -1) samples",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bb927ad43d60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mK\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBNMF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_BNMF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_BNMF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtestset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RMSE@{}:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-bb927ad43d60>\u001b[0m in \u001b[0;36mtrain_BNMF\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbnmf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mapprox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mADVI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapprox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdraws\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pymc3/variational/opvi.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, draws, include_transformed)\u001b[0m\n\u001b[1;32m   1587\u001b[0m         vars_sampled = get_default_varnames(self.model.unobserved_RVs,\n\u001b[1;32m   1588\u001b[0m                                             include_transformed=include_transformed)\n\u001b[0;32m-> 1589\u001b[0;31m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_dict_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdraws\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1590\u001b[0m         \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdraws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m         trace = pm.sampling.NDArray(model=self.model, vars=vars_sampled, test_point={\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pymc3/variational/opvi.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(draws)\u001b[0m\n\u001b[1;32m   1565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1566\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdraws\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1567\u001b[0;31m             \u001b[0m_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdraws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1568\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfree_RVs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    915\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[1;32m    918\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m                 \u001b[0;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/gof/link.py\u001b[0m in \u001b[0;36mraise_with_op\u001b[0;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# extra long error message in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    690\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: rng_mrg cpu-implementation does not support more than (2**31 -1) samples\nApply node that caused the error: mrg_uniform{TensorType(float64, vector),inplace}(<TensorType(int32, matrix)>, MakeVector{dtype='int64'}.0)\nToposort index: 11\nInputs types: [TensorType(int32, matrix), TensorType(int64, vector)]\nInputs shapes: [(15360, 6), (1,)]\nInputs strides: [(24, 4), (8,)]\nInputs values: ['not shown', array([19992576000])]\nOutputs clients: [['output'], [Shape_i{0}(mrg_uniform{TensorType(float64, vector),inplace}.1), Subtensor{int64::}(mrg_uniform{TensorType(float64, vector),inplace}.1, ScalarFromTensor.0), Subtensor{:int64:}(mrg_uniform{TensorType(float64, vector),inplace}.1, ScalarFromTensor.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "# coding:utf-8  \n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.08.16\n",
    "@function: BNMF (Bayesian Neural Matrix Factorization)\n",
    "           Datatset: MovieLens-1m:https://grouplens.org/datasets/movielens/  \n",
    "           Evaluation:RMSE\n",
    "@reference:minibatch training, https://twiecki.io/blog/2016/07/05/bayesian-deep-learning/ \n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import heapq\n",
    "import math\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pymc3 as pm\n",
    "import theano\n",
    "import tensorflow as tf\n",
    "import theano.tensor as tt\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self):\n",
    "        self.trainset, self.testset, self.maxu, self.maxi, self.maxr = self._getDataset_as_list()\n",
    "        \n",
    "    def _getDataset_as_list(self):\n",
    "        #trainset\n",
    "        filePath = \"/data/fjsdata/BMF/ml-1m.train.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        maxu, maxi, maxr = data['user'].max()+1, data['item'].max()+1, data['rating'].max()\n",
    "        print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "        trainset = data.values.tolist()\n",
    "        #testset\n",
    "        filePath = \"/data/fjsdata/BMF/ml-1m.test.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        testset = data.values.tolist()\n",
    "        return trainset, testset, maxu, maxi, maxr \n",
    "        \n",
    "    def list_to_matrix(self, dataset, maxu, maxi):              \n",
    "        dataMat = np.zeros([maxu, maxi], dtype=np.float32)\n",
    "        for u,i,r in dataset:\n",
    "            dataMat[int(u)][int(i)] = float(r)\n",
    "        return np.array(dataMat)\n",
    "    \n",
    "    def list_to_dict(self, dataset):\n",
    "        dataDict = {}\n",
    "        for u,i,r in dataset:\n",
    "            dataDict[int(u), int(i)] = float(r)\n",
    "        return dataDict\n",
    "    \n",
    "    def getInstances(self, dataset):\n",
    "        user = []\n",
    "        item = []\n",
    "        rate = []\n",
    "        for u, i, r in dataset:\n",
    "            user.append(int(u))\n",
    "            item.append(int(i))\n",
    "            rate.append(float(r))\n",
    "        return np.array(user), np.array(item), np.array(rate)\n",
    "    \n",
    "class BNMF:\n",
    "    def __init__(self, dataset, K):\n",
    "        self.shape = [dataset.maxu, dataset.maxi]\n",
    "        self.maxr = dataset.maxr\n",
    "        self.R = dataset.list_to_matrix(dataset.trainset, dataset.maxu, dataset.maxi)\n",
    "\n",
    "        #K is number of latent factors\n",
    "        self.userLayer = [1024, K]\n",
    "        self.itemLayer = [1024, K]\n",
    "\n",
    "    def train_BNMF(self, verbose=10):       \n",
    "        print('start training the BNMF model')\n",
    "        tstart = time.time()\n",
    "        \n",
    "        with pm.Model() as self.bnmf:#difine\n",
    "            #user layer\n",
    "            user_W1 = pm.Normal('user_W1', 0, sd=1, shape=[self.shape[1], self.userLayer[0]] )\n",
    "            user_O1 = tt.nnet.relu(tt.dot(self.R, user_W1))\n",
    "            user_W2 = pm.Normal('user_W2', 0, sd=1, shape=[self.userLayer[0],self.userLayer[1]] )\n",
    "            user_O2 = tt.nnet.relu(tt.dot(user_O1, user_W2))\n",
    "            #item layer\n",
    "            item_W1 = pm.Normal('item_W1', 0, sd=1, shape=[self.shape[0], self.itemLayer[0]] )\n",
    "            item_O1 = tt.nnet.relu(tt.dot(self.R.T, item_W1))\n",
    "            item_W2 = pm.Normal('item_W2', 0, sd=1, shape=[self.itemLayer[0],self.itemLayer[1]] )\n",
    "            item_O2 = tt.nnet.relu(tt.dot(item_O1, item_W2))\n",
    "            # latent factor matrix\n",
    "            R = pm.Deterministic('R', tt.dot(user_O2,item_O2.T))\n",
    "            Y = pm.Normal('Y',mu=R, sd=self.maxr/2, observed=self.R)\n",
    "            \n",
    "        with self.bnmf:#inference\n",
    "            approx = pm.fit(n=10000, method=pm.ADVI())\n",
    "            trace = approx.sample(draws=2000)\n",
    "            \n",
    "        elapsed = time.time() - tstart    \n",
    "        print('Completed training the BNMF model in %d seconds' % int(elapsed))\n",
    "        \n",
    "        return trace\n",
    "           \n",
    "    def eval_BNMF(self, trace, testset):\n",
    "        \n",
    "        with self.bnmf:#evaluation\n",
    "            ppc = pm.sample_posterior_predictive(trace, progressbar=True) \n",
    "            nR =  np.mean(ppc['Y'],0) #expectation of parameters\n",
    "            \n",
    "        squaredError = []\n",
    "        for u,i,r in testset:    \n",
    "            error=r - nR[int(u),int(i)]\n",
    "            squaredError.append(error * error)\n",
    "        rmse =math.sqrt(sum(squaredError) / len(squaredError))\n",
    "        return rmse\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    ds = DataSet()#loading dataset\n",
    "    for K in [8,16,32,64]:\n",
    "        model = BNMF(ds, K)  \n",
    "        trace = model.train_BNMF()\n",
    "        rmse = model.eval_BNMF(trace=trace, testset=ds.testset)\n",
    "        print(\"RMSE@{}:{}\".format(K, rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Interaction = 994169, User = 6040, Item = 3706, Sparsity = 0.0444\n",
      "start building the Multi-layer non-linear projection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0803 14:39:07.944793 140164905830144 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done building the the Multi-layer non-linear projection\n",
      "start training the BNCF model\n",
      "2910 / 2913 : shape = 2980864 start building the Bayesian probabilistic model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 1.1495e+06: 100%|██████████| 10000/10000 [1:21:06<00:00,  1.99it/s]\n",
      "Finished [100%]: Average Loss = 1.1495e+06\n",
      "I0803 16:01:42.123292 140164905830144 inference.py:248] Finished [100%]: Average Loss = 1.1495e+06\n",
      "100%|██████████| 3000/3000 [25:17<00:00,  2.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done building the Bayesian probabilistic model\n",
      "2910 / 2913 : loss = 0.013316974043846139Completed training the BNCF model in 7646 seconds\n",
      "\n",
      "Mean loss in this epoch is: 0.03215548023581505\n",
      "580 / 590 : shape = 594944 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [05:26<00:00,  8.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training the BNCF model\n",
      "2910 / 2913 : shape = 2980864 start building the Bayesian probabilistic model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 1.1393e+06: 100%|██████████| 10000/10000 [1:21:30<00:00,  2.08it/s]\n",
      "Finished [100%]: Average Loss = 1.1392e+06\n",
      "I0803 18:15:37.709226 140164905830144 inference.py:248] Finished [100%]: Average Loss = 1.1392e+06\n",
      "100%|██████████| 3000/3000 [30:41<00:00,  1.86it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done building the Bayesian probabilistic model\n",
      "2910 / 2913 : loss = 0.0086232619360089335Completed training the BNCF model in 8050 seconds\n",
      "\n",
      "Mean loss in this epoch is: 0.010228825733065605\n",
      "580 / 590 : shape = 594944 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [05:38<00:00,  8.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR@10:0.4652317880794702, NDCG@10:0.26075071741590367, K:8\n",
      "start building the Multi-layer non-linear projection\n",
      "done building the the Multi-layer non-linear projection\n",
      "start training the BNCF model\n",
      "2910 / 2913 : shape = 2980864 start building the Bayesian probabilistic model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 1.1485e+06: 100%|██████████| 10000/10000 [1:24:45<00:00,  2.07it/s]\n",
      "Finished [100%]: Average Loss = 1.1484e+06\n",
      "I0803 20:39:48.358055 140164905830144 inference.py:248] Finished [100%]: Average Loss = 1.1484e+06\n",
      "100%|██████████| 3000/3000 [23:47<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done building the Bayesian probabilistic model\n",
      "2910 / 2913 : loss = 0.009208471514284612Completed training the BNCF model in 7483 seconds\n",
      "\n",
      "Mean loss in this epoch is: 0.032850608229637146\n",
      "580 / 590 : shape = 594944 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [06:44<00:00,  8.17it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training the BNCF model\n",
      "2910 / 2913 : shape = 2980864 start building the Bayesian probabilistic model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 1.1373e+06: 100%|██████████| 10000/10000 [1:22:32<00:00,  1.98it/s]\n",
      "Finished [100%]: Average Loss = 1.1373e+06\n",
      "I0803 22:49:19.572719 140164905830144 inference.py:248] Finished [100%]: Average Loss = 1.1373e+06\n",
      "100%|██████████| 3000/3000 [25:14<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done building the Bayesian probabilistic model\n",
      "2910 / 2913 : loss = 0.008598057553172112Completed training the BNCF model in 7753 seconds\n",
      "\n",
      "Mean loss in this epoch is: 0.008413445204496384\n",
      "580 / 590 : shape = 594944 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [05:06<00:00,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR@10:0.4630794701986755, NDCG@10:0.2593814462660595, K:16\n",
      "start building the Multi-layer non-linear projection\n",
      "done building the the Multi-layer non-linear projection\n",
      "start training the BNCF model\n",
      "2910 / 2913 : shape = 2980864 start building the Bayesian probabilistic model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 1.1481e+06: 100%|██████████| 10000/10000 [1:20:19<00:00,  2.06it/s]\n",
      "Finished [100%]: Average Loss = 1.1481e+06\n",
      "I0804 01:02:28.361850 140164905830144 inference.py:248] Finished [100%]: Average Loss = 1.1481e+06\n",
      "100%|██████████| 3000/3000 [27:59<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done building the Bayesian probabilistic model\n",
      "2910 / 2913 : loss = 0.008582066744565964Completed training the BNCF model in 7495 seconds\n",
      "\n",
      "Mean loss in this epoch is: 0.04655837267637253\n",
      "580 / 590 : shape = 594944 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [05:38<00:00,  9.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training the BNCF model\n",
      "2910 / 2913 : shape = 2980864 start building the Bayesian probabilistic model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 1.1378e+06: 100%|██████████| 10000/10000 [1:23:24<00:00,  2.31it/s]\n",
      "Finished [100%]: Average Loss = 1.1377e+06\n",
      "I0804 03:16:51.848708 140164905830144 inference.py:248] Finished [100%]: Average Loss = 1.1377e+06\n",
      "100%|██████████| 3000/3000 [27:46<00:00,  1.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done building the Bayesian probabilistic model\n",
      "2910 / 2913 : loss = 0.008137103170156479Completed training the BNCF model in 8432 seconds\n",
      "\n",
      "Mean loss in this epoch is: 0.008488461375236511\n",
      "580 / 590 : shape = 594944 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [05:54<00:00,  8.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR@10:0.46556291390728477, NDCG@10:0.25983256693945883, K:32\n",
      "start building the Multi-layer non-linear projection\n",
      "done building the the Multi-layer non-linear projection\n",
      "start training the BNCF model\n",
      "2910 / 2913 : shape = 2980864 start building the Bayesian probabilistic model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 1.1483e+06: 100%|██████████| 10000/10000 [1:23:46<00:00,  2.05it/s]\n",
      "Finished [100%]: Average Loss = 1.1482e+06\n",
      "I0804 05:44:47.529829 140164905830144 inference.py:248] Finished [100%]: Average Loss = 1.1482e+06\n",
      "100%|██████████| 3000/3000 [25:37<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done building the Bayesian probabilistic model\n",
      "2910 / 2913 : loss = 0.008155688643455505Completed training the BNCF model in 8146 seconds\n",
      "\n",
      "Mean loss in this epoch is: 0.042705513536930084\n",
      "580 / 590 : shape = 594944 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [07:39<00:00,  6.86it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training the BNCF model\n",
      "2910 / 2913 : shape = 2980864 start building the Bayesian probabilistic model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 1.138e+06: 100%|██████████| 10000/10000 [1:26:13<00:00,  1.94it/s]\n",
      "Finished [100%]: Average Loss = 1.1379e+06\n",
      "I0804 08:11:11.584494 140164905830144 inference.py:248] Finished [100%]: Average Loss = 1.1379e+06\n",
      "100%|██████████| 3000/3000 [30:24<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done building the Bayesian probabilistic model\n",
      "2910 / 2913 : loss = 4.186251163482666881Completed training the BNCF model in 8526 seconds\n",
      "\n",
      "Mean loss in this epoch is: 5.048369407653809\n",
      "580 / 590 : shape = 594944 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [05:26<00:00, 10.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR@10:0.46341059602649004, NDCG@10:0.25838422916669485, K:64\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8  \n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.08.02\n",
    "@function: BNMF (Bayesian Neural Matrix Factorization)\n",
    "           Datatset: MovieLens-1m:https://grouplens.org/datasets/movielens/  \n",
    "           Evaluation: HR@10 NDCG@10\n",
    "'''\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import heapq\n",
    "import math\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pymc3 as pm\n",
    "import theano\n",
    "import tensorflow as tf\n",
    "import theano.tensor as tt\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self):\n",
    "        self.trainset, self.testset, self.maxu, self.maxi = self._getDataset_as_list()\n",
    "        \n",
    "    def _getDataset_as_list(self):\n",
    "        #trainset\n",
    "        filePath = \"/data/fjsdata/BNMF/ml-1m.train.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        data['rating']=data['rating'].apply(lambda x: 1.0 if float(x)>0.0 else 0.0)\n",
    "        maxu, maxi = data['user'].max()+1, data['item'].max()+1\n",
    "        print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "        trainset = data.values.tolist()\n",
    "        #testset\n",
    "        testset = []\n",
    "        filePath = \"/data/fjsdata/BNMF/ml-1m.test.negative\" \n",
    "        with open(filePath, 'r') as fd:\n",
    "            line = fd.readline()\n",
    "            while line != None and line != '':\n",
    "                arr = line.split('\\t')\n",
    "                u = eval(arr[0])[0]\n",
    "                testset.append([u, eval(arr[0])[1], 1.0])#first is one postive item\n",
    "                for i in arr[1:]:\n",
    "                    testset.append([u, int(i), 0.0]) #99 negative items\n",
    "                line = fd.readline()\n",
    "        return trainset, testset, maxu, maxi\n",
    "        \n",
    "    def list_to_matrix(self, dataset, maxu, maxi):              \n",
    "        dataMat = np.zeros([maxu, maxi], dtype=np.float32)\n",
    "        for u,i,r in dataset:\n",
    "            dataMat[int(u)][int(i)] = float(r)\n",
    "        return np.array(dataMat)\n",
    "    \n",
    "    def list_to_dict(self, dataset):\n",
    "        dataDict = {}\n",
    "        for u,i,r in dataset:\n",
    "            dataDict[int(u), int(i)] = float(r)\n",
    "        return dataDict\n",
    "    \n",
    "    def getInstances(self, dataset, maxu=None, maxi=None, num_ng=None, isTest=False):\n",
    "        user = []\n",
    "        item = []\n",
    "        rate = []\n",
    "        if isTest==True: #test\n",
    "            for u, i, r in dataset:\n",
    "                user.append(int(u))\n",
    "                item.append(int(i))\n",
    "                rate.append(float(r))\n",
    "        else:#train\n",
    "            for u, i, r in dataset:\n",
    "                user.append(int(u))\n",
    "                item.append(int(i))\n",
    "                rate.append(float(r))\n",
    "            #negative samples\n",
    "            dataDict = self.list_to_dict(dataset)\n",
    "            for j in range(len(dataset)*num_ng):\n",
    "                u = np.random.randint(maxu)\n",
    "                i = np.random.randint(maxi)\n",
    "                while (u, i) in dataDict:\n",
    "                    u = np.random.randint(maxu)\n",
    "                    i = np.random.randint(maxi)\n",
    "                user.append(int(u))\n",
    "                item.append(int(i))\n",
    "                rate.append(float(0.0)) \n",
    "        return np.array(user), np.array(item), np.array(rate)\n",
    "    \n",
    "class BNMF:\n",
    "    def __init__(self, dataset, K, num_ng):\n",
    "        self.shape = [dataset.maxu, dataset.maxi]\n",
    "        #get the trainset and testset\n",
    "        self.train_u, self.train_i, self.train_r = dataset.getInstances(dataset = dataset.trainset, \\\n",
    "                                                                        maxu=dataset.maxu, \\\n",
    "                                                                        maxi=dataset.maxi, \\\n",
    "                                                                        num_ng=num_ng, isTest=False)\n",
    "        assert(len(self.train_u) == len(self.train_i) and len(self.train_i) == len(self.train_r))\n",
    "        shuffled_idx = np.random.permutation(np.arange(len(self.train_u)))\n",
    "        self.train_u = self.train_u[shuffled_idx]\n",
    "        self.train_i = self.train_i[shuffled_idx]\n",
    "        self.train_r = self.train_r[shuffled_idx]\n",
    "        self.test_u, self.test_i, self.test_r = dataset.getInstances(dataset.testset, isTest=True)\n",
    "        assert(len(self.test_u) == len(self.test_i) and len(self.test_i) == len(self.test_r))\n",
    "        \n",
    "        #initialize\n",
    "        #K is number of latent factors\n",
    "        self.userLayer = [512, K]\n",
    "        self.itemLayer = [512, K]\n",
    "        self.batchSize = 1024\n",
    "        self.lr = 0.001\n",
    "        self.reg = 0.01\n",
    "        tf.reset_default_graph()\n",
    "        self._build_MLP(dataset)\n",
    "        self._init_sess()\n",
    "        \n",
    "    def _init_sess(self):\n",
    "        #define seesion\n",
    "        self.config = tf.ConfigProto()\n",
    "        self.config.gpu_options.allow_growth = True\n",
    "        self.config.allow_soft_placement = True\n",
    "        self.sess = tf.Session(config=self.config)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def _build_MLP(self, dataset):\n",
    "        print('start building the Multi-layer non-linear projection')\n",
    "        # add placeholder\n",
    "        self.user = tf.placeholder(tf.int32)\n",
    "        self.item = tf.placeholder(tf.int32)\n",
    "        self.rate = tf.placeholder(tf.float32)\n",
    "        user_item_embedding = tf.convert_to_tensor(dataset.list_to_matrix(dataset.trainset, dataset.maxu, dataset.maxi))\n",
    "        item_user_embedding = tf.transpose(user_item_embedding)\n",
    "        user_input = tf.nn.embedding_lookup(user_item_embedding, self.user)\n",
    "        item_input = tf.nn.embedding_lookup(item_user_embedding, self.item)\n",
    "        \n",
    "        def init_variable(shape, name):\n",
    "            return tf.Variable(tf.truncated_normal(shape=shape, dtype=tf.float32, stddev=0.01), name=name)\n",
    "\n",
    "        with tf.name_scope(\"User_Layer\"):\n",
    "            user_W1 = init_variable([self.shape[1], self.userLayer[0]], \"user_W1\")\n",
    "            user_out = tf.matmul(user_input, user_W1)\n",
    "            for i in range(0, len(self.userLayer)-1):\n",
    "                W = init_variable([self.userLayer[i], self.userLayer[i+1]], \"user_W\"+str(i+2))\n",
    "                b = init_variable([self.userLayer[i+1]], \"user_b\"+str(i+2))\n",
    "                user_out = tf.nn.relu(tf.add(tf.matmul(user_out, W), b))\n",
    "\n",
    "        with tf.name_scope(\"Item_Layer\"):\n",
    "            item_W1 = init_variable([self.shape[0], self.itemLayer[0]], \"item_W1\")\n",
    "            item_out = tf.matmul(item_input, item_W1)\n",
    "            for i in range(0, len(self.itemLayer)-1):\n",
    "                W = init_variable([self.itemLayer[i], self.itemLayer[i+1]], \"item_W\"+str(i+2))\n",
    "                b = init_variable([self.itemLayer[i+1]], \"item_b\"+str(i+2))\n",
    "                item_out = tf.nn.relu(tf.add(tf.matmul(item_out, W), b))\n",
    "           \n",
    "        self.r_ui = tf.reduce_sum(tf.multiply(user_out, item_out), axis=1, keepdims=False)\n",
    "        loss = tf.reduce_sum(tf.losses.mean_squared_error(labels = self.rate, predictions=self.r_ui))\n",
    "        regLoss = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()])#reguralizer\n",
    "        self.loss = loss + self.reg * regLoss\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_step = optimizer.minimize(self.loss)\n",
    "        print('done building the the Multi-layer non-linear projection')\n",
    "        \n",
    "    def _build_BPF(self):\n",
    "        print('start building the Bayesian probabilistic model')  \n",
    "        self.x_u = theano.shared(self.train_u)\n",
    "        self.x_i = theano.shared(self.train_i)\n",
    "        self.y_r = theano.shared(self.train_r)\n",
    "        self.y_r_ui = theano.shared(np.array(self.nn_r_ui))\n",
    "        assert(len(self.y_r.get_value())==len(self.y_r_ui.get_value()))\n",
    "        with pm.Model() as self.bncf:#define the prior and likelihood\n",
    "            b_u = pm.Normal('b_u', 0, sd=1/2, shape=self.shape[0])\n",
    "            b_i = pm.Normal('b_i', 0, sd=1/2, shape=self.shape[1])\n",
    "            u = pm.Normal('u', 0, sd=1/2)\n",
    "            tY = pm.Deterministic('tY', tt.add(tt.add(tt.add(b_u[self.x_u],b_i[self.x_i]),self.y_r_ui),u))\n",
    "            #tY = pm.Deterministic('tY', ((b_u[self.x_u]+b_i[self.x_i])+self.y_r_ui)+u)#b_u+b_i+u+nn_r_ui\n",
    "            nY = pm.Deterministic('nY', pm.math.sigmoid(tY))\n",
    "            # likelihood of observed data\n",
    "            Y = pm.Bernoulli('Y', nY, observed=self.y_r)#Categorical \n",
    "        with self.bncf:#inference\n",
    "            approx = pm.fit(n=10000, method=pm.ADVI())\n",
    "            self.trace = approx.sample(draws=3000)\n",
    "        with self.bncf: #posterior prediction\n",
    "            ppc = pm.sample_posterior_predictive(self.trace, progressbar=True)\n",
    "            self.by_r_ui = ppc['Y'].mean(axis=0)\n",
    "        print('done building the Bayesian probabilistic model')\n",
    "        \n",
    "    def train_BNMF(self, verbose=10):       \n",
    "        print('start training the BNCF model')\n",
    "        tstart = time.time()\n",
    "        \n",
    "        num_batches = len(self.train_u) // self.batchSize + 1\n",
    "        #1.traing r_ui in neural network \n",
    "        self.nn_r_ui=[]\n",
    "        for i in range(num_batches):\n",
    "            min_idx = i * self.batchSize\n",
    "            max_idx = np.min([len(self.train_u), (i+1)*self.batchSize])\n",
    "            train_u_batch = self.train_u[min_idx: max_idx]\n",
    "            train_i_batch = self.train_i[min_idx: max_idx]\n",
    "            #train_r_batch = self.train_r[min_idx: max_idx]\n",
    "            pre_r_ui_batch = self.sess.run(self.r_ui, feed_dict={self.user: train_u_batch, \\\n",
    "                                                                 self.item: train_i_batch})\n",
    "            self.nn_r_ui.extend(pre_r_ui_batch)\n",
    "            if verbose and i % verbose == 0:\n",
    "                sys.stdout.write('\\r{} / {} : shape = {} '.format(i, num_batches, len(self.nn_r_ui)))\n",
    "                sys.stdout.flush()\n",
    "        #2.training bias in Bayesian inference\n",
    "        self._build_BPF()\n",
    "        #3.training self.loss in neural network\n",
    "        losses = []\n",
    "        for i in range(num_batches):\n",
    "            min_idx = i * self.batchSize\n",
    "            max_idx = np.min([len(self.train_u), (i+1)*self.batchSize])\n",
    "            train_u_batch = self.train_u[min_idx: max_idx]\n",
    "            train_i_batch = self.train_i[min_idx: max_idx]\n",
    "            train_r_batch = self.by_r_ui[min_idx: max_idx]\n",
    "            _, tmp_loss = self.sess.run([self.train_step, self.loss], feed_dict={self.user: train_u_batch, \\\n",
    "                                                                                 self.item: train_i_batch, \\\n",
    "                                                                                 self.rate: train_r_batch})\n",
    "            losses.append(tmp_loss)\n",
    "            if verbose and i % verbose == 0:\n",
    "                sys.stdout.write('\\r{} / {} : loss = {}'.format(i, num_batches, np.mean(losses[-verbose:])))\n",
    "                sys.stdout.flush()\n",
    "        loss = np.mean(losses)  \n",
    "        elapsed = time.time() - tstart    \n",
    "        print('Completed training the BNCF model in %d seconds' % int(elapsed))\n",
    "        return loss\n",
    "           \n",
    "    def eval_BNMF(self, verbose=10):\n",
    "        def getHitRatio(ranklist, targetItem):\n",
    "            for item in ranklist:\n",
    "                if item == targetItem:\n",
    "                    return 1\n",
    "            return 0\n",
    "    \n",
    "        def getNDCG(ranklist, targetItem):\n",
    "            for i in range(len(ranklist)):\n",
    "                item = ranklist[i]\n",
    "                if item == targetItem:\n",
    "                    return math.log(2) / math.log(i+2)\n",
    "            return 0\n",
    "    \n",
    "        #1.get r_ui in neural network\n",
    "        num_batches = len(self.test_u) // self.batchSize + 1\n",
    "        #1.traing r_ui in neural network \n",
    "        self.nn_r_ui=[]\n",
    "        for i in range(num_batches):\n",
    "            min_idx = i * self.batchSize\n",
    "            max_idx = np.min([len(self.test_u), (i+1)*self.batchSize])\n",
    "            test_u_batch = self.test_u[min_idx: max_idx]\n",
    "            test_i_batch = self.test_i[min_idx: max_idx]\n",
    "            pre_r_ui_batch = self.sess.run(self.r_ui, feed_dict={self.user: test_u_batch, \\\n",
    "                                                                 self.item: test_i_batch})\n",
    "            self.nn_r_ui.extend(pre_r_ui_batch)\n",
    "            if verbose and i % verbose == 0:\n",
    "                sys.stdout.write('\\r{} / {} : shape = {} '.format(i, num_batches, len(self.nn_r_ui)))\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "        #2. get biais in Bayesian inference\n",
    "        self.x_u.set_value(self.test_u)\n",
    "        self.x_i.set_value(self.test_i)\n",
    "        self.y_r.set_value(self.test_r)\n",
    "        self.y_r_ui.set_value(self.nn_r_ui)\n",
    "        with self.bncf:#evaluation\n",
    "            ppc = pm.sample_posterior_predictive(self.trace, progressbar=True) \n",
    "            pre_r = ppc['Y'].mean(axis=0)\n",
    "        assert(pre_r.shape[0]==self.test_i.shape[0])\n",
    "        #every user have one positive item and 99 negative items\n",
    "        hrs = []\n",
    "        ndcgs = []\n",
    "        num_batches = len(self.test_i) // 100\n",
    "        for i in range(num_batches):\n",
    "            test_batch = self.test_i[i*100: (i+1)*100]\n",
    "            pre_r_batch = pre_r[i*100: (i+1)*100]\n",
    "            targetItem = int(test_batch[0])#positive item\n",
    "            map_item_score = {}\n",
    "            for j in range(100):\n",
    "                map_item_score[int(test_batch[j])] = pre_r_batch[j]\n",
    "            ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "            hrs.append(getHitRatio(ranklist, targetItem))\n",
    "            ndcgs.append(getNDCG(ranklist, targetItem))\n",
    "        hr, ndcg = np.array(hrs).mean(), np.array(ndcgs).mean()\n",
    "        return hr, ndcg\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    ds = DataSet()#loading dataset\n",
    "    for K in [8, 16, 32, 64]:\n",
    "        model = BNMF(ds, K, num_ng=2)\n",
    "        best_hr = 0.0\n",
    "        best_ndcg = 0.0\n",
    "        for epoch in range(2):\n",
    "            loss = model.train_BNMF()\n",
    "            print(\"\\nMean loss in this epoch is: {}\".format(loss))\n",
    "            hr, ndcg = model.eval_BNMF()\n",
    "            if hr>best_hr: best_hr=hr\n",
    "            if ndcg>best_ndcg: best_ndcg=ndcg\n",
    "        print(\"HR@10:{}, NDCG@10:{}, K:{}\".format(best_hr, best_ndcg, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
