{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Success!\n",
      "Data Info:\n",
      "\tUser Num: 6039\n",
      "\tItem Num: 3705\n",
      "\tData Size: 994169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0803 07:22:19.338432 140058288604928 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training!\n",
      "====================Epoch  0 ====================\n",
      "11650 / 11651 : loss = 84.43284606933594\n",
      "Mean loss in this epoch is: 92.08018493652344\n",
      "==================================================\n",
      "Start Evaluation!\n",
      "Epoch  0 HR: 0.6062261963901309, NDCG: 0.35002431600300526\n",
      "====================Epoch  0 End====================\n",
      "Best hr: 0.6062261963901309, NDCG: 0.35002431600300526, At Epoch 0\n",
      "Training complete!\n",
      "HR@10:0.6062261963901309, NDCG@10:0.35002431600300526, K:32\n",
      "Loading Success!\n",
      "Data Info:\n",
      "\tUser Num: 6039\n",
      "\tItem Num: 3705\n",
      "\tData Size: 994169\n",
      "Start Training!\n",
      "====================Epoch  0 ====================\n",
      "11650 / 11651 : loss = 85.46730041503906\n",
      "Mean loss in this epoch is: 91.33636474609375\n",
      "==================================================\n",
      "Start Evaluation!\n",
      "Epoch  0 HR: 0.610862725616824, NDCG: 0.35388135483940986\n",
      "====================Epoch  0 End====================\n",
      "Best hr: 0.610862725616824, NDCG: 0.35388135483940986, At Epoch 0\n",
      "Training complete!\n",
      "HR@10:0.610862725616824, NDCG@10:0.35388135483940986, K:64\n",
      "Loading Success!\n",
      "Data Info:\n",
      "\tUser Num: 6039\n",
      "\tItem Num: 3705\n",
      "\tData Size: 994169\n",
      "Start Training!\n",
      "====================Epoch  0 ====================\n",
      "11650 / 11651 : loss = 87.41201782226562\n",
      "Mean loss in this epoch is: 90.8943862915039\n",
      "==================================================\n",
      "Start Evaluation!\n",
      "Epoch  0 HR: 0.6221228680245073, NDCG: 0.3585229487988311\n",
      "====================Epoch  0 End====================\n",
      "Best hr: 0.6221228680245073, NDCG: 0.3585229487988311, At Epoch 0\n",
      "Training complete!\n",
      "HR@10:0.6221228680245073, NDCG@10:0.3585229487988311, K:80\n",
      "Loading Success!\n",
      "Data Info:\n",
      "\tUser Num: 6039\n",
      "\tItem Num: 3705\n",
      "\tData Size: 994169\n",
      "Start Training!\n",
      "====================Epoch  0 ====================\n",
      "11650 / 11651 : loss = 81.95731353759766\n",
      "Mean loss in this epoch is: 90.56356048583984\n",
      "==================================================\n",
      "Start Evaluation!\n",
      "Epoch  0 HR: 0.6154992548435171, NDCG: 0.3615592307119328\n",
      "====================Epoch  0 End====================\n",
      "Best hr: 0.6154992548435171, NDCG: 0.3615592307119328, At Epoch 0\n",
      "Training complete!\n",
      "HR@10:0.6154992548435171, NDCG@10:0.3615592307119328, K:96\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8  \n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.08.02\n",
    "@function: baseline: DMF(Deep Matrix Factorization Models for Recommender Systems.)\n",
    "           paper: https://www.ijcai.org/proceedings/2017/0447.pdf\n",
    "           Datatset: MovieLens-1m:https://grouplens.org/datasets/movielens/  \n",
    "           Evaluation: HR@10 NDCG@10\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import heapq\n",
    "import math\n",
    "import sys\n",
    "\n",
    "class DMF:\n",
    "    def __init__(self, K, negNum=2, lr=0.001, maxEpochs=20, topK=10):\n",
    "        #prepare data\n",
    "        self.dataSet = DataSet()\n",
    "        self.shape = self.dataSet.shape\n",
    "        self.maxRate = self.dataSet.maxRate\n",
    "\n",
    "        self.train = self.dataSet.train\n",
    "        self.testNeg = self.dataSet.getTestNeg()\n",
    "        \n",
    "        #initiate model\n",
    "        self.negNum = negNum\n",
    "        self.add_embedding_matrix()\n",
    "        self.add_placeholders()\n",
    "\n",
    "        self.userLayer = [512, K]\n",
    "        self.itemLayer = [512, K]\n",
    "        self.add_model()\n",
    "\n",
    "        self.add_loss()\n",
    "\n",
    "        self.lr = lr\n",
    "        self.add_train_step()\n",
    "        self.init_sess()\n",
    "\n",
    "        self.maxEpochs = maxEpochs\n",
    "        self.batchSize = 256\n",
    "        self.topK = topK\n",
    "        self.earlyStop = 5\n",
    "\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        self.user = tf.placeholder(tf.int32)\n",
    "        self.item = tf.placeholder(tf.int32)\n",
    "        self.rate = tf.placeholder(tf.float32)\n",
    "        self.drop = tf.placeholder(tf.float32)\n",
    "\n",
    "    def add_embedding_matrix(self):\n",
    "        self.user_item_embedding = tf.convert_to_tensor(self.dataSet.getEmbedding())\n",
    "        self.item_user_embedding = tf.transpose(self.user_item_embedding)\n",
    "\n",
    "    def add_model(self):\n",
    "        user_input = tf.nn.embedding_lookup(self.user_item_embedding, self.user)\n",
    "        item_input = tf.nn.embedding_lookup(self.item_user_embedding, self.item)\n",
    "\n",
    "        def init_variable(shape, name):\n",
    "            return tf.Variable(tf.truncated_normal(shape=shape, dtype=tf.float32, stddev=0.01), name=name)\n",
    "\n",
    "        with tf.name_scope(\"User_Layer\"):\n",
    "            user_W1 = init_variable([self.shape[1], self.userLayer[0]], \"user_W1\")\n",
    "            user_out = tf.matmul(user_input, user_W1)\n",
    "            for i in range(0, len(self.userLayer)-1):\n",
    "                W = init_variable([self.userLayer[i], self.userLayer[i+1]], \"user_W\"+str(i+2))\n",
    "                b = init_variable([self.userLayer[i+1]], \"user_b\"+str(i+2))\n",
    "                user_out = tf.nn.relu(tf.add(tf.matmul(user_out, W), b))\n",
    "\n",
    "        with tf.name_scope(\"Item_Layer\"):\n",
    "            item_W1 = init_variable([self.shape[0], self.itemLayer[0]], \"item_W1\")\n",
    "            item_out = tf.matmul(item_input, item_W1)\n",
    "            for i in range(0, len(self.itemLayer)-1):\n",
    "                W = init_variable([self.itemLayer[i], self.itemLayer[i+1]], \"item_W\"+str(i+2))\n",
    "                b = init_variable([self.itemLayer[i+1]], \"item_b\"+str(i+2))\n",
    "                item_out = tf.nn.relu(tf.add(tf.matmul(item_out, W), b))\n",
    "\n",
    "        norm_user_output = tf.sqrt(tf.reduce_sum(tf.square(user_out), axis=1))\n",
    "        norm_item_output = tf.sqrt(tf.reduce_sum(tf.square(item_out), axis=1))\n",
    "        self.y_ = tf.reduce_sum(tf.multiply(user_out, item_out), axis=1, keepdims=False) / (norm_item_output* norm_user_output)\n",
    "        self.y_ = tf.maximum(1e-6, self.y_)\n",
    "\n",
    "    def add_loss(self):\n",
    "        regRate = self.rate / self.maxRate\n",
    "        losses = regRate * tf.log(self.y_) + (1 - regRate) * tf.log(1 - self.y_)\n",
    "        loss = -tf.reduce_sum(losses)\n",
    "        # regLoss = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()])\n",
    "        # self.loss = loss + self.reg * regLoss\n",
    "        self.loss = loss\n",
    "\n",
    "    def add_train_step(self):\n",
    "        '''\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        self.lr = tf.train.exponential_decay(self.lr, global_step,\n",
    "                                             self.decay_steps, self.decay_rate, staircase=True)\n",
    "        '''\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_step = optimizer.minimize(self.loss)\n",
    "\n",
    "    def init_sess(self):\n",
    "        self.config = tf.ConfigProto()\n",
    "        self.config.gpu_options.allow_growth = True\n",
    "        self.config.allow_soft_placement = True\n",
    "        self.sess = tf.Session(config=self.config)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def run(self):\n",
    "        best_hr = -1\n",
    "        best_NDCG = -1\n",
    "        best_epoch = -1\n",
    "        print(\"Start Training!\")\n",
    "        for epoch in range(self.maxEpochs):\n",
    "            print(\"=\"*20+\"Epoch \", epoch, \"=\"*20)\n",
    "            self.run_epoch(self.sess)\n",
    "            print('='*50)\n",
    "            print(\"Start Evaluation!\")\n",
    "            hr, NDCG = self.evaluate(self.sess, self.topK)\n",
    "            print(\"Epoch \", epoch, \"HR: {}, NDCG: {}\".format(hr, NDCG))\n",
    "            if hr > best_hr or NDCG > best_NDCG:\n",
    "                best_hr = hr\n",
    "                best_NDCG = NDCG\n",
    "                best_epoch = epoch\n",
    "            if epoch - best_epoch > self.earlyStop:\n",
    "                print(\"Normal Early stop!\")\n",
    "                break\n",
    "            print(\"=\"*20+\"Epoch \", epoch, \"End\"+\"=\"*20)\n",
    "        print(\"Best hr: {}, NDCG: {}, At Epoch {}\".format(best_hr, best_NDCG, best_epoch))\n",
    "        print(\"Training complete!\")\n",
    "        return best_hr, best_NDCG\n",
    "\n",
    "    def run_epoch(self, sess, verbose=10):\n",
    "        train_u, train_i, train_r = self.dataSet.getInstances(self.train, self.negNum)\n",
    "        train_len = len(train_u)\n",
    "        shuffled_idx = np.random.permutation(np.arange(train_len))\n",
    "        train_u = train_u[shuffled_idx]\n",
    "        train_i = train_i[shuffled_idx]\n",
    "        train_r = train_r[shuffled_idx]\n",
    "\n",
    "        num_batches = len(train_u) // self.batchSize + 1\n",
    "\n",
    "        losses = []\n",
    "        for i in range(num_batches):\n",
    "            min_idx = i * self.batchSize\n",
    "            max_idx = np.min([train_len, (i+1)*self.batchSize])\n",
    "            train_u_batch = train_u[min_idx: max_idx]\n",
    "            train_i_batch = train_i[min_idx: max_idx]\n",
    "            train_r_batch = train_r[min_idx: max_idx]\n",
    "            \n",
    "            feed_dict = self.create_feed_dict(train_u_batch, train_i_batch, train_r_batch)\n",
    "            _, tmp_loss = sess.run([self.train_step, self.loss], feed_dict=feed_dict)\n",
    "            losses.append(tmp_loss)\n",
    "            if verbose and i % verbose == 0:\n",
    "                sys.stdout.write('\\r{} / {} : loss = {}'.format(i, num_batches, np.mean(losses[-verbose:])))\n",
    "                sys.stdout.flush()\n",
    "        loss = np.mean(losses)\n",
    "        print(\"\\nMean loss in this epoch is: {}\".format(loss))\n",
    "        return loss\n",
    "\n",
    "    def create_feed_dict(self, u, i, r=None, drop=None):\n",
    "        return {self.user: u,\n",
    "                self.item: i,\n",
    "                self.rate: r,\n",
    "                self.drop: drop}\n",
    "\n",
    "    def evaluate(self, sess, topK):\n",
    "        def getHitRatio(ranklist, targetItem):\n",
    "            for item in ranklist:\n",
    "                if item == targetItem:\n",
    "                    return 1\n",
    "            return 0\n",
    "        def getNDCG(ranklist, targetItem):\n",
    "            for i in range(len(ranklist)):\n",
    "                item = ranklist[i]\n",
    "                if item == targetItem:\n",
    "                    return math.log(2) / math.log(i+2)\n",
    "            return 0\n",
    "\n",
    "\n",
    "        hr =[]\n",
    "        NDCG = []\n",
    "        testUser = self.testNeg[0]\n",
    "        testItem = self.testNeg[1]\n",
    "        for i in range(len(testUser)):\n",
    "            target = testItem[i][0]\n",
    "            feed_dict = self.create_feed_dict(testUser[i], testItem[i])\n",
    "            predict = sess.run(self.y_, feed_dict=feed_dict)\n",
    "\n",
    "            item_score_dict = {}\n",
    "\n",
    "            for j in range(len(testItem[i])):\n",
    "                item = testItem[i][j]\n",
    "                item_score_dict[item] = predict[j]\n",
    "\n",
    "            ranklist = heapq.nlargest(topK, item_score_dict, key=item_score_dict.get)\n",
    "\n",
    "            tmp_hr = getHitRatio(ranklist, target)\n",
    "            tmp_NDCG = getNDCG(ranklist, target)\n",
    "            hr.append(tmp_hr)\n",
    "            NDCG.append(tmp_NDCG)\n",
    "        return np.mean(hr), np.mean(NDCG)\n",
    "\n",
    "class DataSet(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train, self.shape = self.getTrainData()\n",
    "        self.trainDict = self.getTrainDict()\n",
    "        \n",
    "    def getTrainData(self):\n",
    "        data = []\n",
    "        filePath = '/data/fjsdata/BNMF/ml-1m.train.rating'\n",
    "        u = 0\n",
    "        i = 0\n",
    "        maxr = 0.0\n",
    "        with open(filePath, 'r') as f:\n",
    "            for line in f:\n",
    "                if line:\n",
    "                    lines = line[:-1].split(\"\\t\")\n",
    "                    user = int(lines[0])\n",
    "                    movie = int(lines[1])\n",
    "                    score = float(lines[2])\n",
    "                    data.append((user, movie, score))\n",
    "                    if user > u:u = user\n",
    "                    if movie > i:i = movie\n",
    "                    if score > maxr:maxr = score\n",
    "        self.maxRate = maxr\n",
    "        print(\"Loading Success!\\n\"\n",
    "                  \"Data Info:\\n\"\n",
    "                  \"\\tUser Num: {}\\n\"\n",
    "                  \"\\tItem Num: {}\\n\"\n",
    "                  \"\\tData Size: {}\".format(u, i, len(data)))\n",
    "        return data, [u+1, i+1]\n",
    "\n",
    "    def getTrainDict(self):\n",
    "        dataDict = {}\n",
    "        for i in self.train:\n",
    "            dataDict[(i[0], i[1])] = i[2]\n",
    "        return dataDict\n",
    "\n",
    "    def getEmbedding(self):\n",
    "        train_matrix = np.zeros([self.shape[0], self.shape[1]], dtype=np.float32)\n",
    "        for i in self.train:\n",
    "            user = i[0]\n",
    "            movie = i[1]\n",
    "            rating = i[2]\n",
    "            train_matrix[user][movie] = rating\n",
    "        return np.array(train_matrix)\n",
    "\n",
    "    def getInstances(self, data, negNum):\n",
    "        user = []\n",
    "        item = []\n",
    "        rate = []\n",
    "        for i in data:\n",
    "            user.append(i[0])\n",
    "            item.append(i[1])\n",
    "            rate.append(i[2])\n",
    "            for t in range(negNum):\n",
    "                j = np.random.randint(self.shape[1])\n",
    "                while (i[0], j) in self.trainDict:\n",
    "                    j = np.random.randint(self.shape[1])\n",
    "                user.append(i[0])\n",
    "                item.append(j)\n",
    "                rate.append(0.0)\n",
    "        return np.array(user), np.array(item), np.array(rate)\n",
    "\n",
    "    def getTestNeg(self):\n",
    "        #loading data\n",
    "        testset = []\n",
    "        filePath = '/data/fjsdata/BNMF/ml-1m.test.negative'\n",
    "        with open(filePath, 'r') as fd:\n",
    "            line = fd.readline()\n",
    "            while line != None and line != '':\n",
    "                arr = line.split('\\t')\n",
    "                u = eval(arr[0])[0]\n",
    "                testset.append([u, eval(arr[0])[1]])#one postive item\n",
    "                for i in arr[1:]:\n",
    "                    testset.append([u, int(i)]) #99 negative items\n",
    "                line = fd.readline()\n",
    "        #format    \n",
    "        user = []\n",
    "        item = []\n",
    "        u_prev = testset[0][0]\n",
    "        tmp_user = []\n",
    "        tmp_item = []\n",
    "        for u, i in testset:\n",
    "            if u_prev ==u:\n",
    "                tmp_user.append(u)\n",
    "                tmp_item.append(i)\n",
    "            else:\n",
    "                user.append(tmp_user)\n",
    "                item.append(tmp_item)\n",
    "                tmp_user = []\n",
    "                tmp_item = []\n",
    "                tmp_user.append(u)\n",
    "                tmp_item.append(i)\n",
    "            u_prev = u\n",
    "        return [np.array(user), np.array(item)]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    for K in [32, 64, 80, 96]:\n",
    "        dmf = DMF(K, negNum=2, lr=0.001, maxEpochs=1, topK=10)\n",
    "        best_hr, best_ndcg = dmf.run()\n",
    "        print(\"HR@10:{}, NDCG@10:{}, K:{}\".format(best_hr, best_ndcg, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Interaction = 994169, User = 6040, Item = 3706, Sparsity = 0.0444\n",
      "Start Training!\n",
      "====================Epoch  0 ====================\n",
      "2910 / 2913 : loss = 4.7838525772094735\n",
      "Mean loss in this epoch is: 5.296616077423096\n",
      "==================================================\n",
      "Start Evaluation!\n",
      "Epoch  0 HR: 0.8855960264900662, NDCG: 0.83441029255177\n",
      "====================Epoch  0 End====================\n",
      "Best hr: 0.8855960264900662, NDCG: 0.83441029255177, At Epoch 0\n",
      "Training complete!\n",
      "HR@10:0.8855960264900662, NDCG@10:0.83441029255177, K:8\n",
      "Dataset Statistics: Interaction = 994169, User = 6040, Item = 3706, Sparsity = 0.0444\n",
      "Start Training!\n",
      "====================Epoch  0 ====================\n",
      "2910 / 2913 : loss = 4.6475491523742685\n",
      "Mean loss in this epoch is: 5.0844807624816895\n",
      "==================================================\n",
      "Start Evaluation!\n",
      "Epoch  0 HR: 0.9697019867549669, NDCG: 0.37110761319045754\n",
      "====================Epoch  0 End====================\n",
      "Best hr: 0.9697019867549669, NDCG: 0.37110761319045754, At Epoch 0\n",
      "Training complete!\n",
      "HR@10:0.9697019867549669, NDCG@10:0.37110761319045754, K:16\n",
      "Dataset Statistics: Interaction = 994169, User = 6040, Item = 3706, Sparsity = 0.0444\n",
      "Start Training!\n",
      "====================Epoch  0 ====================\n",
      "2910 / 2913 : loss = 4.8199377059936525\n",
      "Mean loss in this epoch is: 5.791607856750488\n",
      "==================================================\n",
      "Start Evaluation!\n",
      "Epoch  0 HR: 0.9995033112582782, NDCG: 0.4780701864012833\n",
      "====================Epoch  0 End====================\n",
      "Best hr: 0.9995033112582782, NDCG: 0.4780701864012833, At Epoch 0\n",
      "Training complete!\n",
      "HR@10:0.9995033112582782, NDCG@10:0.4780701864012833, K:32\n",
      "Dataset Statistics: Interaction = 994169, User = 6040, Item = 3706, Sparsity = 0.0444\n",
      "Start Training!\n",
      "====================Epoch  0 ====================\n",
      "2910 / 2913 : loss = 4.5654296875551375\n",
      "Mean loss in this epoch is: 5.6663923263549805\n",
      "==================================================\n",
      "Start Evaluation!\n",
      "Epoch  0 HR: 1.0, NDCG: 1.0\n",
      "====================Epoch  0 End====================\n",
      "Best hr: 1.0, NDCG: 1.0, At Epoch 0\n",
      "Training complete!\n",
      "HR@10:1.0, NDCG@10:1.0, K:64\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8  \n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.08.03\n",
    "@function: baseline: DMF(Deep Matrix Factorization Models for Recommender Systems.)\n",
    "           paper: https://www.ijcai.org/proceedings/2017/0447.pdf\n",
    "           Datatset: MovieLens-1m:https://grouplens.org/datasets/movielens/  \n",
    "           Evaluation: HR@10 NDCG@10\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import heapq\n",
    "import math\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "class DMF:\n",
    "    def __init__(self, K, negNum=2, lr=0.001, maxEpochs=20, topK=10):\n",
    "        #prepare data\n",
    "        self.dataSet = DataSet()\n",
    "        self.shape = [self.dataSet.maxu, self.dataSet.maxi]\n",
    "\n",
    "        self.train = self.dataSet.trainset\n",
    "        self.testNeg = self.dataSet.getTestNeg()\n",
    "        \n",
    "        #initiate model\n",
    "        self.negNum = negNum\n",
    "        self.add_embedding_matrix()\n",
    "        self.add_placeholders()\n",
    "\n",
    "        self.userLayer = [512, K]\n",
    "        self.itemLayer = [512, K]\n",
    "        self.lr = lr\n",
    "        self.add_model()\n",
    "        self.init_sess()\n",
    "\n",
    "        self.maxEpochs = maxEpochs\n",
    "        self.batchSize = 1024\n",
    "        self.topK = topK\n",
    "        self.earlyStop = 5\n",
    "\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        self.user = tf.placeholder(tf.int32)\n",
    "        self.item = tf.placeholder(tf.int32)\n",
    "        self.rate = tf.placeholder(tf.float32)\n",
    "        self.drop = tf.placeholder(tf.float32)\n",
    "\n",
    "    def add_embedding_matrix(self):\n",
    "        self.user_item_embedding = tf.convert_to_tensor(self.dataSet.getEmbedding())\n",
    "        self.item_user_embedding = tf.transpose(self.user_item_embedding)\n",
    "\n",
    "    def add_model(self):\n",
    "        user_input = tf.nn.embedding_lookup(self.user_item_embedding, self.user)\n",
    "        item_input = tf.nn.embedding_lookup(self.item_user_embedding, self.item)\n",
    "\n",
    "        def init_variable(shape, name):\n",
    "            return tf.Variable(tf.truncated_normal(shape=shape, dtype=tf.float32, stddev=0.01), name=name)\n",
    "\n",
    "        with tf.name_scope(\"User_Layer\"):\n",
    "            user_W1 = init_variable([self.shape[1], self.userLayer[0]], \"user_W1\")\n",
    "            user_out = tf.matmul(user_input, user_W1)\n",
    "            for i in range(0, len(self.userLayer)-1):\n",
    "                W = init_variable([self.userLayer[i], self.userLayer[i+1]], \"user_W\"+str(i+2))\n",
    "                b = init_variable([self.userLayer[i+1]], \"user_b\"+str(i+2))\n",
    "                user_out = tf.nn.relu(tf.add(tf.matmul(user_out, W), b))\n",
    "\n",
    "        with tf.name_scope(\"Item_Layer\"):\n",
    "            item_W1 = init_variable([self.shape[0], self.itemLayer[0]], \"item_W1\")\n",
    "            item_out = tf.matmul(item_input, item_W1)\n",
    "            for i in range(0, len(self.itemLayer)-1):\n",
    "                W = init_variable([self.itemLayer[i], self.itemLayer[i+1]], \"item_W\"+str(i+2))\n",
    "                b = init_variable([self.itemLayer[i+1]], \"item_b\"+str(i+2))\n",
    "                item_out = tf.nn.relu(tf.add(tf.matmul(item_out, W), b))\n",
    "                \n",
    "        self.y_ = tf.reduce_sum(tf.multiply(user_out, item_out), axis=1, keepdims=False)\n",
    "        self.loss = tf.reduce_sum(tf.losses.mean_squared_error(labels = self.rate, predictions=self.y_))\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_step = optimizer.minimize(self.loss)\n",
    "\n",
    "    def init_sess(self):\n",
    "        self.config = tf.ConfigProto()\n",
    "        self.config.gpu_options.allow_growth = True\n",
    "        self.config.allow_soft_placement = True\n",
    "        self.sess = tf.Session(config=self.config)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def run(self):\n",
    "        best_hr = -1\n",
    "        best_NDCG = -1\n",
    "        best_epoch = -1\n",
    "        print(\"Start Training!\")\n",
    "        for epoch in range(self.maxEpochs):\n",
    "            print(\"=\"*20+\"Epoch \", epoch, \"=\"*20)\n",
    "            self.run_epoch(self.sess)\n",
    "            print('='*50)\n",
    "            print(\"Start Evaluation!\")\n",
    "            hr, NDCG = self.evaluate(self.sess, self.topK)\n",
    "            print(\"Epoch \", epoch, \"HR: {}, NDCG: {}\".format(hr, NDCG))\n",
    "            if hr > best_hr or NDCG > best_NDCG:\n",
    "                best_hr = hr\n",
    "                best_NDCG = NDCG\n",
    "                best_epoch = epoch\n",
    "            if epoch - best_epoch > self.earlyStop:\n",
    "                print(\"Normal Early stop!\")\n",
    "                break\n",
    "            print(\"=\"*20+\"Epoch \", epoch, \"End\"+\"=\"*20)\n",
    "        print(\"Best hr: {}, NDCG: {}, At Epoch {}\".format(best_hr, best_NDCG, best_epoch))\n",
    "        print(\"Training complete!\")\n",
    "        return best_hr, best_NDCG\n",
    "\n",
    "    def run_epoch(self, sess, verbose=10):\n",
    "        train_u, train_i, train_r = self.dataSet.getInstances(self.train, self.negNum)\n",
    "        train_len = len(train_u)\n",
    "        shuffled_idx = np.random.permutation(np.arange(train_len))\n",
    "        train_u = train_u[shuffled_idx]\n",
    "        train_i = train_i[shuffled_idx]\n",
    "        train_r = train_r[shuffled_idx]\n",
    "\n",
    "        num_batches = len(train_u) // self.batchSize + 1\n",
    "\n",
    "        losses = []\n",
    "        for i in range(num_batches):\n",
    "            min_idx = i * self.batchSize\n",
    "            max_idx = np.min([train_len, (i+1)*self.batchSize])\n",
    "            train_u_batch = train_u[min_idx: max_idx]\n",
    "            train_i_batch = train_i[min_idx: max_idx]\n",
    "            train_r_batch = train_r[min_idx: max_idx]\n",
    "            \n",
    "            feed_dict = self.create_feed_dict(train_u_batch, train_i_batch, train_r_batch)\n",
    "            _, tmp_loss = sess.run([self.train_step, self.loss], feed_dict=feed_dict)\n",
    "            losses.append(tmp_loss)\n",
    "            if verbose and i % verbose == 0:\n",
    "                sys.stdout.write('\\r{} / {} : loss = {}'.format(i, num_batches, np.mean(losses[-verbose:])))\n",
    "                sys.stdout.flush()\n",
    "        loss = np.mean(losses)\n",
    "        print(\"\\nMean loss in this epoch is: {}\".format(loss))\n",
    "        return loss\n",
    "\n",
    "    def create_feed_dict(self, u, i, r=None, drop=None):\n",
    "        return {self.user: u,\n",
    "                self.item: i,\n",
    "                self.rate: r,\n",
    "                self.drop: drop}\n",
    "\n",
    "    def evaluate(self, sess, topK):\n",
    "        def getHitRatio(ranklist, targetItem):\n",
    "            for item in ranklist:\n",
    "                if item == targetItem:\n",
    "                    return 1\n",
    "            return 0\n",
    "        def getNDCG(ranklist, targetItem):\n",
    "            for i in range(len(ranklist)):\n",
    "                item = ranklist[i]\n",
    "                if item == targetItem:\n",
    "                    return math.log(2) / math.log(i+2)\n",
    "            return 0\n",
    "\n",
    "\n",
    "        hr =[]\n",
    "        NDCG = []\n",
    "        testUser = self.testNeg[0]\n",
    "        testItem = self.testNeg[1]\n",
    "        for i in range(len(testUser)):\n",
    "            target = testItem[i][0]\n",
    "            feed_dict = self.create_feed_dict(testUser[i], testItem[i])\n",
    "            predict = sess.run(self.y_, feed_dict=feed_dict)\n",
    "\n",
    "            item_score_dict = {}\n",
    "            for j in range(len(testItem[i])):\n",
    "                item = testItem[i][j]\n",
    "                item_score_dict[item] = predict[j]\n",
    "\n",
    "            ranklist = heapq.nlargest(topK, item_score_dict, key=item_score_dict.get)\n",
    "\n",
    "            tmp_hr = getHitRatio(ranklist, target)\n",
    "            tmp_NDCG = getNDCG(ranklist, target)\n",
    "            hr.append(tmp_hr)\n",
    "            NDCG.append(tmp_NDCG)\n",
    "        return np.mean(hr), np.mean(NDCG)\n",
    "\n",
    "class DataSet(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.trainset, self.testset, self.maxu, self.maxi, self.maxr = self._getDataset_as_list()\n",
    "        self.trainDict = self.getTrainDict()\n",
    "    \n",
    "    def _getDataset_as_list(self):\n",
    "        #trainset\n",
    "        filePath = \"/data/fjsdata/BMF/ml-1m.train.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        maxu, maxi, maxr = data['user'].max()+1, data['item'].max()+1, data['rating'].max()\n",
    "        print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "        trainset = data.values.tolist()\n",
    "        #testset\n",
    "        filePath = \"/data/fjsdata/BMF/ml-1m.test.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        testset = data.values.tolist()\n",
    "        return trainset, testset, maxu, maxi, maxr \n",
    "\n",
    "    def getTrainDict(self):\n",
    "        dataDict = {}\n",
    "        for i in self.trainset:\n",
    "            dataDict[(i[0], i[1])] = i[2]\n",
    "        return dataDict\n",
    "\n",
    "    def getEmbedding(self):\n",
    "        train_matrix = np.zeros([self.maxu, self.maxi], dtype=np.float32)\n",
    "        for i in self.trainset:\n",
    "            user = int(i[0])\n",
    "            movie = int(i[1])\n",
    "            rating = float(i[2])\n",
    "            train_matrix[user][movie] = rating\n",
    "        return np.array(train_matrix)\n",
    "\n",
    "    def getInstances(self, data, negNum):\n",
    "        user = []\n",
    "        item = []\n",
    "        rate = []\n",
    "        for i in data:\n",
    "            user.append(i[0])\n",
    "            item.append(i[1])\n",
    "            rate.append(i[2])\n",
    "            for t in range(negNum):\n",
    "                j = np.random.randint(self.maxi)\n",
    "                while (i[0], j) in self.trainDict:\n",
    "                    j = np.random.randint(self.maxi)\n",
    "                user.append(i[0])\n",
    "                item.append(j)\n",
    "                rate.append(0.0)\n",
    "        return np.array(user), np.array(item), np.array(rate)\n",
    "\n",
    "    def getTestNeg(self, negNum=99):\n",
    "        user = []\n",
    "        item = []\n",
    "        for s in self.testset:\n",
    "            tmp_user = []\n",
    "            tmp_item = []\n",
    "            u = s[0]\n",
    "            i = s[1]\n",
    "            tmp_user.append(u)\n",
    "            tmp_item.append(i)\n",
    "            neglist = set()\n",
    "            neglist.add(i)\n",
    "            for t in range(negNum):\n",
    "                j = np.random.randint(self.maxi)\n",
    "                while (u, j) in self.trainDict or j in neglist:\n",
    "                    j = np.random.randint(self.maxi)\n",
    "                neglist.add(j)\n",
    "                tmp_user.append(u)\n",
    "                tmp_item.append(j)\n",
    "            user.append(tmp_user)\n",
    "            item.append(tmp_item)\n",
    "        return [np.array(user), np.array(item)]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    for K in [8, 16, 32, 64]:\n",
    "        dmf = DMF(K, negNum=2, lr=0.001, maxEpochs=1, topK=10)\n",
    "        best_hr, best_ndcg = dmf.run()\n",
    "        print(\"HR@10:{}, NDCG@10:{}, K:{}\".format(best_hr, best_ndcg, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
