{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Interaction = 994169, User = 6040, Item = 3706, Sparsity = 0.0444\n",
      "RMSE@8@100:1.1376623384284457\n",
      "RMSE@8@200:1.0135008650918558\n",
      "RMSE@8@300:0.9698640179114743\n",
      "RMSE@8@400:0.9528893325548711\n",
      "RMSE@8@500:0.9419879920471648\n",
      "RMSE@8@600:0.9299279277746\n",
      "RMSE@8@700:0.9259939707052821\n",
      "RMSE@8@800:0.9221955920175581\n",
      "RMSE@8@900:0.9183238117807937\n",
      "RMSE@8@1000:0.9141626907266064\n",
      "RMSE@16@100:1.1296926038985662\n",
      "RMSE@16@200:1.0105795073425912\n",
      "RMSE@16@300:0.9704869531218405\n",
      "RMSE@16@400:0.9474916677327472\n",
      "RMSE@16@500:0.9347607561889565\n",
      "RMSE@16@600:0.9244573851405132\n",
      "RMSE@16@700:0.9194894231817686\n",
      "RMSE@16@800:0.9155865048803947\n",
      "RMSE@16@900:0.9147120256150667\n",
      "RMSE@16@1000:0.9124669663334384\n",
      "RMSE@32@100:1.1195218790514974\n",
      "RMSE@32@200:1.002633047518133\n",
      "RMSE@32@300:0.9641656688996694\n",
      "RMSE@32@400:0.9450634928825616\n",
      "RMSE@32@500:0.930727111798766\n",
      "RMSE@32@600:0.925714539052937\n",
      "RMSE@32@700:0.9232112022218245\n",
      "RMSE@32@800:0.9122104202037624\n",
      "RMSE@32@900:0.9102472162807341\n",
      "RMSE@32@1000:0.9093776549052565\n",
      "RMSE@64@100:1.106664512507401\n",
      "RMSE@64@200:0.9963026370732705\n",
      "RMSE@64@300:0.961564062653251\n",
      "RMSE@64@400:0.9434418142107381\n",
      "RMSE@64@500:0.9280646636372134\n",
      "RMSE@64@600:0.9231791840511395\n",
      "RMSE@64@700:0.9147269797103638\n",
      "RMSE@64@800:0.9126398638300793\n",
      "RMSE@64@900:0.9122147081559854\n",
      "RMSE@64@1000:0.9059066338226208\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8  \n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.08.13\n",
    "@function: baseline: PMF(Probabilistic Matrix Factorization)\n",
    "           paper: http://papers.nips.cc/paper/3208-probabilistic-matrix-factorization.pdf\n",
    "           Datatset: MovieLens-1m:https://grouplens.org/datasets/movielens/  \n",
    "           Evaluation: RMSE\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import heapq\n",
    "import math\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from numpy.random import RandomState\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import tensorflow as tf\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self):\n",
    "        self.trainset, self.testset, self.maxu, self.maxi, self.maxr = self._getDataset_as_list()\n",
    "        \n",
    "    def _getDataset_as_list(self):\n",
    "        #trainset\n",
    "        filePath = \"/data/fjsdata/BMF/ml-1m.train.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        maxu, maxi, maxr = data['user'].max()+1, data['item'].max()+1, data['rating'].max()\n",
    "        print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "        trainset = data.values.tolist()\n",
    "        #testset\n",
    "        filePath = \"/data/fjsdata/BMF/ml-1m.test.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        testset = data.values.tolist()\n",
    "        return trainset, testset, maxu, maxi, maxr \n",
    "    \n",
    "    def list_to_matrix(self, dataset, maxu, maxi):              \n",
    "        dataMat = np.zeros([maxu, maxi], dtype=np.float32)\n",
    "        for u,i,r in dataset:\n",
    "            dataMat[int(u)][int(i)] = float(r)\n",
    "        return np.array(dataMat)\n",
    "    \n",
    "    def list_to_dict(self, dataset):\n",
    "        dataDict = {}\n",
    "        for u,i,r in dataset:\n",
    "            dataDict[int(u), int(i)] = float(r)\n",
    "        return dataDict\n",
    "    \n",
    "class PMF:\n",
    "    def __init__(self, num_feat=8, epsilon=1, _lambda=0.1, momentum=0.9, maxepoch=200, num_batches=10, batch_size=1000):\n",
    "        self.num_feat = num_feat\n",
    "        self.epsilon = epsilon\n",
    "        self._lambda = _lambda\n",
    "        self.momentum = momentum\n",
    "        self.maxepoch = maxepoch\n",
    "        self.num_batches = num_batches\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.w_C = None\n",
    "        self.w_I = None\n",
    "\n",
    "        self.err_train = []\n",
    "        self.err_val = []\n",
    "        \n",
    "    def fit(self, train_vec, val_vec):   \n",
    "        # mean subtraction\n",
    "        self.mean_inv = np.mean(train_vec[:,2])\n",
    "        \n",
    "        pairs_tr = train_vec.shape[0]\n",
    "        pairs_va = val_vec.shape[0]\n",
    "        \n",
    "        # 1-p-i, 2-m-c\n",
    "        num_inv = int(max(np.amax(train_vec[:,0]), np.amax(val_vec[:,0]))) + 1\n",
    "        num_com = int(max(np.amax(train_vec[:,1]), np.amax(val_vec[:,1]))) + 1\n",
    "\n",
    "        incremental = False\n",
    "        if ((not incremental) or (self.w_C is None)):\n",
    "            # initialize\n",
    "            self.epoch = 0\n",
    "            self.w_C = 0.1 * np.random.randn(num_com, self.num_feat)\n",
    "            self.w_I = 0.1 * np.random.randn(num_inv, self.num_feat)\n",
    "            \n",
    "            self.w_C_inc = np.zeros((num_com, self.num_feat))\n",
    "            self.w_I_inc = np.zeros((num_inv, self.num_feat))\n",
    "        \n",
    "        \n",
    "        while self.epoch < self.maxepoch:\n",
    "            self.epoch += 1\n",
    "\n",
    "            # Shuffle training truples\n",
    "            shuffled_order = np.arange(train_vec.shape[0])\n",
    "            np.random.shuffle(shuffled_order)\n",
    "\n",
    "            # Batch update\n",
    "            for batch in range(self.num_batches):\n",
    "                # print \"epoch %d batch %d\" % (self.epoch, batch+1)\n",
    "\n",
    "                batch_idx = np.mod(np.arange(self.batch_size * batch,\n",
    "                                             self.batch_size * (batch+1)),\n",
    "                                   shuffled_order.shape[0])\n",
    "\n",
    "                batch_invID = np.array(train_vec[shuffled_order[batch_idx], 0], dtype='int32')\n",
    "                batch_comID = np.array(train_vec[shuffled_order[batch_idx], 1], dtype='int32')\n",
    "\n",
    "                # Compute Objective Function\n",
    "                pred_out = np.sum(np.multiply(self.w_I[batch_invID,:], \n",
    "                                                self.w_C[batch_comID,:]),\n",
    "                                axis=1) # mean_inv subtracted\n",
    "\n",
    "                rawErr = pred_out - train_vec[shuffled_order[batch_idx], 2] + self.mean_inv\n",
    "\n",
    "                # Compute gradients\n",
    "                Ix_C = 2 * np.multiply(rawErr[:, np.newaxis], self.w_I[batch_invID,:]) \\\n",
    "                        + self._lambda * self.w_C[batch_comID,:]\n",
    "                Ix_I = 2 * np.multiply(rawErr[:, np.newaxis], self.w_C[batch_comID,:]) \\\n",
    "                        + self._lambda * self.w_I[batch_invID,:]\n",
    "            \n",
    "                dw_C = np.zeros((num_com, self.num_feat))\n",
    "                dw_I = np.zeros((num_inv, self.num_feat))\n",
    "\n",
    "                # loop to aggreate the gradients of the same element\n",
    "                for i in range(self.batch_size):\n",
    "                    dw_C[batch_comID[i],:] += Ix_C[i,:]\n",
    "                    dw_I[batch_invID[i],:] += Ix_I[i,:]\n",
    "\n",
    "\n",
    "                # Update with momentum\n",
    "                self.w_C_inc = self.momentum * self.w_C_inc + self.epsilon * dw_C / self.batch_size\n",
    "                self.w_I_inc = self.momentum * self.w_I_inc + self.epsilon * dw_I / self.batch_size\n",
    "\n",
    "\n",
    "                self.w_C = self.w_C - self.w_C_inc\n",
    "                self.w_I = self.w_I - self.w_I_inc\n",
    "\n",
    "                # Compute Objective Function after\n",
    "                if batch == self.num_batches - 1:\n",
    "                    pred_out = np.sum(np.multiply(self.w_I[np.array(train_vec[:,0], dtype='int32'),:],\n",
    "                                                    self.w_C[np.array(train_vec[:,1], dtype='int32'),:]),\n",
    "                                        axis=1) # mean_inv subtracted\n",
    "                    rawErr = pred_out - train_vec[:, 2] + self.mean_inv\n",
    "                    obj = LA.norm(rawErr) ** 2 \\\n",
    "                            + 0.5*self._lambda*(LA.norm(self.w_I) ** 2 + LA.norm(self.w_C) ** 2)\n",
    "\n",
    "                    self.err_train.append(np.sqrt(obj/pairs_tr))\n",
    "\n",
    "                # Compute validation error\n",
    "                if batch == self.num_batches - 1:\n",
    "                    pred_out = np.sum(np.multiply(self.w_I[np.array(val_vec[:,0], dtype='int32'),:],\n",
    "                                                    self.w_C[np.array(val_vec[:,1], dtype='int32'),:]),\n",
    "                                        axis=1) # mean_inv subtracted\n",
    "                    rawErr = pred_out - val_vec[:, 2] + self.mean_inv\n",
    "                    self.err_val.append(LA.norm(rawErr)/np.sqrt(pairs_va))\n",
    "\n",
    "                # Print info\n",
    "                #if batch == self.num_batches - 1:\n",
    "                    #print ('Training RMSE: %f, Val RMSE %f' % (self.err_train[-1], self.err_val[-1]))\n",
    "    \n",
    "    def predict(self, invID, comID): \n",
    "        return np.dot(self.w_C[comID,:], self.w_I[invID,:]) + self.mean_inv\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ds = DataSet()#loading dataset\n",
    "    for K in [8, 16, 32, 64]:\n",
    "        for maxepoch in [100,200,300,400,500,600,700,800,900,1000]:\n",
    "            pmf = PMF(num_feat=K,maxepoch=maxepoch)\n",
    "            valtest = random.sample(ds.trainset,int(0.2*len(ds.trainset)))\n",
    "            pmf.fit(np.array(ds.trainset), np.array(valtest))\n",
    "            squaredError = []\n",
    "            for u,i,r in ds.testset:\n",
    "                error=r - pmf.predict(int(u),int(i))\n",
    "                squaredError.append(error * error)\n",
    "            rmse =math.sqrt(sum(squaredError) / len(squaredError))\n",
    "            print(\"RMSE@{}@{}:{}\".format(K, maxepoch, rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Interaction = 994169, User = 6040, Item = 3706, Sparsity = 0.0444\n",
      "RMSE@200:1.0138809256429975\n",
      "RMSE@500:0.9379763303663161\n",
      "RMSE@800:0.9236148833492648\n",
      "RMSE@1000:0.9163266560676905\n",
      "RMSE@1200:0.9143268841790512\n",
      "RMSE@1500:0.9116184805110863\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8  \n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.08.13\n",
    "@function: baseline: PMF(Probabilistic Matrix Factorization)\n",
    "           paper: http://papers.nips.cc/paper/3208-probabilistic-matrix-factorization.pdf\n",
    "           Datatset: MovieLens-1m:https://grouplens.org/datasets/movielens/  \n",
    "           Evaluation: RMSE\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import heapq\n",
    "import math\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from numpy.random import RandomState\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import tensorflow as tf\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self):\n",
    "        self.trainset, self.testset, self.maxu, self.maxi, self.maxr = self._getDataset_as_list()\n",
    "        \n",
    "    def _getDataset_as_list(self):\n",
    "        #trainset\n",
    "        filePath = \"/data/fjsdata/BMF/ml-1m.train.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        maxu, maxi, maxr = data['user'].max()+1, data['item'].max()+1, data['rating'].max()\n",
    "        print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "        trainset = data.values.tolist()\n",
    "        #testset\n",
    "        filePath = \"/data/fjsdata/BMF/ml-1m.test.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        testset = data.values.tolist()\n",
    "        return trainset, testset, maxu, maxi, maxr \n",
    "    \n",
    "    def list_to_matrix(self, dataset, maxu, maxi):              \n",
    "        dataMat = np.zeros([maxu, maxi], dtype=np.float32)\n",
    "        for u,i,r in dataset:\n",
    "            dataMat[int(u)][int(i)] = float(r)\n",
    "        return np.array(dataMat)\n",
    "    \n",
    "    def list_to_dict(self, dataset):\n",
    "        dataDict = {}\n",
    "        for u,i,r in dataset:\n",
    "            dataDict[int(u), int(i)] = float(r)\n",
    "        return dataDict\n",
    "    \n",
    "class PMF:\n",
    "    def __init__(self, num_feat=8, epsilon=1, _lambda=0.1, momentum=0.9, maxepoch=200, num_batches=10, batch_size=1000):\n",
    "        self.num_feat = num_feat\n",
    "        self.epsilon = epsilon\n",
    "        self._lambda = _lambda\n",
    "        self.momentum = momentum\n",
    "        self.maxepoch = maxepoch\n",
    "        self.num_batches = num_batches\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.w_C = None\n",
    "        self.w_I = None\n",
    "\n",
    "        self.err_train = []\n",
    "        self.err_val = []\n",
    "        \n",
    "    def fit(self, train_vec, val_vec):   \n",
    "        # mean subtraction\n",
    "        self.mean_inv = np.mean(train_vec[:,2])\n",
    "        \n",
    "        pairs_tr = train_vec.shape[0]\n",
    "        pairs_va = val_vec.shape[0]\n",
    "        \n",
    "        # 1-p-i, 2-m-c\n",
    "        num_inv = int(max(np.amax(train_vec[:,0]), np.amax(val_vec[:,0]))) + 1\n",
    "        num_com = int(max(np.amax(train_vec[:,1]), np.amax(val_vec[:,1]))) + 1\n",
    "\n",
    "        incremental = False\n",
    "        if ((not incremental) or (self.w_C is None)):\n",
    "            # initialize\n",
    "            self.epoch = 0\n",
    "            self.w_C = 0.1 * np.random.randn(num_com, self.num_feat)\n",
    "            self.w_I = 0.1 * np.random.randn(num_inv, self.num_feat)\n",
    "            \n",
    "            self.w_C_inc = np.zeros((num_com, self.num_feat))\n",
    "            self.w_I_inc = np.zeros((num_inv, self.num_feat))\n",
    "        \n",
    "        \n",
    "        while self.epoch < self.maxepoch:\n",
    "            self.epoch += 1\n",
    "\n",
    "            # Shuffle training truples\n",
    "            shuffled_order = np.arange(train_vec.shape[0])\n",
    "            np.random.shuffle(shuffled_order)\n",
    "\n",
    "            # Batch update\n",
    "            for batch in range(self.num_batches):\n",
    "                # print \"epoch %d batch %d\" % (self.epoch, batch+1)\n",
    "\n",
    "                batch_idx = np.mod(np.arange(self.batch_size * batch,\n",
    "                                             self.batch_size * (batch+1)),\n",
    "                                   shuffled_order.shape[0])\n",
    "\n",
    "                batch_invID = np.array(train_vec[shuffled_order[batch_idx], 0], dtype='int32')\n",
    "                batch_comID = np.array(train_vec[shuffled_order[batch_idx], 1], dtype='int32')\n",
    "\n",
    "                # Compute Objective Function\n",
    "                pred_out = np.sum(np.multiply(self.w_I[batch_invID,:], \n",
    "                                                self.w_C[batch_comID,:]),\n",
    "                                axis=1) # mean_inv subtracted\n",
    "\n",
    "                rawErr = pred_out - train_vec[shuffled_order[batch_idx], 2] + self.mean_inv\n",
    "\n",
    "                # Compute gradients\n",
    "                Ix_C = 2 * np.multiply(rawErr[:, np.newaxis], self.w_I[batch_invID,:]) \\\n",
    "                        + self._lambda * self.w_C[batch_comID,:]\n",
    "                Ix_I = 2 * np.multiply(rawErr[:, np.newaxis], self.w_C[batch_comID,:]) \\\n",
    "                        + self._lambda * self.w_I[batch_invID,:]\n",
    "            \n",
    "                dw_C = np.zeros((num_com, self.num_feat))\n",
    "                dw_I = np.zeros((num_inv, self.num_feat))\n",
    "\n",
    "                # loop to aggreate the gradients of the same element\n",
    "                for i in range(self.batch_size):\n",
    "                    dw_C[batch_comID[i],:] += Ix_C[i,:]\n",
    "                    dw_I[batch_invID[i],:] += Ix_I[i,:]\n",
    "\n",
    "\n",
    "                # Update with momentum\n",
    "                self.w_C_inc = self.momentum * self.w_C_inc + self.epsilon * dw_C / self.batch_size\n",
    "                self.w_I_inc = self.momentum * self.w_I_inc + self.epsilon * dw_I / self.batch_size\n",
    "\n",
    "\n",
    "                self.w_C = self.w_C - self.w_C_inc\n",
    "                self.w_I = self.w_I - self.w_I_inc\n",
    "\n",
    "                # Compute Objective Function after\n",
    "                if batch == self.num_batches - 1:\n",
    "                    pred_out = np.sum(np.multiply(self.w_I[np.array(train_vec[:,0], dtype='int32'),:],\n",
    "                                                    self.w_C[np.array(train_vec[:,1], dtype='int32'),:]),\n",
    "                                        axis=1) # mean_inv subtracted\n",
    "                    rawErr = pred_out - train_vec[:, 2] + self.mean_inv\n",
    "                    obj = LA.norm(rawErr) ** 2 \\\n",
    "                            + 0.5*self._lambda*(LA.norm(self.w_I) ** 2 + LA.norm(self.w_C) ** 2)\n",
    "\n",
    "                    self.err_train.append(np.sqrt(obj/pairs_tr))\n",
    "\n",
    "                # Compute validation error\n",
    "                if batch == self.num_batches - 1:\n",
    "                    pred_out = np.sum(np.multiply(self.w_I[np.array(val_vec[:,0], dtype='int32'),:],\n",
    "                                                    self.w_C[np.array(val_vec[:,1], dtype='int32'),:]),\n",
    "                                        axis=1) # mean_inv subtracted\n",
    "                    rawErr = pred_out - val_vec[:, 2] + self.mean_inv\n",
    "                    self.err_val.append(LA.norm(rawErr)/np.sqrt(pairs_va))\n",
    "\n",
    "                # Print info\n",
    "                #if batch == self.num_batches - 1:\n",
    "                    #print ('Training RMSE: %f, Val RMSE %f' % (self.err_train[-1], self.err_val[-1]))\n",
    "    \n",
    "    def predict(self, invID, comID): \n",
    "        return np.dot(self.w_C[comID,:], self.w_I[invID,:]) + self.mean_inv\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ds = DataSet()#loading dataset\n",
    "    for maxepoch in [200,500,800,1000,1200,1500]:\n",
    "        pmf = PMF(num_feat=8,maxepoch=maxepoch)\n",
    "        valtest = random.sample(ds.trainset,int(0.2*len(ds.trainset)))\n",
    "        pmf.fit(np.array(ds.trainset), np.array(valtest))\n",
    "        squaredError = []\n",
    "        for u,i,r in ds.testset:\n",
    "            error=r - pmf.predict(int(u),int(i))\n",
    "            squaredError.append(error * error)\n",
    "        rmse =math.sqrt(sum(squaredError) / len(squaredError))\n",
    "        print(\"RMSE@{}:{}\".format(maxepoch, rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Interaction = 994169, User = 6040, Item = 3706, Sparsity = 0.0444\n",
      "RMSE@2000:0.9104183102074722\n",
      "RMSE@2500:0.9078621316286339\n",
      "RMSE@3000:0.9102854348600679\n",
      "RMSE@3500:0.9108877313329796\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ds = DataSet()#loading dataset\n",
    "    for maxepoch in [2000,2500,3000,3500]:\n",
    "        pmf = PMF(num_feat=8,maxepoch=maxepoch)\n",
    "        valtest = random.sample(ds.trainset,int(0.2*len(ds.trainset)))\n",
    "        pmf.fit(np.array(ds.trainset), np.array(valtest))\n",
    "        squaredError = []\n",
    "        for u,i,r in ds.testset:\n",
    "            error=r - pmf.predict(int(u),int(i))\n",
    "            squaredError.append(error * error)\n",
    "        rmse =math.sqrt(sum(squaredError) / len(squaredError))\n",
    "        print(\"RMSE@{}:{}\".format(maxepoch, rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Interaction = 994169, User = 6040, Item = 3706, Sparsity = 0.0444\n",
      "RMSE@8:1.0179648182313636 of testset\n",
      "RMSE@8:0.926293276467461 of valset\n",
      "RMSE@16:1.0113062781040372 of testset\n",
      "RMSE@16:0.925099734513955 of valset\n",
      "RMSE@32:1.000355947647999 of testset\n",
      "RMSE@32:0.9000060426581552 of valset\n",
      "RMSE@64:0.9953756350779035 of testset\n",
      "RMSE@64:0.8982219902765491 of valset\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8  \n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.31\n",
    "@function: baseline: PMF(Probabilistic Matrix Factorization)\n",
    "           paper: http://papers.nips.cc/paper/3208-probabilistic-matrix-factorization.pdf\n",
    "           Datatset: MovieLens-1m:https://grouplens.org/datasets/movielens/  \n",
    "           Evaluation: RMSE\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import heapq\n",
    "import math\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from numpy.random import RandomState\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import tensorflow as tf\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self):\n",
    "        self.trainset, self.testset, self.maxu, self.maxi, self.maxr = self._getDataset_as_list()\n",
    "        \n",
    "    def _getDataset_as_list(self):\n",
    "        #trainset\n",
    "        filePath = \"/data/fjsdata/BMF/ml-1m.train.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        maxu, maxi, maxr = data['user'].max()+1, data['item'].max()+1, data['rating'].max()\n",
    "        print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "        trainset = data.values.tolist()\n",
    "        #testset\n",
    "        filePath = \"/data/fjsdata/BMF/ml-1m.test.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        testset = data.values.tolist()\n",
    "        return trainset, testset, maxu, maxi, maxr \n",
    "    \n",
    "    def list_to_matrix(self, dataset, maxu, maxi):              \n",
    "        dataMat = np.zeros([maxu, maxi], dtype=np.float32)\n",
    "        for u,i,r in dataset:\n",
    "            dataMat[int(u)][int(i)] = float(r)\n",
    "        return np.array(dataMat)\n",
    "    \n",
    "    def list_to_dict(self, dataset):\n",
    "        dataDict = {}\n",
    "        for u,i,r in dataset:\n",
    "            dataDict[int(u), int(i)] = float(r)\n",
    "        return dataDict\n",
    "    \n",
    "class PMF:\n",
    "    def __init__(self, num_feat=8, epsilon=1, _lambda=0.1, momentum=0.9, maxepoch=200, num_batches=10, batch_size=1000):\n",
    "        self.num_feat = num_feat\n",
    "        self.epsilon = epsilon\n",
    "        self._lambda = _lambda\n",
    "        self.momentum = momentum\n",
    "        self.maxepoch = maxepoch\n",
    "        self.num_batches = num_batches\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.w_C = None\n",
    "        self.w_I = None\n",
    "\n",
    "        self.err_train = []\n",
    "        self.err_val = []\n",
    "        \n",
    "    def fit(self, train_vec, val_vec):   \n",
    "        # mean subtraction\n",
    "        self.mean_inv = np.mean(train_vec[:,2])\n",
    "        \n",
    "        pairs_tr = train_vec.shape[0]\n",
    "        pairs_va = val_vec.shape[0]\n",
    "        \n",
    "        # 1-p-i, 2-m-c\n",
    "        num_inv = int(max(np.amax(train_vec[:,0]), np.amax(val_vec[:,0]))) + 1\n",
    "        num_com = int(max(np.amax(train_vec[:,1]), np.amax(val_vec[:,1]))) + 1\n",
    "\n",
    "        incremental = False\n",
    "        if ((not incremental) or (self.w_C is None)):\n",
    "            # initialize\n",
    "            self.epoch = 0\n",
    "            self.w_C = 0.1 * np.random.randn(num_com, self.num_feat)\n",
    "            self.w_I = 0.1 * np.random.randn(num_inv, self.num_feat)\n",
    "            \n",
    "            self.w_C_inc = np.zeros((num_com, self.num_feat))\n",
    "            self.w_I_inc = np.zeros((num_inv, self.num_feat))\n",
    "        \n",
    "        \n",
    "        while self.epoch < self.maxepoch:\n",
    "            self.epoch += 1\n",
    "\n",
    "            # Shuffle training truples\n",
    "            shuffled_order = np.arange(train_vec.shape[0])\n",
    "            np.random.shuffle(shuffled_order)\n",
    "\n",
    "            # Batch update\n",
    "            for batch in range(self.num_batches):\n",
    "                # print \"epoch %d batch %d\" % (self.epoch, batch+1)\n",
    "\n",
    "                batch_idx = np.mod(np.arange(self.batch_size * batch,\n",
    "                                             self.batch_size * (batch+1)),\n",
    "                                   shuffled_order.shape[0])\n",
    "\n",
    "                batch_invID = np.array(train_vec[shuffled_order[batch_idx], 0], dtype='int32')\n",
    "                batch_comID = np.array(train_vec[shuffled_order[batch_idx], 1], dtype='int32')\n",
    "\n",
    "                # Compute Objective Function\n",
    "                pred_out = np.sum(np.multiply(self.w_I[batch_invID,:], \n",
    "                                                self.w_C[batch_comID,:]),\n",
    "                                axis=1) # mean_inv subtracted\n",
    "\n",
    "                rawErr = pred_out - train_vec[shuffled_order[batch_idx], 2] + self.mean_inv\n",
    "\n",
    "                # Compute gradients\n",
    "                Ix_C = 2 * np.multiply(rawErr[:, np.newaxis], self.w_I[batch_invID,:]) \\\n",
    "                        + self._lambda * self.w_C[batch_comID,:]\n",
    "                Ix_I = 2 * np.multiply(rawErr[:, np.newaxis], self.w_C[batch_comID,:]) \\\n",
    "                        + self._lambda * self.w_I[batch_invID,:]\n",
    "            \n",
    "                dw_C = np.zeros((num_com, self.num_feat))\n",
    "                dw_I = np.zeros((num_inv, self.num_feat))\n",
    "\n",
    "                # loop to aggreate the gradients of the same element\n",
    "                for i in range(self.batch_size):\n",
    "                    dw_C[batch_comID[i],:] += Ix_C[i,:]\n",
    "                    dw_I[batch_invID[i],:] += Ix_I[i,:]\n",
    "\n",
    "\n",
    "                # Update with momentum\n",
    "                self.w_C_inc = self.momentum * self.w_C_inc + self.epsilon * dw_C / self.batch_size\n",
    "                self.w_I_inc = self.momentum * self.w_I_inc + self.epsilon * dw_I / self.batch_size\n",
    "\n",
    "\n",
    "                self.w_C = self.w_C - self.w_C_inc\n",
    "                self.w_I = self.w_I - self.w_I_inc\n",
    "\n",
    "                # Compute Objective Function after\n",
    "                if batch == self.num_batches - 1:\n",
    "                    pred_out = np.sum(np.multiply(self.w_I[np.array(train_vec[:,0], dtype='int32'),:],\n",
    "                                                    self.w_C[np.array(train_vec[:,1], dtype='int32'),:]),\n",
    "                                        axis=1) # mean_inv subtracted\n",
    "                    rawErr = pred_out - train_vec[:, 2] + self.mean_inv\n",
    "                    obj = LA.norm(rawErr) ** 2 \\\n",
    "                            + 0.5*self._lambda*(LA.norm(self.w_I) ** 2 + LA.norm(self.w_C) ** 2)\n",
    "\n",
    "                    self.err_train.append(np.sqrt(obj/pairs_tr))\n",
    "\n",
    "                # Compute validation error\n",
    "                if batch == self.num_batches - 1:\n",
    "                    pred_out = np.sum(np.multiply(self.w_I[np.array(val_vec[:,0], dtype='int32'),:],\n",
    "                                                    self.w_C[np.array(val_vec[:,1], dtype='int32'),:]),\n",
    "                                        axis=1) # mean_inv subtracted\n",
    "                    rawErr = pred_out - val_vec[:, 2] + self.mean_inv\n",
    "                    self.err_val.append(LA.norm(rawErr)/np.sqrt(pairs_va))\n",
    "\n",
    "                # Print info\n",
    "                #if batch == self.num_batches - 1:\n",
    "                    #print ('Training RMSE: %f, Val RMSE %f' % (self.err_train[-1], self.err_val[-1]))\n",
    "    \n",
    "    def predict(self, invID, comID): \n",
    "        return np.dot(self.w_C[comID,:], self.w_I[invID,:]) + self.mean_inv\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ds = DataSet()#loading dataset\n",
    "    for K in [8, 16, 32, 64]:\n",
    "        pmf = PMF(num_feat=K)\n",
    "        valtest = random.sample(ds.trainset,int(0.2*len(ds.trainset)))\n",
    "        pmf.fit(np.array(ds.trainset), np.array(valtest))\n",
    "        squaredError = []\n",
    "        for u,i,r in ds.testset:\n",
    "            error=r - pmf.predict(int(u),int(i))\n",
    "            squaredError.append(error * error)\n",
    "        rmse =math.sqrt(sum(squaredError) / len(squaredError))\n",
    "        print(\"RMSE@{}:{} of testset\".format(K, rmse))\n",
    "        valset = random.sample(ds.trainset, len(ds.testset))#get the same len of testset from trainset as validset.\n",
    "        valError = []\n",
    "        for u,i,r in valset:\n",
    "            error=r - pmf.predict(int(u),int(i))\n",
    "            valError.append(error * error)\n",
    "        rmse =math.sqrt(sum(valError) / len(valError))\n",
    "        print(\"RMSE@{}:{} of valset\".format(K, rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Interaction = 2313189, User = 10216, Item = 96324, Sparsity = 0.0024\n",
      "RMSE@8:1.259523685026561\n",
      "RMSE@16:1.2499875055271275\n",
      "RMSE@32:1.2536294397190462\n",
      "RMSE@64:1.2628182875219398\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8  \n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.08.10\n",
    "@function: baseline: PMF(Probabilistic Matrix Factorization)\n",
    "           paper: http://papers.nips.cc/paper/3208-probabilistic-matrix-factorization.pdf\n",
    "           Datatset: KnowledgeBase-CC \n",
    "           Evaluation: RMSE\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import heapq\n",
    "import math\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from numpy.random import RandomState\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import tensorflow as tf\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self):\n",
    "        self.trainset, self.testset, self.maxu, self.maxi, self.maxr = self._getDataset_as_list()\n",
    "        \n",
    "    def _getDataset_as_list(self):\n",
    "        #trainset\n",
    "        filePath = \"/data/fjsdata/BMF/kbcc_trainset.csv\" \n",
    "        data = pd.read_csv(filePath, sep='|', low_memory=False, dtype={'csr':int, 'ke':int, 'num':float})\n",
    "        maxu, maxi, maxr = data['csr'].max()+1, data['ke'].max()+1, data['num'].max()\n",
    "        print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "        trainset = data.values.tolist()\n",
    "        #testset\n",
    "        filePath = \"/data/fjsdata/BMF/kbcc_testset.csv\" \n",
    "        data = pd.read_csv(filePath, sep='|', low_memory=False, dtype={'csr':int, 'ke':int, 'num':float})\n",
    "        testset = data.values.tolist()\n",
    "        return trainset, testset, maxu, maxi, maxr \n",
    "    \n",
    "    def list_to_matrix(self, dataset, maxu, maxi):              \n",
    "        dataMat = np.zeros([maxu, maxi], dtype=np.float32)\n",
    "        for u,i,r in dataset:\n",
    "            dataMat[int(u)][int(i)] = float(r)\n",
    "        return np.array(dataMat)\n",
    "    \n",
    "    def list_to_dict(self, dataset):\n",
    "        dataDict = {}\n",
    "        for u,i,r in dataset:\n",
    "            dataDict[int(u), int(i)] = float(r)\n",
    "        return dataDict\n",
    "    \n",
    "class PMF:\n",
    "    def __init__(self, num_feat=8, epsilon=1, _lambda=0.1, momentum=0.9, maxepoch=500, num_batches=10, batch_size=1000):\n",
    "        self.num_feat = num_feat\n",
    "        self.epsilon = epsilon\n",
    "        self._lambda = _lambda\n",
    "        self.momentum = momentum\n",
    "        self.maxepoch = maxepoch\n",
    "        self.num_batches = num_batches\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.w_C = None\n",
    "        self.w_I = None\n",
    "\n",
    "        self.err_train = []\n",
    "        self.err_val = []\n",
    "        \n",
    "    def fit(self, train_vec, val_vec):   \n",
    "        # mean subtraction\n",
    "        self.mean_inv = np.mean(train_vec[:,2])\n",
    "        \n",
    "        pairs_tr = train_vec.shape[0]\n",
    "        pairs_va = val_vec.shape[0]\n",
    "        \n",
    "        # 1-p-i, 2-m-c\n",
    "        num_inv = int(max(np.amax(train_vec[:,0]), np.amax(val_vec[:,0]))) + 1\n",
    "        num_com = int(max(np.amax(train_vec[:,1]), np.amax(val_vec[:,1]))) + 1\n",
    "\n",
    "        incremental = False\n",
    "        if ((not incremental) or (self.w_C is None)):\n",
    "            # initialize\n",
    "            self.epoch = 0\n",
    "            self.w_C = 0.1 * np.random.randn(num_com, self.num_feat)\n",
    "            self.w_I = 0.1 * np.random.randn(num_inv, self.num_feat)\n",
    "            \n",
    "            self.w_C_inc = np.zeros((num_com, self.num_feat))\n",
    "            self.w_I_inc = np.zeros((num_inv, self.num_feat))\n",
    "        \n",
    "        \n",
    "        while self.epoch < self.maxepoch:\n",
    "            self.epoch += 1\n",
    "\n",
    "            # Shuffle training truples\n",
    "            shuffled_order = np.arange(train_vec.shape[0])\n",
    "            np.random.shuffle(shuffled_order)\n",
    "\n",
    "            # Batch update\n",
    "            for batch in range(self.num_batches):\n",
    "                # print \"epoch %d batch %d\" % (self.epoch, batch+1)\n",
    "\n",
    "                batch_idx = np.mod(np.arange(self.batch_size * batch,\n",
    "                                             self.batch_size * (batch+1)),\n",
    "                                   shuffled_order.shape[0])\n",
    "\n",
    "                batch_invID = np.array(train_vec[shuffled_order[batch_idx], 0], dtype='int32')\n",
    "                batch_comID = np.array(train_vec[shuffled_order[batch_idx], 1], dtype='int32')\n",
    "\n",
    "                # Compute Objective Function\n",
    "                pred_out = np.sum(np.multiply(self.w_I[batch_invID,:], \n",
    "                                                self.w_C[batch_comID,:]),\n",
    "                                axis=1) # mean_inv subtracted\n",
    "\n",
    "                rawErr = pred_out - train_vec[shuffled_order[batch_idx], 2] + self.mean_inv\n",
    "\n",
    "                # Compute gradients\n",
    "                Ix_C = 2 * np.multiply(rawErr[:, np.newaxis], self.w_I[batch_invID,:]) \\\n",
    "                        + self._lambda * self.w_C[batch_comID,:]\n",
    "                Ix_I = 2 * np.multiply(rawErr[:, np.newaxis], self.w_C[batch_comID,:]) \\\n",
    "                        + self._lambda * self.w_I[batch_invID,:]\n",
    "            \n",
    "                dw_C = np.zeros((num_com, self.num_feat))\n",
    "                dw_I = np.zeros((num_inv, self.num_feat))\n",
    "\n",
    "                # loop to aggreate the gradients of the same element\n",
    "                for i in range(self.batch_size):\n",
    "                    dw_C[batch_comID[i],:] += Ix_C[i,:]\n",
    "                    dw_I[batch_invID[i],:] += Ix_I[i,:]\n",
    "\n",
    "\n",
    "                # Update with momentum\n",
    "                self.w_C_inc = self.momentum * self.w_C_inc + self.epsilon * dw_C / self.batch_size\n",
    "                self.w_I_inc = self.momentum * self.w_I_inc + self.epsilon * dw_I / self.batch_size\n",
    "\n",
    "\n",
    "                self.w_C = self.w_C - self.w_C_inc\n",
    "                self.w_I = self.w_I - self.w_I_inc\n",
    "\n",
    "                # Compute Objective Function after\n",
    "                if batch == self.num_batches - 1:\n",
    "                    pred_out = np.sum(np.multiply(self.w_I[np.array(train_vec[:,0], dtype='int32'),:],\n",
    "                                                    self.w_C[np.array(train_vec[:,1], dtype='int32'),:]),\n",
    "                                        axis=1) # mean_inv subtracted\n",
    "                    rawErr = pred_out - train_vec[:, 2] + self.mean_inv\n",
    "                    obj = LA.norm(rawErr) ** 2 \\\n",
    "                            + 0.5*self._lambda*(LA.norm(self.w_I) ** 2 + LA.norm(self.w_C) ** 2)\n",
    "\n",
    "                    self.err_train.append(np.sqrt(obj/pairs_tr))\n",
    "\n",
    "                # Compute validation error\n",
    "                if batch == self.num_batches - 1:\n",
    "                    pred_out = np.sum(np.multiply(self.w_I[np.array(val_vec[:,0], dtype='int32'),:],\n",
    "                                                    self.w_C[np.array(val_vec[:,1], dtype='int32'),:]),\n",
    "                                        axis=1) # mean_inv subtracted\n",
    "                    rawErr = pred_out - val_vec[:, 2] + self.mean_inv\n",
    "                    self.err_val.append(LA.norm(rawErr)/np.sqrt(pairs_va))\n",
    "\n",
    "                # Print info\n",
    "                #if batch == self.num_batches - 1:\n",
    "                    #print ('Training RMSE: %f, Val RMSE %f' % (self.err_train[-1], self.err_val[-1]))\n",
    "    \n",
    "    def predict(self, invID, comID): \n",
    "        return np.dot(self.w_C[comID,:], self.w_I[invID,:]) + self.mean_inv\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ds = DataSet()#loading dataset\n",
    "    for K in [8, 16, 32, 64]:\n",
    "        pmf = PMF(num_feat=K)\n",
    "        valtest = random.sample(ds.trainset,int(0.2*len(ds.trainset)))\n",
    "        pmf.fit(np.array(ds.trainset), np.array(valtest))\n",
    "        squaredError = []\n",
    "        for u,i,r in ds.testset:\n",
    "            error=r - pmf.predict(int(u),int(i))\n",
    "            squaredError.append(error * error)\n",
    "        rmse =math.sqrt(sum(squaredError) / len(squaredError))\n",
    "        print(\"RMSE@{}:{}\".format(K, rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Interaction = 994169, User = 6040, Item = 3706, Sparsity = 0.0444\n",
      "RMSE@8:1.0141986226984128\n",
      "RMSE@16:1.0076482723021978\n",
      "RMSE@32:0.9999458411689918\n",
      "RMSE@64:0.9952336524728492\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8  \n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.31\n",
    "@function: baseline: PMF(Probabilistic Matrix Factorization)\n",
    "           paper: http://papers.nips.cc/paper/3208-probabilistic-matrix-factorization.pdf\n",
    "           Datatset: MovieLens-1m:https://grouplens.org/datasets/movielens/  \n",
    "           Evaluation: RMSE\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import heapq\n",
    "import math\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from numpy.random import RandomState\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import tensorflow as tf\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self):\n",
    "        self.trainset, self.testset, self.maxu, self.maxi, self.maxr = self._getDataset_as_list()\n",
    "        \n",
    "    def _getDataset_as_list(self):\n",
    "        #trainset\n",
    "        filePath = \"/data/fjsdata/BMF/ml-1m.train.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        maxu, maxi, maxr = data['user'].max()+1, data['item'].max()+1, data['rating'].max()\n",
    "        print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "        trainset = data.values.tolist()\n",
    "        #testset\n",
    "        filePath = \"/data/fjsdata/BMF/ml-1m.test.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        testset = data.values.tolist()\n",
    "        return trainset, testset, maxu, maxi, maxr \n",
    "    \n",
    "    def list_to_matrix(self, dataset, maxu, maxi):              \n",
    "        dataMat = np.zeros([maxu, maxi], dtype=np.float32)\n",
    "        for u,i,r in dataset:\n",
    "            dataMat[int(u)][int(i)] = float(r)\n",
    "        return np.array(dataMat)\n",
    "    \n",
    "    def list_to_dict(self, dataset):\n",
    "        dataDict = {}\n",
    "        for u,i,r in dataset:\n",
    "            dataDict[int(u), int(i)] = float(r)\n",
    "        return dataDict\n",
    "    \n",
    "class PMF:\n",
    "    def __init__(self, num_feat=8, epsilon=1, _lambda=0.1, momentum=0.9, maxepoch=200, num_batches=10, batch_size=1000):\n",
    "        self.num_feat = num_feat\n",
    "        self.epsilon = epsilon\n",
    "        self._lambda = _lambda\n",
    "        self.momentum = momentum\n",
    "        self.maxepoch = maxepoch\n",
    "        self.num_batches = num_batches\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.w_C = None\n",
    "        self.w_I = None\n",
    "\n",
    "        self.err_train = []\n",
    "        self.err_val = []\n",
    "        \n",
    "    def fit(self, train_vec, val_vec):   \n",
    "        # mean subtraction\n",
    "        self.mean_inv = np.mean(train_vec[:,2])\n",
    "        \n",
    "        pairs_tr = train_vec.shape[0]\n",
    "        pairs_va = val_vec.shape[0]\n",
    "        \n",
    "        # 1-p-i, 2-m-c\n",
    "        num_inv = int(max(np.amax(train_vec[:,0]), np.amax(val_vec[:,0]))) + 1\n",
    "        num_com = int(max(np.amax(train_vec[:,1]), np.amax(val_vec[:,1]))) + 1\n",
    "\n",
    "        incremental = False\n",
    "        if ((not incremental) or (self.w_C is None)):\n",
    "            # initialize\n",
    "            self.epoch = 0\n",
    "            self.w_C = 0.1 * np.random.randn(num_com, self.num_feat)\n",
    "            self.w_I = 0.1 * np.random.randn(num_inv, self.num_feat)\n",
    "            \n",
    "            self.w_C_inc = np.zeros((num_com, self.num_feat))\n",
    "            self.w_I_inc = np.zeros((num_inv, self.num_feat))\n",
    "        \n",
    "        \n",
    "        while self.epoch < self.maxepoch:\n",
    "            self.epoch += 1\n",
    "\n",
    "            # Shuffle training truples\n",
    "            shuffled_order = np.arange(train_vec.shape[0])\n",
    "            np.random.shuffle(shuffled_order)\n",
    "\n",
    "            # Batch update\n",
    "            for batch in range(self.num_batches):\n",
    "                # print \"epoch %d batch %d\" % (self.epoch, batch+1)\n",
    "\n",
    "                batch_idx = np.mod(np.arange(self.batch_size * batch,\n",
    "                                             self.batch_size * (batch+1)),\n",
    "                                   shuffled_order.shape[0])\n",
    "\n",
    "                batch_invID = np.array(train_vec[shuffled_order[batch_idx], 0], dtype='int32')\n",
    "                batch_comID = np.array(train_vec[shuffled_order[batch_idx], 1], dtype='int32')\n",
    "\n",
    "                # Compute Objective Function\n",
    "                pred_out = np.sum(np.multiply(self.w_I[batch_invID,:], \n",
    "                                                self.w_C[batch_comID,:]),\n",
    "                                axis=1) # mean_inv subtracted\n",
    "\n",
    "                rawErr = pred_out - train_vec[shuffled_order[batch_idx], 2] + self.mean_inv\n",
    "\n",
    "                # Compute gradients\n",
    "                Ix_C = 2 * np.multiply(rawErr[:, np.newaxis], self.w_I[batch_invID,:]) \\\n",
    "                        + self._lambda * self.w_C[batch_comID,:]\n",
    "                Ix_I = 2 * np.multiply(rawErr[:, np.newaxis], self.w_C[batch_comID,:]) \\\n",
    "                        + self._lambda * self.w_I[batch_invID,:]\n",
    "            \n",
    "                dw_C = np.zeros((num_com, self.num_feat))\n",
    "                dw_I = np.zeros((num_inv, self.num_feat))\n",
    "\n",
    "                # loop to aggreate the gradients of the same element\n",
    "                for i in range(self.batch_size):\n",
    "                    dw_C[batch_comID[i],:] += Ix_C[i,:]\n",
    "                    dw_I[batch_invID[i],:] += Ix_I[i,:]\n",
    "\n",
    "\n",
    "                # Update with momentum\n",
    "                self.w_C_inc = self.momentum * self.w_C_inc + self.epsilon * dw_C / self.batch_size\n",
    "                self.w_I_inc = self.momentum * self.w_I_inc + self.epsilon * dw_I / self.batch_size\n",
    "\n",
    "\n",
    "                self.w_C = self.w_C - self.w_C_inc\n",
    "                self.w_I = self.w_I - self.w_I_inc\n",
    "\n",
    "                # Compute Objective Function after\n",
    "                if batch == self.num_batches - 1:\n",
    "                    pred_out = np.sum(np.multiply(self.w_I[np.array(train_vec[:,0], dtype='int32'),:],\n",
    "                                                    self.w_C[np.array(train_vec[:,1], dtype='int32'),:]),\n",
    "                                        axis=1) # mean_inv subtracted\n",
    "                    rawErr = pred_out - train_vec[:, 2] + self.mean_inv\n",
    "                    obj = LA.norm(rawErr) ** 2 \\\n",
    "                            + 0.5*self._lambda*(LA.norm(self.w_I) ** 2 + LA.norm(self.w_C) ** 2)\n",
    "\n",
    "                    self.err_train.append(np.sqrt(obj/pairs_tr))\n",
    "\n",
    "                # Compute validation error\n",
    "                if batch == self.num_batches - 1:\n",
    "                    pred_out = np.sum(np.multiply(self.w_I[np.array(val_vec[:,0], dtype='int32'),:],\n",
    "                                                    self.w_C[np.array(val_vec[:,1], dtype='int32'),:]),\n",
    "                                        axis=1) # mean_inv subtracted\n",
    "                    rawErr = pred_out - val_vec[:, 2] + self.mean_inv\n",
    "                    self.err_val.append(LA.norm(rawErr)/np.sqrt(pairs_va))\n",
    "\n",
    "                # Print info\n",
    "                #if batch == self.num_batches - 1:\n",
    "                    #print ('Training RMSE: %f, Val RMSE %f' % (self.err_train[-1], self.err_val[-1]))\n",
    "    \n",
    "    def predict(self, invID, comID): \n",
    "        return np.dot(self.w_C[comID,:], self.w_I[invID,:]) + self.mean_inv\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ds = DataSet()#loading dataset\n",
    "    for K in [8, 16, 32, 64]:\n",
    "        pmf = PMF(num_feat=K)\n",
    "        valtest = random.sample(ds.trainset,int(0.2*len(ds.trainset)))\n",
    "        pmf.fit(np.array(ds.trainset), np.array(valtest))\n",
    "        squaredError = []\n",
    "        for u,i,r in ds.testset:\n",
    "            error=r - pmf.predict(int(u),int(i))\n",
    "            squaredError.append(error * error)\n",
    "        rmse =math.sqrt(sum(squaredError) / len(squaredError))\n",
    "        print(\"RMSE@{}:{}\".format(K, rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Interaction = 994169, User = 6040, Item = 3706, Sparsity = 0.0444\n",
      "RMSE@8:2.6857109756335107\n",
      "RMSE@16:2.6852826856311798\n",
      "RMSE@32:2.685775117071988\n",
      "RMSE@64:2.6851736310662075\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8  \n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.31\n",
    "@function: baseline PMF(Probabilistic Matrix Factorization)\n",
    "           Datatset: MovieLens-1m:https://grouplens.org/datasets/movielens/  \n",
    "           Evaluation: RMSE\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import heapq\n",
    "import math\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from numpy.random import RandomState\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import tensorflow as tf\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self):\n",
    "        self.trainset, self.testset, self.maxu, self.maxi, self.maxr = self._getDataset_as_list()\n",
    "        \n",
    "    def _getDataset_as_list(self):\n",
    "        #trainset\n",
    "        filePath = \"/data/fjsdata/BMF/ml-1m.train.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        maxu, maxi, maxr = data['user'].max()+1, data['item'].max()+1, data['rating'].max()\n",
    "        print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "        trainset = data.values.tolist()\n",
    "        #testset\n",
    "        filePath = \"/data/fjsdata/BMF/ml-1m.test.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        testset = data.values.tolist()\n",
    "        return trainset, testset, maxu, maxi, maxr \n",
    "    \n",
    "    def list_to_matrix(self, dataset, maxu, maxi):              \n",
    "        dataMat = np.zeros([maxu, maxi], dtype=np.float32)\n",
    "        for u,i,r in dataset:\n",
    "            dataMat[int(u)][int(i)] = float(r)\n",
    "        return np.array(dataMat)\n",
    "    \n",
    "    def list_to_dict(self, dataset):\n",
    "        dataDict = {}\n",
    "        for u,i,r in dataset:\n",
    "            dataDict[int(u), int(i)] = float(r)\n",
    "        return dataDict\n",
    "    \n",
    "    def generate_neg_sample(self, dataset, maxi, num_ng):\n",
    "        datadict = self.list_to_dict(dataset)\n",
    "        trainPN = []\n",
    "        for i in dataset:\n",
    "            trainPN.append([i[0],i[1],i[2]])\n",
    "            for t in range(num_ng):\n",
    "                j = np.random.randint(maxi)\n",
    "                while (i[0], j) in datadict:\n",
    "                    j = np.random.randint(maxi)\n",
    "                trainPN.append([i[0], j, 0.0])\n",
    "        return trainPN\n",
    "    \n",
    "class PMF:\n",
    "    def __init__(self, num_feat=8, epsilon=1, _lambda=0.1, momentum=0.8, maxepoch=20, num_batches=10, batch_size=1000):\n",
    "        self.num_feat = num_feat\n",
    "        self.epsilon = epsilon\n",
    "        self._lambda = _lambda\n",
    "        self.momentum = momentum\n",
    "        self.maxepoch = maxepoch\n",
    "        self.num_batches = num_batches\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.w_C = None\n",
    "        self.w_I = None\n",
    "\n",
    "        self.err_train = []\n",
    "        self.err_val = []\n",
    "        \n",
    "    def fit(self, train_vec, val_vec):   \n",
    "        # mean subtraction\n",
    "        self.mean_inv = np.mean(train_vec[:,2])\n",
    "        \n",
    "        pairs_tr = train_vec.shape[0]\n",
    "        pairs_va = val_vec.shape[0]\n",
    "        \n",
    "        # 1-p-i, 2-m-c\n",
    "        num_inv = int(max(np.amax(train_vec[:,0]), np.amax(val_vec[:,0]))) + 1\n",
    "        num_com = int(max(np.amax(train_vec[:,1]), np.amax(val_vec[:,1]))) + 1\n",
    "\n",
    "        incremental = False\n",
    "        if ((not incremental) or (self.w_C is None)):\n",
    "            # initialize\n",
    "            self.epoch = 0\n",
    "            self.w_C = 0.1 * np.random.randn(num_com, self.num_feat)\n",
    "            self.w_I = 0.1 * np.random.randn(num_inv, self.num_feat)\n",
    "            \n",
    "            self.w_C_inc = np.zeros((num_com, self.num_feat))\n",
    "            self.w_I_inc = np.zeros((num_inv, self.num_feat))\n",
    "        \n",
    "        \n",
    "        while self.epoch < self.maxepoch:\n",
    "            self.epoch += 1\n",
    "\n",
    "            # Shuffle training truples\n",
    "            shuffled_order = np.arange(train_vec.shape[0])\n",
    "            np.random.shuffle(shuffled_order)\n",
    "\n",
    "            # Batch update\n",
    "            for batch in range(self.num_batches):\n",
    "                # print \"epoch %d batch %d\" % (self.epoch, batch+1)\n",
    "\n",
    "                batch_idx = np.mod(np.arange(self.batch_size * batch,\n",
    "                                             self.batch_size * (batch+1)),\n",
    "                                   shuffled_order.shape[0])\n",
    "\n",
    "                batch_invID = np.array(train_vec[shuffled_order[batch_idx], 0], dtype='int32')\n",
    "                batch_comID = np.array(train_vec[shuffled_order[batch_idx], 1], dtype='int32')\n",
    "\n",
    "                # Compute Objective Function\n",
    "                pred_out = np.sum(np.multiply(self.w_I[batch_invID,:], \n",
    "                                                self.w_C[batch_comID,:]),\n",
    "                                axis=1) # mean_inv subtracted\n",
    "\n",
    "                rawErr = pred_out - train_vec[shuffled_order[batch_idx], 2] + self.mean_inv\n",
    "\n",
    "                # Compute gradients\n",
    "                Ix_C = 2 * np.multiply(rawErr[:, np.newaxis], self.w_I[batch_invID,:]) \\\n",
    "                        + self._lambda * self.w_C[batch_comID,:]\n",
    "                Ix_I = 2 * np.multiply(rawErr[:, np.newaxis], self.w_C[batch_comID,:]) \\\n",
    "                        + self._lambda * self.w_I[batch_invID,:]\n",
    "            \n",
    "                dw_C = np.zeros((num_com, self.num_feat))\n",
    "                dw_I = np.zeros((num_inv, self.num_feat))\n",
    "\n",
    "                # loop to aggreate the gradients of the same element\n",
    "                for i in range(self.batch_size):\n",
    "                    dw_C[batch_comID[i],:] += Ix_C[i,:]\n",
    "                    dw_I[batch_invID[i],:] += Ix_I[i,:]\n",
    "\n",
    "\n",
    "                # Update with momentum\n",
    "                self.w_C_inc = self.momentum * self.w_C_inc + self.epsilon * dw_C / self.batch_size\n",
    "                self.w_I_inc = self.momentum * self.w_I_inc + self.epsilon * dw_I / self.batch_size\n",
    "\n",
    "\n",
    "                self.w_C = self.w_C - self.w_C_inc\n",
    "                self.w_I = self.w_I - self.w_I_inc\n",
    "\n",
    "                # Compute Objective Function after\n",
    "                if batch == self.num_batches - 1:\n",
    "                    pred_out = np.sum(np.multiply(self.w_I[np.array(train_vec[:,0], dtype='int32'),:],\n",
    "                                                    self.w_C[np.array(train_vec[:,1], dtype='int32'),:]),\n",
    "                                        axis=1) # mean_inv subtracted\n",
    "                    rawErr = pred_out - train_vec[:, 2] + self.mean_inv\n",
    "                    obj = LA.norm(rawErr) ** 2 \\\n",
    "                            + 0.5*self._lambda*(LA.norm(self.w_I) ** 2 + LA.norm(self.w_C) ** 2)\n",
    "\n",
    "                    self.err_train.append(np.sqrt(obj/pairs_tr))\n",
    "\n",
    "                # Compute validation error\n",
    "                if batch == self.num_batches - 1:\n",
    "                    pred_out = np.sum(np.multiply(self.w_I[np.array(val_vec[:,0], dtype='int32'),:],\n",
    "                                                    self.w_C[np.array(val_vec[:,1], dtype='int32'),:]),\n",
    "                                        axis=1) # mean_inv subtracted\n",
    "                    rawErr = pred_out - val_vec[:, 2] + self.mean_inv\n",
    "                    self.err_val.append(LA.norm(rawErr)/np.sqrt(pairs_va))\n",
    "\n",
    "                # Print info\n",
    "                #if batch == self.num_batches - 1:\n",
    "                    #print ('Training RMSE: %f, Val RMSE %f' % (self.err_train[-1], self.err_val[-1]))\n",
    "    \n",
    "    def predict(self, invID, comID): \n",
    "        return np.dot(self.w_C[comID,:], self.w_I[invID,:]) + self.mean_inv\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ds = DataSet()#loading dataset\n",
    "    trainPN = ds.generate_neg_sample(ds.trainset, ds.maxi, num_ng=2)\n",
    "    for K in [8, 16, 32, 64]:\n",
    "        pmf = PMF(num_feat=K)\n",
    "        valtest = random.sample(ds.trainset,int(0.2*len(ds.trainset)))\n",
    "        pmf.fit(np.array(trainPN), np.array(valtest))\n",
    "        squaredError = []\n",
    "        for u,i,r in ds.testset:\n",
    "            error=r - pmf.predict(int(u),int(i))\n",
    "            squaredError.append(error * error)\n",
    "        rmse =math.sqrt(sum(squaredError) / len(squaredError))\n",
    "        print(\"RMSE@{}:{}\".format(K, rmse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
