{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Model: BDMF(Bayesian Deep Matrix Factorization for Recommeder Systems).\n",
    "2. Datatset: MovieLens-1m:https://grouplens.org/datasets/movielens/  \n",
    "3. Evaluation: HR,NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "9.0.176\n",
      "GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "import heapq\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Optimizer\n",
    "from sklearn.model_selection import KFold\n",
    "from torchvision import datasets, transforms\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, explained_variance_score,mean_squared_error\n",
    "print (torch.cuda.is_available())\n",
    "print (torch.version.cuda)\n",
    "print (torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation of metrics\n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "#dataset\n",
    "class DataSet_1M(object):\n",
    "    \n",
    "    def __init__(self, negNum=2):\n",
    "        self.trainList, self.shape = self._getTrainData()\n",
    "        self.trainDict = self._getTrainDict()\n",
    "        self.trainMat = self._getTrainMatrix()\n",
    "        self.trainset = self._getInstances(negNum)#sample negative samples\n",
    "        self.testset = self._getTest()\n",
    "        \n",
    "    def _getTrainData(self):\n",
    "        data = []\n",
    "        filePath = '/data/fjsdata/BNMF/ml-1m.train.rating'\n",
    "        u = 0\n",
    "        i = 0\n",
    "        maxr = 0.0\n",
    "        with open(filePath, 'r') as f:\n",
    "            for line in f:\n",
    "                if line:\n",
    "                    lines = line[:-1].split(\"\\t\")\n",
    "                    user = int(lines[0])\n",
    "                    movie = int(lines[1])\n",
    "                    score = float(lines[2])\n",
    "                    data.append((user, movie, score))\n",
    "                    if user > u:u = user\n",
    "                    if movie > i:i = movie\n",
    "                    if score > maxr:maxr = score\n",
    "        self.maxRate = maxr\n",
    "        print(\"Loading Success!\\n\"\n",
    "                  \"Data Info:\\n\"\n",
    "                  \"\\tUser Num: {}\\n\"\n",
    "                  \"\\tItem Num: {}\\n\"\n",
    "                  \"\\tData Size: {}\\n\"\n",
    "                  \"\\tSparsity: {}\".format(u+1, i+1, len(data), len(data)/((u+1)*(i+1))))\n",
    "        return data, [u+1, i+1]\n",
    "\n",
    "    def _getTrainDict(self):\n",
    "        dataDict = {}\n",
    "        for i in self.trainList:\n",
    "            dataDict[(i[0], i[1])] = i[2]\n",
    "        return dataDict\n",
    "\n",
    "    def _getTrainMatrix(self):\n",
    "        train_matrix = np.zeros([self.shape[0], self.shape[1]], dtype=np.float32)\n",
    "        for i in self.trainList:\n",
    "            user = i[0]\n",
    "            movie = i[1]\n",
    "            rating = i[2]\n",
    "            train_matrix[user][movie] = rating\n",
    "        return np.array(train_matrix)\n",
    "\n",
    "    def _getInstances(self, negNum):\n",
    "        trainset = []\n",
    "        for i in self.trainList:\n",
    "            trainset.append([i[0],i[1],i[2]])\n",
    "            for t in range(negNum):\n",
    "                j = np.random.randint(self.shape[1])\n",
    "                while (i[0], j) in self.trainDict:\n",
    "                    j = np.random.randint(self.shape[1])\n",
    "                trainset.append([i[0],j,0.0])\n",
    "        print ('The length of Trainset: %d'%(len(trainset)))\n",
    "        return trainset\n",
    "\n",
    "    def _getTest(self):\n",
    "        #loading data\n",
    "        testset = []\n",
    "        filePath = '/data/fjsdata/BNMF/ml-1m.test.negative'\n",
    "        with open(filePath, 'r') as fd:\n",
    "            line = fd.readline()\n",
    "            while line != None and line != '':\n",
    "                arr = line.split('\\t')\n",
    "                u = eval(arr[0])[0]\n",
    "                testset.append([u, eval(arr[0])[1], 1.0])#first is one postive item\n",
    "                for i in arr[1:]:\n",
    "                    testset.append([u, int(i), 0.0]) #99 negative items\n",
    "                line = fd.readline()\n",
    "        print ('The length of Testset: %d'%(len(testset)))\n",
    "        return testset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Baselien: Deep Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Success!\n",
      "Data Info:\n",
      "\tUser Num: 6040\n",
      "\tItem Num: 3706\n",
      "\tData Size: 994169\n",
      "\tSparsity: 0.04441379291858915\n",
      "The length of Trainset: 2982507\n",
      "The length of Testset: 604000\n",
      " 298 / 299 : loss = 4.773833.0Epoch:     1 total_loss = 6141704.305227\n",
      "HR@5=1.000000, NDCG@5=1.000000\n",
      " 104 / 299 : loss = 4.681472.0"
     ]
    }
   ],
   "source": [
    "class BBP_Model(nn.Module):\n",
    "    def __init__(self, input_dim_u, input_dim_i, factors_dim_k, num_units=[512]):\n",
    "        super(BBP_Model, self).__init__()\n",
    "        \n",
    "        self.input_dim_u = input_dim_u #user vector\n",
    "        self.input_dim_i = input_dim_i #item vector\n",
    "        self.factors_dim_k = factors_dim_k #latent factors vector\n",
    "        \n",
    "        # network with three hidden and k output layer\n",
    "        self.layer1_u = nn.Linear(input_dim_u, num_units[0])\n",
    "        self.layer2_u = nn.Linear(num_units[0], factors_dim_k)\n",
    "        \n",
    "        self.layer1_i = nn.Linear(input_dim_i, num_units[0])\n",
    "        self.layer2_i = nn.Linear(num_units[0], factors_dim_k)\n",
    "        \n",
    "        # activation to be used between hidden layers\n",
    "        self.activation = nn.ReLU(inplace = True)\n",
    "    \n",
    "    def forward(self, x_u, x_i):\n",
    "    \n",
    "        x_u = x_u.view(-1, self.input_dim_u)\n",
    "        x_i = x_i.view(-1, self.input_dim_i)\n",
    "        #layer1\n",
    "        x_u = self.layer1_u(x_u)\n",
    "        x_u = self.activation(x_u)\n",
    "        x_i = self.layer1_i(x_i)\n",
    "        x_i = self.activation(x_i)\n",
    "        #layer2\n",
    "        x_u = self.layer2_u(x_u)\n",
    "        x_u = self.activation(x_u)\n",
    "        x_i = self.layer2_i(x_i)\n",
    "        x_i = self.activation(x_i)\n",
    "        \n",
    "        output = torch.sum(torch.mul(x_u,x_i),1)#pxq\n",
    "        \n",
    "        return output\n",
    "    \n",
    "class BBP_Model_Wrapper:\n",
    "    def __init__(self, network, learn_rate=1e-2):\n",
    "        \n",
    "        self.learn_rate = learn_rate\n",
    "        self.network = network\n",
    "        self.network.cuda()\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr = self.learn_rate)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "    \n",
    "    def fit(self, x_u, x_i, y):\n",
    "        x_u = torch.from_numpy(np.array(x_u)).type(torch.FloatTensor).cuda()\n",
    "        x_i = torch.from_numpy(np.array(x_i)).type(torch.FloatTensor).cuda()\n",
    "        y = torch.from_numpy(np.array(y)).type(torch.FloatTensor).cuda()\n",
    "        # reset gradient and total loss\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.network(x_u, x_i)\n",
    "        fit_loss = self.loss_func(output, y)\n",
    "        \n",
    "        fit_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return fit_loss\n",
    "    \n",
    "#training model\n",
    "num_epochs = 1\n",
    "batchSize = 10000\n",
    "ds_1m = DataSet_1M()\n",
    "trainset = ds_1m.trainset\n",
    "trainMat = ds_1m.trainMat\n",
    "testset = ds_1m.testset\n",
    "shape = ds_1m.shape\n",
    "for k in [5,10,15,20]:  \n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    net = BBP_Model_Wrapper(network=BBP_Model(input_dim_u=shape[1], input_dim_i = shape[0],factors_dim_k=k))\n",
    "    best_net, best_loss = None, float('inf')\n",
    "    for epoch in range(num_epochs): #iteration\n",
    "        shuffled_idx = np.random.permutation(np.arange(len(trainset)))\n",
    "        trainset = np.array(trainset)[shuffled_idx].tolist()\n",
    "        num_batches = len(trainset) // batchSize + 1 \n",
    "        total_loss = []\n",
    "        for i in range(num_batches):#batch\n",
    "            min_idx = i * batchSize\n",
    "            max_idx = np.min([len(trainset), (i+1)*batchSize])\n",
    "            train_batch = trainset[min_idx: max_idx]\n",
    "            x_u, x_i, y = [], [], []\n",
    "            for uu,ii,rr in train_batch:\n",
    "                x_u.append(trainMat[int(uu),:])\n",
    "                x_i.append(trainMat[:,int(ii)])\n",
    "                y.append(float(rr))\n",
    "            _loss = net.fit(np.array(x_u), np.array(x_i), np.array(y))\n",
    "            sys.stdout.write('\\r {} / {} : loss = {}'.format(i, num_batches, float('%0.6f'%_loss.item())))\n",
    "            sys.stdout.flush()\n",
    "            total_loss.append(_loss.item())\n",
    "        print(\"Epoch: %5d total_loss = %.6f\" % (epoch + 1, np.mean(total_loss)))\n",
    "        if np.mean(total_loss) < best_loss:\n",
    "            best_loss = np.mean(total_loss)\n",
    "            best_net = copy.deepcopy(net.network)\n",
    "    #torch.save(best_net, \"/data/tmpexec/BDMF_torch\")\n",
    "    #best_net = torch.load(\"/data/tmpexec/BDMF_torch\").eval()\n",
    "    #best_net = torch.load(\"/data/tmpexec/BDMF_torch\").to('cuda:0')\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    for c in range(0,shape[0]):#6040\n",
    "        scorelist = []\n",
    "        gtItem = -1\n",
    "        x_u, x_i, y_i = [], [], []\n",
    "        for uu,ii,rr in testset[c*100:(c+1)*100]:#604000\n",
    "            if rr == 1.0: \n",
    "                gtItem = ii\n",
    "            x_u.append(np.array(trainMat[int(uu),:]))\n",
    "            x_i.append(np.array(trainMat[:,int(ii)]))\n",
    "            y_i.append(ii)\n",
    "        x_u = torch.from_numpy(np.array(x_u)).type(torch.FloatTensor).cuda()\n",
    "        x_i = torch.from_numpy(np.array(x_i)).type(torch.FloatTensor).cuda()\n",
    "        output = best_net(x_u, x_i)\n",
    "        output = output.cpu().data.numpy().tolist()\n",
    "        for j in range(len(y_i)):\n",
    "            scorelist.append([y_i[j],output[j]])\n",
    "        map_item_score = {}\n",
    "        for item, rate in scorelist: #turn dict\n",
    "            map_item_score[item] = rate\n",
    "        ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#topn=10\n",
    "        hr = getHitRatio(ranklist, gtItem)\n",
    "        hits.append(hr)\n",
    "        ndcg = getNDCG(ranklist, gtItem)\n",
    "        ndcgs.append(ndcg)\n",
    "    hitratio,ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "    print (\"HR@%d=%.6f, NDCG@%d=%.6f\" % (k, hitratio, k, ndcg))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2. BDMF: Bayesian Deep Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 298 / 299 : loss = 2411.391602Epoch:     1 total_loss = 1152.599930\n",
      " 298 / 299 : loss = 827.132141Epoch:     2 total_loss = 361.045931\n",
      " 298 / 299 : loss = 489.840424Epoch:     3 total_loss = 159.178854\n",
      " 298 / 299 : loss = 392.186005Epoch:     4 total_loss = 111.871830\n",
      " 298 / 299 : loss = 337.146179Epoch:     5 total_loss = 94.093116\n",
      " 298 / 299 : loss = 295.023895Epoch:     6 total_loss = 81.599802\n",
      " 298 / 299 : loss = 260.816528Epoch:     7 total_loss = 72.812910\n",
      " 298 / 299 : loss = 231.683945Epoch:     8 total_loss = 64.020904\n",
      " 298 / 299 : loss = 206.612305Epoch:     9 total_loss = 57.127743\n",
      " 298 / 299 : loss = 184.69249Epoch:    10 total_loss = 51.144272\n"
     ]
    }
   ],
   "source": [
    "def log_gaussian_loss(output, target, sigma, no_dim, sum_reduce=True):\n",
    "    exponent = -0.5*(target - output)**2/sigma**2\n",
    "    log_coeff = -no_dim*torch.log(sigma) - 0.5*no_dim*np.log(2*np.pi)\n",
    "    \n",
    "    if sum_reduce:\n",
    "        return -(log_coeff + exponent).sum()\n",
    "    else:\n",
    "        return -(log_coeff + exponent)\n",
    "    \n",
    "class gaussian:\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def loglik(self, weights):\n",
    "        exponent = -0.5*(weights - self.mu)**2/self.sigma**2\n",
    "        log_coeff = -0.5*(np.log(2*np.pi) + 2*np.log(self.sigma))\n",
    "        \n",
    "        return (exponent + log_coeff).sum()\n",
    "    \n",
    "class BayesLinear_Normalq(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, prior):\n",
    "        super(BayesLinear_Normalq, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.prior = prior\n",
    "        \n",
    "        self.weight_mus = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-0.01, 0.01))\n",
    "        self.weight_rhos = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-3, -3))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # sample gaussian noise for each weight\n",
    "        weight_epsilons = Variable(self.weight_mus.data.new(self.weight_mus.size()).normal_())      \n",
    "        # calculate the weight stds from the rho parameters\n",
    "        weight_stds = torch.log(1 + torch.exp(self.weight_rhos))\n",
    "        # calculate samples from the posterior from the sampled noise and mus/stds\n",
    "        weight_sample = self.weight_mus + weight_epsilons*weight_stds\n",
    "            \n",
    "        torch.cuda.synchronize()\n",
    "        output = torch.mm(x, weight_sample)\n",
    "            \n",
    "        # computing the KL loss term\n",
    "        #reference: https://github.com/jojonki/AutoEncoders/blob/master/kl_divergence_between_two_gaussians.pdf\n",
    "        prior_cov, varpost_cov = self.prior.sigma**2, weight_stds**2\n",
    "        KL_loss = 0.5*(torch.log(prior_cov/varpost_cov)).sum() - 0.5*weight_stds.numel()\n",
    "        KL_loss = KL_loss + 0.5*(varpost_cov/prior_cov).sum()\n",
    "        KL_loss = KL_loss + 0.5*((self.weight_mus - self.prior.mu)**2/prior_cov).sum()\n",
    "            \n",
    "        return output, KL_loss\n",
    "    \n",
    "class BBP_Model(nn.Module):\n",
    "    def __init__(self, input_dim_u, input_dim_i, factors_dim_k, num_units=[512]):\n",
    "        super(BBP_Model, self).__init__()\n",
    "        \n",
    "        self.input_dim_u = input_dim_u #user vector\n",
    "        self.input_dim_i = input_dim_i #item vector\n",
    "        self.factors_dim_k = factors_dim_k #latent factors vector\n",
    "        \n",
    "        # network with three hidden and k output layer\n",
    "        self.layer1_u = BayesLinear_Normalq(input_dim_u, num_units[0], gaussian(0, 3))\n",
    "        self.layer2_u = BayesLinear_Normalq(num_units[0], factors_dim_k, gaussian(0, 3))\n",
    "        \n",
    "        self.layer1_i = BayesLinear_Normalq(input_dim_i, num_units[0], gaussian(0, 3))\n",
    "        self.layer2_i = BayesLinear_Normalq(num_units[0], factors_dim_k, gaussian(0, 3))\n",
    "        \n",
    "        # activation to be used between hidden layers\n",
    "        self.activation = nn.ReLU(inplace = True)\n",
    "        # noise\n",
    "        self.log_noise = nn.Parameter(torch.cuda.FloatTensor([3]))\n",
    "    \n",
    "    def forward(self, x_u, x_i):\n",
    "        \n",
    "        KL_loss_total = 0\n",
    "        x_u = x_u.view(-1, self.input_dim_u)\n",
    "        x_i = x_i.view(-1, self.input_dim_i)\n",
    "        #layer1\n",
    "        x_u, KL_loss_u = self.layer1_u(x_u)\n",
    "        x_u = self.activation(x_u)\n",
    "        x_i, KL_loss_i = self.layer1_i(x_i)\n",
    "        x_i = self.activation(x_i)\n",
    "        KL_loss_total = KL_loss_total + KL_loss_u + KL_loss_i\n",
    "        #layer2\n",
    "        x_u, KL_loss_u = self.layer2_u(x_u)\n",
    "        x_u = self.activation(x_u)\n",
    "        x_i, KL_loss_i = self.layer2_i(x_i)\n",
    "        x_i = self.activation(x_i)\n",
    "        KL_loss_total = KL_loss_total + KL_loss_u + KL_loss_i\n",
    "        #pxq\n",
    "        output = torch.sum(torch.mul(x_u,x_i),1)\n",
    "        \n",
    "        return output, KL_loss_total\n",
    "    \n",
    "class BBP_Model_Wrapper:\n",
    "    def __init__(self, network, learn_rate=1e-2):\n",
    "        \n",
    "        self.learn_rate = learn_rate\n",
    "        self.network = network\n",
    "        self.network.cuda()\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr = self.learn_rate)\n",
    "        self.loss_func = log_gaussian_loss#nn.MSELoss() \n",
    "    \n",
    "    def fit(self, x_u, x_i, y, no_samples):\n",
    "        len_sql = y.shape[0]\n",
    "        x_u = torch.from_numpy(np.array(x_u)).type(torch.FloatTensor).cuda()\n",
    "        x_i = torch.from_numpy(np.array(x_i)).type(torch.FloatTensor).cuda()\n",
    "        y = torch.from_numpy(np.array(y)).type(torch.FloatTensor).cuda()\n",
    "        # reset gradient and total loss\n",
    "        self.optimizer.zero_grad()\n",
    "        fit_loss_total = 0\n",
    "        KL_loss_total = 0\n",
    "        for i in range(no_samples):\n",
    "            output, KL_loss = self.network(x_u, x_i)\n",
    "            KL_loss_total = KL_loss_total + KL_loss\n",
    "            # calculate fit loss based on mean and standard deviation of output\n",
    "            fit_loss = self.loss_func(output, y, self.network.log_noise.exp(), 1) \n",
    "            fit_loss_total = fit_loss_total + fit_loss\n",
    "        \n",
    "        total_loss = (fit_loss_total + KL_loss_total)/(no_samples*len_sql)\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return total_loss\n",
    "    \n",
    "#training model\n",
    "num_epochs = 10\n",
    "batchSize = 10000\n",
    "#ds_1m = DataSet_1M()\n",
    "#trainset = ds_1m.trainset\n",
    "#trainMat = ds_1m.trainMat\n",
    "#testset = ds_1m.testset\n",
    "#shape = ds_1m.shape\n",
    "for k in [5,10,15,20]:  \n",
    "    torch.cuda.empty_cache()\n",
    "    net = BBP_Model_Wrapper(network=BBP_Model(input_dim_u=shape[1], input_dim_i = shape[0],factors_dim_k=k))\n",
    "    best_net, best_loss = None, float('inf')\n",
    "    for epoch in range(num_epochs): #iteration\n",
    "        shuffled_idx = np.random.permutation(np.arange(len(trainset)))\n",
    "        trainset = np.array(trainset)[shuffled_idx].tolist()\n",
    "        num_batches = len(trainset) // batchSize + 1 \n",
    "        total_loss = []\n",
    "        for i in range(num_batches):#batch\n",
    "            min_idx = i * batchSize\n",
    "            max_idx = np.min([len(trainset), (i+1)*batchSize])\n",
    "            train_batch = trainset[min_idx: max_idx]\n",
    "            x_u, x_i, y = [], [], []\n",
    "            for uu,ii,rr in train_batch:\n",
    "                x_u.append(trainMat[int(uu),:])\n",
    "                x_i.append(trainMat[:,int(ii)])\n",
    "                y.append(float(rr))\n",
    "            _loss = net.fit(np.array(x_u), np.array(x_i), np.array(y), no_samples=10)\n",
    "            sys.stdout.write('\\r {} / {} : loss = {}'.format(i, num_batches, float('%0.6f'%_loss.item())))\n",
    "            sys.stdout.flush()\n",
    "            total_loss.append(_loss.item())\n",
    "        print(\"Epoch: %5d total_loss = %.6f\" % (epoch + 1, np.mean(total_loss)))\n",
    "        if np.mean(total_loss) < best_loss:\n",
    "            best_loss = np.mean(total_loss)\n",
    "            best_net = copy.deepcopy(net.network)\n",
    "    #torch.save(best_net, \"/data/tmpexec/BDMF_torch\")\n",
    "    #best_net = torch.load(\"/data/tmpexec/BDMF_torch\").eval()\n",
    "    #best_net = torch.load(\"/data/tmpexec/BDMF_torch\").to('cuda:0')\n",
    "    #performance   \n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    for c in range(0,shape[0]):#6040\n",
    "        scorelist = []\n",
    "        gtItem = -1\n",
    "        x_u, x_i, y_i = [], [], []\n",
    "        for uu,ii,rr in testset[c*100:(c+1)*100]:#604000\n",
    "            if rr == 1.0: \n",
    "                gtItem = ii #real hit item\n",
    "            x_u.append(np.array(trainMat[int(uu),:]))\n",
    "            x_i.append(np.array(trainMat[:,int(ii)]))\n",
    "            y_i.append(ii)\n",
    "        x_u = torch.from_numpy(np.array(x_u)).type(torch.FloatTensor).cuda()\n",
    "        x_i = torch.from_numpy(np.array(x_i)).type(torch.FloatTensor).cuda()\n",
    "        output,KL_loss = best_net(x_u, x_i)\n",
    "        output = output.cpu().data.numpy().tolist()\n",
    "        for j in range(len(y_i)):\n",
    "            scorelist.append([y_i[j],output[j]])\n",
    "        map_item_score = {}\n",
    "        for item, rate in scorelist: #turn dict\n",
    "            map_item_score[item] = rate\n",
    "        ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)##topn=10\n",
    "        hr = getHitRatio(ranklist, gtItem)\n",
    "        hits.append(hr)\n",
    "        ndcg = getNDCG(ranklist, gtItem)\n",
    "        ndcgs.append(ndcg)\n",
    "    hitratio,ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "    print (\"HR@%d=%.6f, NDCG@%d=%.6f\" % (k, hitratio, k, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
