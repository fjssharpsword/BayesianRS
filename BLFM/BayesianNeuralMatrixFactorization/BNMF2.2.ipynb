{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Model: BNMF(Bayesian Matrix Factorization for Recommeder Systems).\n",
    "2. Datatset: MovieLens-1m:https://grouplens.org/datasets/movielens/  \n",
    "3. Evaluation: HR,NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "9.0.176\n",
      "GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import heapq\n",
    "import math\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import surprise as sp\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Optimizer\n",
    "from sklearn.model_selection import KFold\n",
    "from torchvision import datasets, transforms\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, explained_variance_score,mean_squared_error\n",
    "print (torch.cuda.is_available())\n",
    "print (torch.version.cuda)\n",
    "print (torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation of metrics\n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "#dataset\n",
    "class DataSet_1M(object):\n",
    "    def __init__(self, negNum=1):\n",
    "        self.trainList, self.shape = self._getTrainData()\n",
    "        self.trainDict = self._getTrainDict()\n",
    "        self.trainMat = self._getTrainMatrix()\n",
    "        self.trainset = self._getInstances(negNum)#sample negative samples\n",
    "        self.testset = self._getTest()\n",
    "        \n",
    "    def _getTrainData(self):\n",
    "        data = []\n",
    "        filePath = '/data/fjsdata/ResData2019/BayesianMF/ml-1m.train.rating'\n",
    "        u = 0\n",
    "        i = 0\n",
    "        maxr = 0.0\n",
    "        with open(filePath, 'r') as f:\n",
    "            for line in f:\n",
    "                if line:\n",
    "                    lines = line[:-1].split(\"\\t\")\n",
    "                    user = int(lines[0])\n",
    "                    movie = int(lines[1])\n",
    "                    score = float(lines[2])\n",
    "                    data.append((user, movie, score))\n",
    "                    if user > u:u = user\n",
    "                    if movie > i:i = movie\n",
    "                    if score > maxr:maxr = score\n",
    "        self.maxRate = maxr\n",
    "        print(\"Loading Success!\\n\"\n",
    "                  \"Data Info:\\n\"\n",
    "                  \"\\tUser Num: {}\\n\"\n",
    "                  \"\\tItem Num: {}\\n\"\n",
    "                  \"\\tData Size: {}\\n\"\n",
    "                  \"\\tSparsity: {}\".format(u+1, i+1, len(data), len(data)/((u+1)*(i+1))))\n",
    "        return data, [u+1, i+1]\n",
    "\n",
    "    def _getTrainDict(self):\n",
    "        dataDict = {}\n",
    "        for i in self.trainList:\n",
    "            dataDict[(i[0], i[1])] = i[2]\n",
    "        return dataDict\n",
    "\n",
    "    def _getTrainMatrix(self):\n",
    "        train_matrix = np.zeros([self.shape[0], self.shape[1]], dtype=np.float32)\n",
    "        for i in self.trainList:\n",
    "            user = i[0]\n",
    "            movie = i[1]\n",
    "            rating = i[2]\n",
    "            train_matrix[user][movie] = rating\n",
    "        return np.array(train_matrix)\n",
    "\n",
    "    def _getInstances(self, negNum):\n",
    "        trainset = []\n",
    "        for i in self.trainList:\n",
    "            #trainset.append([i[0],i[1],i[2]])\n",
    "            trainset.append([i[0],i[1],1.0])\n",
    "            for t in range(negNum):\n",
    "                j = np.random.randint(self.shape[1])\n",
    "                while (i[0], j) in self.trainDict:\n",
    "                    j = np.random.randint(self.shape[1])\n",
    "                trainset.append([i[0],j,0.0])\n",
    "        print ('The length of Trainset: %d'%(len(trainset)))\n",
    "        return trainset\n",
    "\n",
    "    def _getTest(self):\n",
    "        #loading data\n",
    "        testset = []\n",
    "        filePath = '/data/fjsdata/ResData2019/BayesianMF/ml-1m.test.negative'\n",
    "        with open(filePath, 'r') as fd:\n",
    "            line = fd.readline()\n",
    "            while line != None and line != '':\n",
    "                arr = line.split('\\t')\n",
    "                u = eval(arr[0])[0]\n",
    "                testset.append([u, eval(arr[0])[1], 1.0])#first is one postive item\n",
    "                for i in arr[1:]:\n",
    "                    testset.append([u, int(i), 0.0]) #99 negative items\n",
    "                line = fd.readline()\n",
    "        print ('The length of Testset: %d'%(len(testset)))\n",
    "        return testset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Success!\n",
      "Data Info:\n",
      "\tUser Num: 6040\n",
      "\tItem Num: 3706\n",
      "\tData Size: 994169\n",
      "\tSparsity: 0.04441379291858915\n",
      "The length of Trainset: 1988338\n",
      "The length of Testset: 604000\n",
      "Completed training the SVD model in 42 seconds\n",
      "HR@5=0.444040, NDCG@5=0.246101\n",
      "Completed training the SVD model in 49 seconds\n",
      "HR@10=0.443709, NDCG@10=0.245849\n",
      "Completed training the SVD model in 56 seconds\n",
      "HR@15=0.446192, NDCG@15=0.246611\n",
      "Completed training the SVD model in 64 seconds\n",
      "HR@20=0.441225, NDCG@20=0.242097\n"
     ]
    }
   ],
   "source": [
    "ds1m = DataSet_1M(negNum=1)\n",
    "trainset = ds1m.trainset\n",
    "testset = ds1m.testset\n",
    "reader = sp.Reader(rating_scale=(0, 1))\n",
    "spdata = sp.Dataset.load_from_df(pd.DataFrame(trainset),reader)\n",
    "trainset_svd = spdata.build_full_trainset()\n",
    "for k in [5,10,15,20]:#iterations epoches\n",
    "    algo = sp.SVD(n_factors=k, n_epochs=20, lr_all=0.001, reg_all=0.01 )#\n",
    "    tstart = time.time()\n",
    "    algo.fit(trainset_svd)\n",
    "    elapsed = time.time() - tstart    \n",
    "    print('Completed training the SVD model in %d seconds' % int(elapsed))\n",
    "    predictions = algo.test(testset)#testset include one positive and 99 negtive sample of every user.\n",
    "    user_iid_true_est = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        user_iid_true_est[uid].append((iid, true_r, est))\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    for uid, iid_ratings in user_iid_true_est.items():\n",
    "        # Sort user ratings by estimated value\n",
    "        #iid_ratings.sort(key=lambda x: x[2], reverse=True) #sorted by est\n",
    "        scorelist = []\n",
    "        positem = -1\n",
    "        for iid, ture_r, est in iid_ratings:\n",
    "            if positem == -1: positem=iid #one positive item in first\n",
    "            scorelist.append([iid,est])\n",
    "        map_item_score = {}\n",
    "        for item, rate in scorelist: #turn dict\n",
    "            map_item_score[item] = rate\n",
    "        ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "        hr = getHitRatio(ranklist, positem)\n",
    "        hits.append(hr)\n",
    "        ndcg = getNDCG(ranklist, positem)\n",
    "        ndcgs.append(ndcg)\n",
    "    hitratio,ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "    print (\"HR@%d=%.6f, NDCG@%d=%.6f\" % (k, hitratio, k, ndcg))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2.NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Success!\n",
      "Data Info:\n",
      "\tUser Num: 6040\n",
      "\tItem Num: 3706\n",
      "\tData Size: 994169\n",
      "\tSparsity: 0.04441379291858915\n",
      "The length of Trainset: 1988338\n",
      "The length of Testset: 604000\n",
      " 429 / 995 : loss = 0.693147"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-1e5856167893>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mx_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainMat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0m_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_u\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r {} / {} : loss = {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%0.6f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0m_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class NeuralMatrixFactorization(nn.Module):\n",
    "    def __init__(self, input_dim_u, input_dim_i, factors_dim_k, num_units=[512]):\n",
    "        super(NeuralMatrixFactorization, self).__init__()\n",
    "        \n",
    "        self.input_dim_u = input_dim_u #user vector\n",
    "        self.input_dim_i = input_dim_i #item vector\n",
    "        self.factors_dim_k = factors_dim_k #latent factors vector\n",
    "        \n",
    "        # network with three hidden and k output layer\n",
    "        self.layer1_u = nn.Linear(input_dim_u, num_units[0])\n",
    "        self.layer2_u = nn.Linear(num_units[0], factors_dim_k)\n",
    "        \n",
    "        self.layer1_i = nn.Linear(input_dim_i, num_units[0])\n",
    "        self.layer2_i = nn.Linear(num_units[0], factors_dim_k)\n",
    "        \n",
    "        # activation to be used between hidden layers\n",
    "        #self.activation = nn.ReLU(inplace = True)\n",
    "    \n",
    "    def forward(self, x_u, x_i):\n",
    "    \n",
    "        x_u = x_u.view(-1, self.input_dim_u)\n",
    "        x_i = x_i.view(-1, self.input_dim_i)\n",
    "        #layer1\n",
    "        x_u = self.layer1_u(x_u)\n",
    "        x_i = self.layer1_i(x_i)\n",
    "        #layer2\n",
    "        x_u = self.layer2_u(x_u)\n",
    "        x_i = self.layer2_i(x_i)\n",
    "        \n",
    "        output = torch.sum(torch.mul(x_u,x_i),1)#pxq     \n",
    "        #output = torch.sigmoid(output)\n",
    "        return output\n",
    "    \n",
    "class NMF_Model_Wrapper:\n",
    "    def __init__(self, network, learn_rate=1e-2):\n",
    "        \n",
    "        self.learn_rate = learn_rate\n",
    "        self.network = network\n",
    "        self.network.cuda()\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr = self.learn_rate)\n",
    "        self.loss_func = nn.BCELoss()#nn.BCEWithLogitsLoss()#nn.MSELoss()\n",
    "    \n",
    "    def fit(self, x_u, x_i, y):\n",
    "        x_u = torch.from_numpy(np.array(x_u)).type(torch.FloatTensor).cuda()\n",
    "        x_i = torch.from_numpy(np.array(x_i)).type(torch.FloatTensor).cuda()\n",
    "        y = torch.from_numpy(np.array(y)).type(torch.FloatTensor).cuda()\n",
    "        # reset gradient and total loss\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.network(x_u, x_i)\n",
    "        output = torch.sigmoid(output)\n",
    "        fit_loss = self.loss_func(output, y)\n",
    "        \n",
    "        fit_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return fit_loss\n",
    "\n",
    "#dataset\n",
    "ds_1m = DataSet_1M(negNum=1)\n",
    "trainset = ds_1m.trainset\n",
    "trainMat = ds_1m.trainMat\n",
    "testset = ds_1m.testset\n",
    "shape = ds_1m.shape\n",
    "#training\n",
    "sample_size = 1\n",
    "batchSize = 2000\n",
    "for k in [5,10,15,20]:  \n",
    "    #NMF training\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()     \n",
    "    net = NMF_Model_Wrapper(network=NeuralMatrixFactorization(input_dim_u=shape[1], input_dim_i = shape[0],factors_dim_k=k))\n",
    "    best_net, best_loss = None, float('inf')\n",
    "    for epoch in range(sample_size):\n",
    "        shuffled_idx = np.random.permutation(np.arange(len(trainset)))\n",
    "        trainset = np.array(trainset)[shuffled_idx].tolist()\n",
    "        num_batches = len(trainset) // batchSize + 1 \n",
    "        total_loss = []\n",
    "        for i in range(num_batches):#batch\n",
    "            min_idx = i * batchSize\n",
    "            max_idx = np.min([len(trainset), (i+1)*batchSize])\n",
    "            train_batch = trainset[min_idx: max_idx]\n",
    "            x_u, x_i, y = [], [], []\n",
    "            for uu,ii,rr in train_batch:\n",
    "                x_u.append(trainMat[int(uu),:])\n",
    "                x_i.append(trainMat[:,int(ii)])\n",
    "                y.append(float(rr))\n",
    "            _loss = net.fit(np.array(x_u), np.array(x_i), np.array(y))\n",
    "            sys.stdout.write('\\r {} / {} : loss = {}'.format(i, num_batches, float('%0.6f'%_loss.item())))\n",
    "            sys.stdout.flush()\n",
    "            total_loss.append(_loss.item())\n",
    "        print(\"Eopch: %5d total_loss = %.6f\" % (epoch + 1, np.mean(total_loss)))\n",
    "        if np.mean(total_loss) < best_loss:\n",
    "            best_loss = np.mean(total_loss)\n",
    "            best_net = copy.deepcopy(net.network)\n",
    "    #torch.save(best_net, \"/data/tmpexec/BDMF_torch\")\n",
    "    #best_net = torch.load(\"/data/tmpexec/BDMF_torch\").eval()\n",
    "    #best_net = torch.load(\"/data/tmpexec/BDMF_torch\").to('cuda:0')\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    for c in range(0,shape[0]):#6040\n",
    "        scorelist = []\n",
    "        gtItem = -1\n",
    "        x_u, x_i, y_i = [], [], []\n",
    "        for uu,ii,rr in testset[c*100:(c+1)*100]:#604000\n",
    "            if rr == 1.0: \n",
    "                gtItem = ii\n",
    "            x_u.append(np.array(trainMat[int(uu),:]))\n",
    "            x_i.append(np.array(trainMat[:,int(ii)]))\n",
    "            y_i.append(ii)\n",
    "        x_u = torch.from_numpy(np.array(x_u)).type(torch.FloatTensor).cuda()\n",
    "        x_i = torch.from_numpy(np.array(x_i)).type(torch.FloatTensor).cuda()\n",
    "        output = best_net(x_u, x_i)\n",
    "        output = output.cpu().data.numpy().tolist()\n",
    "        for j in range(len(y_i)):\n",
    "            scorelist.append([y_i[j],output[j]])\n",
    "        map_item_score = {}\n",
    "        for item, rate in scorelist: #turn dict\n",
    "            map_item_score[item] = rate\n",
    "        ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#topn=10\n",
    "        hr = getHitRatio(ranklist, gtItem)\n",
    "        hits.append(hr)\n",
    "        ndcg = getNDCG(ranklist, gtItem)\n",
    "        ndcgs.append(ndcg)\n",
    "    hitratio,ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "    print (\"HR@%d=%.6f, NDCG@%d=%.6f\" % (k, hitratio, k, ndcg))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3.BDMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_gaussian_loss(output, target, sigma, no_dim, sum_reduce=True):\n",
    "    exponent = -0.5*(target - output)**2/sigma**2\n",
    "    log_coeff = -no_dim*torch.log(sigma) - 0.5*no_dim*np.log(2*np.pi)\n",
    "    \n",
    "    if sum_reduce:\n",
    "        return -(log_coeff + exponent).sum()\n",
    "    else:\n",
    "        return -(log_coeff + exponent)\n",
    "    \n",
    "class gaussian:\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def loglik(self, weights):\n",
    "        exponent = -0.5*(weights - self.mu)**2/self.sigma**2\n",
    "        log_coeff = -0.5*(np.log(2*np.pi) + 2*np.log(self.sigma))\n",
    "        \n",
    "        return (exponent + log_coeff).sum()\n",
    "    \n",
    "class BayesLinear_Normalq(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, prior):\n",
    "        super(BayesLinear_Normalq, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.prior = prior\n",
    "        \n",
    "        self.weight_mus = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-0.01, 0.01))\n",
    "        self.weight_rhos = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-3, -3))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # sample gaussian noise for each weight\n",
    "        weight_epsilons = Variable(self.weight_mus.data.new(self.weight_mus.size()).normal_())      \n",
    "        # calculate the weight stds from the rho parameters\n",
    "        weight_stds = torch.log(1 + torch.exp(self.weight_rhos))\n",
    "        # calculate samples from the posterior from the sampled noise and mus/stds\n",
    "        weight_sample = self.weight_mus + weight_epsilons*weight_stds\n",
    "            \n",
    "        torch.cuda.synchronize()\n",
    "        output = torch.mm(x, weight_sample)\n",
    "            \n",
    "        # computing the KL loss term\n",
    "        #reference: https://github.com/jojonki/AutoEncoders/blob/master/kl_divergence_between_two_gaussians.pdf\n",
    "        prior_cov, varpost_cov = self.prior.sigma**2, weight_stds**2\n",
    "        KL_loss = 0.5*(torch.log(prior_cov/varpost_cov)).sum() - 0.5*weight_stds.numel()\n",
    "        KL_loss = KL_loss + 0.5*(varpost_cov/prior_cov).sum()\n",
    "        KL_loss = KL_loss + 0.5*((self.weight_mus - self.prior.mu)**2/prior_cov).sum()\n",
    "            \n",
    "        return output, KL_loss\n",
    "    \n",
    "class BBP_Model(nn.Module):\n",
    "    def __init__(self, input_dim_u, input_dim_i, factors_dim_k, num_units=[512]):\n",
    "        super(BBP_Model, self).__init__()\n",
    "        \n",
    "        self.input_dim_u = input_dim_u #user vector\n",
    "        self.input_dim_i = input_dim_i #item vector\n",
    "        self.factors_dim_k = factors_dim_k #latent factors vector\n",
    "        \n",
    "        # network with three hidden and k output layer\n",
    "        self.layer1_u = BayesLinear_Normalq(input_dim_u, num_units[0], gaussian(0, 3))\n",
    "        self.layer2_u = BayesLinear_Normalq(num_units[0], factors_dim_k, gaussian(0, 3))\n",
    "        \n",
    "        self.layer1_i = BayesLinear_Normalq(input_dim_i, num_units[0], gaussian(0, 3))\n",
    "        self.layer2_i = BayesLinear_Normalq(num_units[0], factors_dim_k, gaussian(0, 3))\n",
    "        \n",
    "        # activation to be used between hidden layers\n",
    "        self.activation = nn.ReLU(inplace = True)\n",
    "        # noise\n",
    "        self.log_noise = nn.Parameter(torch.cuda.FloatTensor([3]))\n",
    "    \n",
    "    def forward(self, x_u, x_i):\n",
    "        \n",
    "        KL_loss_total = 0\n",
    "        x_u = x_u.view(-1, self.input_dim_u)\n",
    "        x_i = x_i.view(-1, self.input_dim_i)\n",
    "        #layer1\n",
    "        x_u, KL_loss_u = self.layer1_u(x_u)\n",
    "        x_u = self.activation(x_u)\n",
    "        x_i, KL_loss_i = self.layer1_i(x_i)\n",
    "        x_i = self.activation(x_i)\n",
    "        KL_loss_total = KL_loss_total + KL_loss_u + KL_loss_i\n",
    "        #layer2\n",
    "        x_u, KL_loss_u = self.layer2_u(x_u)\n",
    "        x_u = self.activation(x_u)\n",
    "        x_i, KL_loss_i = self.layer2_i(x_i)\n",
    "        x_i = self.activation(x_i)\n",
    "        KL_loss_total = KL_loss_total + KL_loss_u + KL_loss_i\n",
    "        #pxq\n",
    "        output = torch.sum(torch.mul(x_u,x_i),1)\n",
    "        \n",
    "        return output, KL_loss_total\n",
    "    \n",
    "class BBP_Model_Wrapper:\n",
    "    def __init__(self, network, learn_rate=1e-2):\n",
    "        \n",
    "        self.learn_rate = learn_rate\n",
    "        self.network = network\n",
    "        self.network.cuda()\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr = self.learn_rate)\n",
    "        self.loss_func = log_gaussian_loss#nn.MSELoss() \n",
    "    \n",
    "    def fit(self, x_u, x_i, y, no_samples):\n",
    "        len_sql = y.shape[0]\n",
    "        x_u = torch.from_numpy(np.array(x_u)).type(torch.FloatTensor).cuda()\n",
    "        x_i = torch.from_numpy(np.array(x_i)).type(torch.FloatTensor).cuda()\n",
    "        y = torch.from_numpy(np.array(y)).type(torch.FloatTensor).cuda()\n",
    "        # reset gradient and total loss\n",
    "        self.optimizer.zero_grad()\n",
    "        fit_loss_total = 0\n",
    "        KL_loss_total = 0\n",
    "        for i in range(no_samples):\n",
    "            output, KL_loss = self.network(x_u, x_i)\n",
    "            KL_loss_total = KL_loss_total + KL_loss\n",
    "            # calculate fit loss based on mean and standard deviation of output\n",
    "            fit_loss = self.loss_func(output, y, self.network.log_noise.exp(), 1) \n",
    "            fit_loss_total = fit_loss_total + fit_loss\n",
    "        \n",
    "        total_loss = (fit_loss_total + KL_loss_total)/(no_samples*len_sql)\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return total_loss\n",
    "    \n",
    "#training model\n",
    "num_epochs = 10\n",
    "batchSize = 10000\n",
    "#ds_1m = DataSet_1M()\n",
    "#trainset = ds_1m.trainset\n",
    "#trainMat = ds_1m.trainMat\n",
    "#testset = ds_1m.testset\n",
    "#shape = ds_1m.shape\n",
    "for k in [5,10,15,20]:  \n",
    "    torch.cuda.empty_cache()\n",
    "    net = BBP_Model_Wrapper(network=BBP_Model(input_dim_u=shape[1], input_dim_i = shape[0],factors_dim_k=k))\n",
    "    best_net, best_loss = None, float('inf')\n",
    "    for epoch in range(num_epochs): #iteration\n",
    "        shuffled_idx = np.random.permutation(np.arange(len(trainset)))\n",
    "        trainset = np.array(trainset)[shuffled_idx].tolist()\n",
    "        num_batches = len(trainset) // batchSize + 1 \n",
    "        total_loss = []\n",
    "        for i in range(num_batches):#batch\n",
    "            min_idx = i * batchSize\n",
    "            max_idx = np.min([len(trainset), (i+1)*batchSize])\n",
    "            train_batch = trainset[min_idx: max_idx]\n",
    "            x_u, x_i, y = [], [], []\n",
    "            for uu,ii,rr in train_batch:\n",
    "                x_u.append(trainMat[int(uu),:])\n",
    "                x_i.append(trainMat[:,int(ii)])\n",
    "                y.append(float(rr))\n",
    "            _loss = net.fit(np.array(x_u), np.array(x_i), np.array(y), no_samples=10)\n",
    "            sys.stdout.write('\\r {} / {} : loss = {}'.format(i, num_batches, float('%0.6f'%_loss.item())))\n",
    "            sys.stdout.flush()\n",
    "            total_loss.append(_loss.item())\n",
    "        print(\"Epoch: %5d total_loss = %.6f\" % (epoch + 1, np.mean(total_loss)))\n",
    "        if np.mean(total_loss) < best_loss:\n",
    "            best_loss = np.mean(total_loss)\n",
    "            best_net = copy.deepcopy(net.network)\n",
    "    #torch.save(best_net, \"/data/tmpexec/BDMF_torch\")\n",
    "    #best_net = torch.load(\"/data/tmpexec/BDMF_torch\").eval()\n",
    "    #best_net = torch.load(\"/data/tmpexec/BDMF_torch\").to('cuda:0')\n",
    "    #performance   \n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    for c in range(0,shape[0]):#6040\n",
    "        scorelist = []\n",
    "        gtItem = -1\n",
    "        x_u, x_i, y_i = [], [], []\n",
    "        for uu,ii,rr in testset[c*100:(c+1)*100]:#604000\n",
    "            if rr == 1.0: \n",
    "                gtItem = ii #real hit item\n",
    "            x_u.append(np.array(trainMat[int(uu),:]))\n",
    "            x_i.append(np.array(trainMat[:,int(ii)]))\n",
    "            y_i.append(ii)\n",
    "        x_u = torch.from_numpy(np.array(x_u)).type(torch.FloatTensor).cuda()\n",
    "        x_i = torch.from_numpy(np.array(x_i)).type(torch.FloatTensor).cuda()\n",
    "        output,KL_loss = best_net(x_u, x_i)\n",
    "        output = output.cpu().data.numpy().tolist()\n",
    "        for j in range(len(y_i)):\n",
    "            scorelist.append([y_i[j],output[j]])\n",
    "        map_item_score = {}\n",
    "        for item, rate in scorelist: #turn dict\n",
    "            map_item_score[item] = rate\n",
    "        ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)##topn=10\n",
    "        hr = getHitRatio(ranklist, gtItem)\n",
    "        hits.append(hr)\n",
    "        ndcg = getNDCG(ranklist, gtItem)\n",
    "        ndcgs.append(ndcg)\n",
    "    hitratio,ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "    print (\"HR@%d=%.6f, NDCG@%d=%.6f\" % (k, hitratio, k, ndcg))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4.BNMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianMatrixFactorization():\n",
    "    \"\"\"\n",
    "    Bayesian Matrix Factorization model\n",
    "    R = PxQ\n",
    "    p ~ N(p|0, alpha^(-1)I)\n",
    "    q ~ N(q|0, alpha^(-1)I)\n",
    "    r = p @ q\n",
    "    t ~ N(r|p @ q, beta^(-1))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha_p:float=1., alpha_q:float=1., beta:float=1.):\n",
    "        \"\"\"\n",
    "        ----------\n",
    "        n_u, n_i: the number of users and items, respectively.\n",
    "        k : the number of latent factors\n",
    "        \"\"\"\n",
    "        self.alpha_p = alpha_p\n",
    "        self.alpha_q = alpha_q\n",
    "        self.beta = beta\n",
    "        #posterior of p,q \n",
    "        self.pos_mean_p = None\n",
    "        self.pos_precision_p = None\n",
    "        self.pos_mean_q = None\n",
    "        self.pos_precision_q = None\n",
    "\n",
    "    def fit(self, R:np.ndarray, k:int=5):\n",
    "        \"\"\"\n",
    "        bayesian update of parameters given training dataset\n",
    "        Parameters\n",
    "        ----------\n",
    "        R : (u,i) np.ndarray\n",
    "            training data independent variable, u is the number of users, i is the number of items.\n",
    "        k : int, the number of latent factors.\n",
    "        \"\"\"\n",
    "        #1. generate matrices P, Q\n",
    "        P = np.random.normal(0,self.alpha_p,(R.shape[0],k))#uxk\n",
    "        Q = np.random.normal(0,self.alpha_q,(R.shape[1],k))#ixk\n",
    "        #2.calculate the posterior with analytical solution\n",
    "        self.pos_precision_p = self.alpha_p + self.beta * Q @ Q.T # ixi\n",
    "        self.pos_mean_p = self.beta * R @ np.linalg.inv(self.pos_precision_p) @ Q # uxi,ixi,ixk -> uxk\n",
    "        self.pos_precision_q = self.alpha_q + self.beta * P @ P.T # uxu\n",
    "        self.pos_mean_q = self.beta * R.T @ np.linalg.inv(self.pos_precision_q) @ P # ixu,uxu,uxk -> ixk\n",
    "\n",
    "    def predict(self, sample_size:int=None):\n",
    "        \"\"\"\n",
    "        return mean  of predictive distribution\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample_size : int, optional\n",
    "            number of samples to draw from the predictive distribution\n",
    "            (the default is None, no sampling from the distribution)\n",
    "        Returns\n",
    "        -------\n",
    "        R_pred : (u,i) np.ndarray\n",
    "            mean of the predictive distribution\n",
    "        R_pred_sample : (u,i,sample_size) np.ndarray\n",
    "            samples from the predictive distribution\n",
    "        \"\"\"\n",
    "        if sample_size is not None:\n",
    "            R_sample = []\n",
    "            for i in range(sample_size):\n",
    "                p_sample, q_sample = [], []\n",
    "                for k in range(self.pos_mean_p.shape[1]):#latent factors    \n",
    "                    mean_p = self.pos_mean_p[:,k]\n",
    "                    mean_q = self.pos_mean_q[:,k]\n",
    "                    p_sample_k = np.random.multivariate_normal(mean_p, np.linalg.inv(self.pos_precision_q), size=1)\n",
    "                    q_sample_k = np.random.multivariate_normal(mean_q, np.linalg.inv(self.pos_precision_p), size=1)\n",
    "                    p_sample.append(p_sample_k.flatten())\n",
    "                    q_sample.append(q_sample_k.flatten())\n",
    "                R_sample.append(np.dot(np.array(p_sample).T, np.array(q_sample)))\n",
    "            return  R_sample #uxi\n",
    "        \n",
    "        R_pred = self.pos_mean_p @ self.pos_mean_q.T #R = PxQ\n",
    "        return R_pred #uxi\n",
    "    \n",
    "class NeuralMatrixFactorization(nn.Module):\n",
    "    def __init__(self, input_dim_u, input_dim_i, factors_dim_k, num_units=[512]):\n",
    "        super(NeuralMatrixFactorization, self).__init__()\n",
    "        \n",
    "        self.input_dim_u = input_dim_u #user vector\n",
    "        self.input_dim_i = input_dim_i #item vector\n",
    "        self.factors_dim_k = factors_dim_k #latent factors vector\n",
    "        \n",
    "        # network with three hidden and k output layer\n",
    "        self.layer1_u = nn.Linear(input_dim_u, num_units[0])\n",
    "        self.layer2_u = nn.Linear(num_units[0], factors_dim_k)\n",
    "        \n",
    "        self.layer1_i = nn.Linear(input_dim_i, num_units[0])\n",
    "        self.layer2_i = nn.Linear(num_units[0], factors_dim_k)\n",
    "        \n",
    "        # activation to be used between hidden layers\n",
    "        self.activation = nn.ReLU(inplace = True)\n",
    "    \n",
    "    def forward(self, x_u, x_i):\n",
    "    \n",
    "        x_u = x_u.view(-1, self.input_dim_u)\n",
    "        x_i = x_i.view(-1, self.input_dim_i)\n",
    "        #layer1\n",
    "        x_u = self.layer1_u(x_u)\n",
    "        x_u = self.activation(x_u)\n",
    "        x_i = self.layer1_i(x_i)\n",
    "        x_i = self.activation(x_i)\n",
    "        #layer2\n",
    "        x_u = self.layer2_u(x_u)\n",
    "        x_u = self.activation(x_u)\n",
    "        x_i = self.layer2_i(x_i)\n",
    "        x_i = self.activation(x_i)\n",
    "        \n",
    "        output = torch.sum(torch.mul(x_u,x_i),1)#pxq\n",
    "        #output = torch.sigmoid(output)\n",
    "        return output\n",
    "    \n",
    "class NMF_Model_Wrapper:\n",
    "    def __init__(self, network, learn_rate=1e-2):\n",
    "        \n",
    "        self.learn_rate = learn_rate\n",
    "        self.network = network\n",
    "        self.network.cuda()\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr = self.learn_rate)\n",
    "        self.loss_func = nn.BCELoss()#nn.BCEWithLogitsLoss()#nn.MSELoss()\n",
    "    \n",
    "    def fit(self, x_u, x_i, y):\n",
    "        x_u = torch.from_numpy(np.array(x_u)).type(torch.FloatTensor).cuda()\n",
    "        x_i = torch.from_numpy(np.array(x_i)).type(torch.FloatTensor).cuda()\n",
    "        y = torch.from_numpy(np.array(y)).type(torch.FloatTensor).cuda()\n",
    "        # reset gradient and total loss\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.network(x_u, x_i)\n",
    "        output = torch.sigmoid(output)\n",
    "        fit_loss = self.loss_func(output, y)\n",
    "        \n",
    "        fit_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return fit_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Success!\n",
      "Data Info:\n",
      "\tUser Num: 6040\n",
      "\tItem Num: 3706\n",
      "\tData Size: 994169\n",
      "\tSparsity: 0.04441379291858915\n",
      "The length of Trainset: 994169\n",
      "The length of Testset: 604000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:66: RuntimeWarning: covariance is not positive-semidefinite.\n",
      "/root/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:67: RuntimeWarning: covariance is not positive-semidefinite.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 497 / 498 : loss = 0.184566Sample:     1 total_loss = 0.189159\n",
      " 497 / 498 : loss = 0.176363Sample:     2 total_loss = 0.193778\n",
      "HR@5=0.327980, NDCG@5=0.176795\n",
      " 497 / 498 : loss = 0.045116Sample:     1 total_loss = 0.054246\n",
      " 497 / 498 : loss = 0.094334Sample:     2 total_loss = 0.083603\n",
      "HR@10=0.266225, NDCG@10=0.140981\n",
      " 497 / 498 : loss = 0.008203Sample:     1 total_loss = 0.002754\n",
      " 497 / 498 : loss = 0.000693Sample:     2 total_loss = 0.001895\n",
      "HR@15=0.297351, NDCG@15=0.150598\n",
      " 497 / 498 : loss = 0.000693Sample:     1 total_loss = 0.001021\n",
      " 497 / 498 : loss = 0.000347Sample:     2 total_loss = 0.000666\n"
     ]
    }
   ],
   "source": [
    "#dataset\n",
    "ds_1m = DataSet_1M(negNum=0)\n",
    "trainset = ds_1m.trainset\n",
    "trainMat = ds_1m.trainMat\n",
    "testset = ds_1m.testset\n",
    "shape = ds_1m.shape\n",
    "#training\n",
    "sample_size = 2 # samples \n",
    "batchSize = 2000\n",
    "for k in [5,10,15,20]:  \n",
    "    #get approximate matrice\n",
    "    bmf = BayesianMatrixFactorization()\n",
    "    bmf.fit(R=trainMat, k=k)\n",
    "    R_sample = bmf.predict(sample_size=sample_size)\n",
    "    #NMF training\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()     \n",
    "    net = NMF_Model_Wrapper(network=NeuralMatrixFactorization(input_dim_u=shape[1], input_dim_i = shape[0],factors_dim_k=k))\n",
    "    best_net, best_loss = None, float('inf')\n",
    "    for iSpl, iR in enumerate(R_sample):\n",
    "        shuffled_idx = np.random.permutation(np.arange(len(trainset)))\n",
    "        trainset = np.array(trainset)[shuffled_idx].tolist()\n",
    "        num_batches = len(trainset) // batchSize + 1 \n",
    "        total_loss = []\n",
    "        for i in range(num_batches):#batch\n",
    "            min_idx = i * batchSize\n",
    "            max_idx = np.min([len(trainset), (i+1)*batchSize])\n",
    "            train_batch = trainset[min_idx: max_idx]\n",
    "            x_u, x_i, y = [], [], []\n",
    "            for uu,ii,rr in train_batch:\n",
    "                x_u.append(iR[int(uu),:])\n",
    "                x_i.append(iR[:,int(ii)])\n",
    "                y.append(float(rr))\n",
    "            _loss = net.fit(np.array(x_u), np.array(x_i), np.array(y))\n",
    "            sys.stdout.write('\\r {} / {} : loss = {}'.format(i, num_batches, float('%0.6f'%_loss.item())))\n",
    "            sys.stdout.flush()\n",
    "            total_loss.append(_loss.item())\n",
    "        print(\"Sample: %5d total_loss = %.6f\" % (iSpl + 1, np.mean(total_loss)))\n",
    "        if np.mean(total_loss) < best_loss:\n",
    "            best_loss = np.mean(total_loss)\n",
    "            best_net = copy.deepcopy(net.network)\n",
    "    #torch.save(best_net, \"/data/tmpexec/BDMF_torch\")\n",
    "    #best_net = torch.load(\"/data/tmpexec/BDMF_torch\").eval()\n",
    "    #best_net = torch.load(\"/data/tmpexec/BDMF_torch\").to('cuda:0')\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    for c in range(0,shape[0]):#6040\n",
    "        scorelist = []\n",
    "        gtItem = -1\n",
    "        x_u, x_i, y_i = [], [], []\n",
    "        for uu,ii,rr in testset[c*100:(c+1)*100]:#604000\n",
    "            if rr == 1.0: \n",
    "                gtItem = ii\n",
    "            x_u.append(np.array(trainMat[int(uu),:]))\n",
    "            x_i.append(np.array(trainMat[:,int(ii)]))\n",
    "            y_i.append(ii)\n",
    "        x_u = torch.from_numpy(np.array(x_u)).type(torch.FloatTensor).cuda()\n",
    "        x_i = torch.from_numpy(np.array(x_i)).type(torch.FloatTensor).cuda()\n",
    "        output = best_net(x_u, x_i)\n",
    "        output = output.cpu().data.numpy().tolist()\n",
    "        for j in range(len(y_i)):\n",
    "            scorelist.append([y_i[j],output[j]])\n",
    "        map_item_score = {}\n",
    "        for item, rate in scorelist: #turn dict\n",
    "            map_item_score[item] = rate\n",
    "        ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#topn=10\n",
    "        hr = getHitRatio(ranklist, gtItem)\n",
    "        hits.append(hr)\n",
    "        ndcg = getNDCG(ranklist, gtItem)\n",
    "        ndcgs.append(ndcg)\n",
    "    hitratio,ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "    print (\"HR@%d=%.6f, NDCG@%d=%.6f\" % (k, hitratio, k, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
