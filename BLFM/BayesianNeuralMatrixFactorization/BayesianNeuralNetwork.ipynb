{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading faiss with AVX2 support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import math\n",
    "import random\n",
    "import heapq \n",
    "import time\n",
    "from PIL import Image\n",
    "from io import StringIO,BytesIO \n",
    "import cv2\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import reduce\n",
    "import wfdb#https://github.com/MIT-LCP/wfdb-python\n",
    "from wfdb import processing\n",
    "import faiss \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "torch.cuda.set_device(0)\n",
    "print (torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 0 : loss = 14933.331055\r",
      " 1 : loss = 14929.307617\r",
      " 2 : loss = 14925.28418\r",
      " 3 : loss = 14921.260742\r",
      " 4 : loss = 14917.244141\r",
      " 5 : loss = 14913.22168\r",
      " 6 : loss = 14909.202148\r",
      " 7 : loss = 14905.173828\r",
      " 8 : loss = 14901.158203\r",
      " 9 : loss = 14897.131836torch.Size([2, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:93: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "def log_gaussian_loss(output, target, sigma, no_dim, sum_reduce=True):\n",
    "    exponent = -0.5*(target - output)**2/sigma**2\n",
    "    log_coeff = -no_dim*torch.log(sigma) - 0.5*no_dim*np.log(2*np.pi)\n",
    "    \n",
    "    if sum_reduce:\n",
    "        return -(log_coeff + exponent).sum()\n",
    "    else:\n",
    "        return -(log_coeff + exponent)\n",
    "    \n",
    "class gaussian:\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def loglik(self, weights):\n",
    "        exponent = -0.5*(weights - self.mu)**2/self.sigma**2\n",
    "        log_coeff = -0.5*(np.log(2*np.pi) + 2*np.log(self.sigma))\n",
    "        \n",
    "        return (exponent + log_coeff).sum()\n",
    "    \n",
    "class BayesLinear_Normalq(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, prior):\n",
    "        super(BayesLinear_Normalq, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.prior = prior\n",
    "        \n",
    "        self.weight_mus = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-0.01, 0.01))\n",
    "        self.weight_rhos = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-3, -3))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # sample gaussian noise for each weight\n",
    "        weight_epsilons = Variable(self.weight_mus.data.new(self.weight_mus.size()).normal_())      \n",
    "        # calculate the weight stds from the rho parameters\n",
    "        weight_stds = torch.log(1 + torch.exp(self.weight_rhos))\n",
    "        # calculate samples from the posterior from the sampled noise and mus/stds\n",
    "        weight_sample = self.weight_mus + weight_epsilons*weight_stds\n",
    "    \n",
    "        output = torch.mm(x, weight_sample)\n",
    "            \n",
    "        # computing the KL loss term\n",
    "        #reference: https://github.com/jojonki/AutoEncoders/blob/master/kl_divergence_between_two_gaussians.pdf\n",
    "        prior_cov, varpost_cov = self.prior.sigma**2, weight_stds**2\n",
    "        KL_loss = 0.5*(torch.log(prior_cov/varpost_cov)).sum() - 0.5*weight_stds.numel()\n",
    "        KL_loss = KL_loss + 0.5*(varpost_cov/prior_cov).sum()\n",
    "        KL_loss = KL_loss + 0.5*((self.weight_mus - self.prior.mu)**2/prior_cov).sum()\n",
    "            \n",
    "        return output, KL_loss\n",
    "\n",
    "\n",
    "class BayesianNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=50, num_units=[128,256], output_dim=8):\n",
    "        super(BayesianNeuralNetwork, self).__init__()\n",
    "        \n",
    "        # network with Bayesian linear.\n",
    "        self.layer1 = BayesLinear_Normalq(input_dim, num_units[0], gaussian(0, 3))\n",
    "        self.layer2 = BayesLinear_Normalq(num_units[0], num_units[1], gaussian(0, 3))\n",
    "        self.layer3 = BayesLinear_Normalq(num_units[1], output_dim, gaussian(0, 3))\n",
    "        \n",
    "        # activation to be used between hidden layers\n",
    "        self.activation = nn.ReLU(inplace = True)\n",
    "        # noise\n",
    "        self.log_noise = nn.Parameter(torch.cuda.FloatTensor([3]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        KL_loss_total = 0\n",
    "        #x = x.view(-1, self.input_dim)\n",
    "        x = x.view(x.size(0),-1) \n",
    "        #layer1\n",
    "        x, KL_loss = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        KL_loss_total = KL_loss_total + KL_loss\n",
    "        #layer2\n",
    "        x, KL_loss = self.layer2(x)\n",
    "        x = self.activation(x) \n",
    "        KL_loss_total = KL_loss_total + KL_loss\n",
    "        #layer3\n",
    "        out, KL_loss = self.layer3(x)\n",
    "        KL_loss_total = KL_loss_total + KL_loss\n",
    "        \n",
    "        return x, out, KL_loss_total\n",
    "\n",
    "\n",
    "x1 = torch.rand(10,50).cuda()\n",
    "y = torch.LongTensor([0,1,2,3,4,5,6,7,0,1]).cuda()\n",
    "model = BayesianNeuralNetwork().cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    _,out,KL_loss = model(x1)\n",
    "    out = F.log_softmax(out)\n",
    "    fit_loss = F.nll_loss(out, y)\n",
    "    loss =  (KL_loss+fit_loss)/len(y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    sys.stdout.write('\\r {} : loss = {}'.format(epoch, float('%0.6f'%loss.item())))\n",
    "    #sys.stdout.flush()\n",
    "#output\n",
    "x2 = torch.rand(2,50).cuda()\n",
    "x2,_,_ = model(x2)\n",
    "#print (x2)\n",
    "print (x2.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
