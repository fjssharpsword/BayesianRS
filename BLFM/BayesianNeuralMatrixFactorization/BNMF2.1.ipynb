{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Model: BNMF(Bayesian Matrix Factorization for Recommeder Systems).\n",
    "2. Datatset: MovieLens-1m:https://grouplens.org/datasets/movielens/  \n",
    "3. Evaluation: HR,NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "9.0.176\n",
      "GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import heapq\n",
    "import math\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import surprise as sp\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Optimizer\n",
    "from sklearn.model_selection import KFold\n",
    "from torchvision import datasets, transforms\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, explained_variance_score,mean_squared_error\n",
    "print (torch.cuda.is_available())\n",
    "print (torch.version.cuda)\n",
    "print (torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation of metrics\n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "#dataset\n",
    "class DataSet_1M(object):\n",
    "    def __init__(self, negNum=1):\n",
    "        self.trainList, self.shape = self._getTrainData()\n",
    "        self.trainDict = self._getTrainDict()\n",
    "        self.trainMat = self._getTrainMatrix()\n",
    "        self.trainset = self._getInstances(negNum)#sample negative samples\n",
    "        self.testset = self._getTest()\n",
    "        \n",
    "    def _getTrainData(self):\n",
    "        data = []\n",
    "        filePath = '/data/fjsdata/ResData2019/BayesianMF/ml-1m.train.rating'\n",
    "        u = 0\n",
    "        i = 0\n",
    "        maxr = 0.0\n",
    "        with open(filePath, 'r') as f:\n",
    "            for line in f:\n",
    "                if line:\n",
    "                    lines = line[:-1].split(\"\\t\")\n",
    "                    user = int(lines[0])\n",
    "                    movie = int(lines[1])\n",
    "                    score = float(lines[2])\n",
    "                    data.append((user, movie, score))\n",
    "                    if user > u:u = user\n",
    "                    if movie > i:i = movie\n",
    "                    if score > maxr:maxr = score\n",
    "        self.maxRate = maxr\n",
    "        print(\"Loading Success!\\n\"\n",
    "                  \"Data Info:\\n\"\n",
    "                  \"\\tUser Num: {}\\n\"\n",
    "                  \"\\tItem Num: {}\\n\"\n",
    "                  \"\\tData Size: {}\\n\"\n",
    "                  \"\\tSparsity: {}\".format(u+1, i+1, len(data), len(data)/((u+1)*(i+1))))\n",
    "        return data, [u+1, i+1]\n",
    "\n",
    "    def _getTrainDict(self):\n",
    "        dataDict = {}\n",
    "        for i in self.trainList:\n",
    "            dataDict[(i[0], i[1])] = i[2]\n",
    "        return dataDict\n",
    "\n",
    "    def _getTrainMatrix(self):\n",
    "        train_matrix = np.zeros([self.shape[0], self.shape[1]], dtype=np.float32)\n",
    "        for i in self.trainList:\n",
    "            user = i[0]\n",
    "            movie = i[1]\n",
    "            rating = i[2]\n",
    "            train_matrix[user][movie] = rating\n",
    "        return np.array(train_matrix)\n",
    "\n",
    "    def _getInstances(self, negNum):\n",
    "        trainset = []\n",
    "        for i in self.trainList:\n",
    "            #trainset.append([i[0],i[1],i[2]])\n",
    "            trainset.append([i[0],i[1],1.0])\n",
    "            for t in range(negNum):\n",
    "                j = np.random.randint(self.shape[1])\n",
    "                while (i[0], j) in self.trainDict:\n",
    "                    j = np.random.randint(self.shape[1])\n",
    "                trainset.append([i[0],j,0.0])\n",
    "        print ('The length of Trainset: %d'%(len(trainset)))\n",
    "        return trainset\n",
    "\n",
    "    def _getTest(self):\n",
    "        #loading data\n",
    "        testset = []\n",
    "        filePath = '/data/fjsdata/ResData2019/BayesianMF/ml-1m.test.negative'\n",
    "        with open(filePath, 'r') as fd:\n",
    "            line = fd.readline()\n",
    "            while line != None and line != '':\n",
    "                arr = line.split('\\t')\n",
    "                u = eval(arr[0])[0]\n",
    "                testset.append([u, eval(arr[0])[1], 1.0])#first is one postive item\n",
    "                for i in arr[1:]:\n",
    "                    testset.append([u, int(i), 0.0]) #99 negative items\n",
    "                line = fd.readline()\n",
    "        print ('The length of Testset: %d'%(len(testset)))\n",
    "        return testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianMatrixFactorization():\n",
    "    \"\"\"\n",
    "    Bayesian Matrix Factorization model\n",
    "    R = PxQ\n",
    "    p ~ N(p|0, alpha^(-1)I)\n",
    "    q ~ N(q|0, alpha^(-1)I)\n",
    "    r = p @ q\n",
    "    t ~ N(r|p @ q, beta^(-1))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha_p:float=1., alpha_q:float=1., beta:float=1.):\n",
    "        \"\"\"\n",
    "        ----------\n",
    "        n_u, n_i: the number of users and items, respectively.\n",
    "        k : the number of latent factors\n",
    "        \"\"\"\n",
    "        self.alpha_p = alpha_p\n",
    "        self.alpha_q = alpha_q\n",
    "        self.beta = beta\n",
    "        #posterior of p,q \n",
    "        self.pos_mean_p = None\n",
    "        self.pos_precision_p = None\n",
    "        self.pos_mean_q = None\n",
    "        self.pos_precision_q = None\n",
    "\n",
    "    def fit(self, R:np.ndarray, k:int=5):\n",
    "        \"\"\"\n",
    "        bayesian update of parameters given training dataset\n",
    "        Parameters\n",
    "        ----------\n",
    "        R : (u,i) np.ndarray\n",
    "            training data independent variable, u is the number of users, i is the number of items.\n",
    "        k : int, the number of latent factors.\n",
    "        \"\"\"\n",
    "        #1. generate matrices P, Q\n",
    "        P = np.random.normal(0,self.alpha_p,(R.shape[0],k))#uxk\n",
    "        Q = np.random.normal(0,self.alpha_q,(R.shape[1],k))#ixk\n",
    "        #2.calculate the posterior with analytical solution\n",
    "        self.pos_precision_p = self.alpha_p + self.beta * Q @ Q.T # ixi\n",
    "        self.pos_mean_p = self.beta * R @ np.linalg.inv(self.pos_precision_p) @ Q # uxi,ixi,ixk -> uxk\n",
    "        self.pos_precision_q = self.alpha_q + self.beta * P @ P.T # uxu\n",
    "        self.pos_mean_q = self.beta * R.T @ np.linalg.inv(self.pos_precision_q) @ P # ixu,uxu,uxk -> ixk\n",
    "\n",
    "    def predict(self, sample_size:int=None):\n",
    "        \"\"\"\n",
    "        return mean  of predictive distribution\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample_size : int, optional\n",
    "            number of samples to draw from the predictive distribution\n",
    "            (the default is None, no sampling from the distribution)\n",
    "        Returns\n",
    "        -------\n",
    "        R_pred : (u,i) np.ndarray\n",
    "            mean of the predictive distribution\n",
    "        R_pred_sample : (u,i,sample_size) np.ndarray\n",
    "            samples from the predictive distribution\n",
    "        \"\"\"\n",
    "        if sample_size is not None:\n",
    "            R_sample = []\n",
    "            for i in range(sample_size):\n",
    "                p_sample, q_sample = [], []\n",
    "                for k in range(self.pos_mean_p.shape[1]):#latent factors    \n",
    "                    mean_p = self.pos_mean_p[:,k]\n",
    "                    mean_q = self.pos_mean_q[:,k]\n",
    "                    p_sample_k = np.random.multivariate_normal(mean_p, np.linalg.inv(self.pos_precision_q), size=1)\n",
    "                    q_sample_k = np.random.multivariate_normal(mean_q, np.linalg.inv(self.pos_precision_p), size=1)\n",
    "                    p_sample.append(p_sample_k.flatten())\n",
    "                    q_sample.append(q_sample_k.flatten())\n",
    "                R_sample.append(np.dot(np.array(p_sample).T, np.array(q_sample)))\n",
    "            return  R_sample #uxi\n",
    "        \n",
    "        R_pred = self.pos_mean_p @ self.pos_mean_q.T #R = PxQ\n",
    "        return R_pred #uxi\n",
    "    \n",
    "class NeuralMatrixFactorization(nn.Module):\n",
    "    def __init__(self, input_dim_u, input_dim_i, factors_dim_k, num_units=[512]):\n",
    "        super(NeuralMatrixFactorization, self).__init__()\n",
    "        \n",
    "        self.input_dim_u = input_dim_u #user vector\n",
    "        self.input_dim_i = input_dim_i #item vector\n",
    "        self.factors_dim_k = factors_dim_k #latent factors vector\n",
    "        \n",
    "        # network with three hidden and k output layer\n",
    "        self.layer1_u = nn.Linear(input_dim_u, num_units[0])\n",
    "        self.layer2_u = nn.Linear(num_units[0], factors_dim_k)\n",
    "        \n",
    "        self.layer1_i = nn.Linear(input_dim_i, num_units[0])\n",
    "        self.layer2_i = nn.Linear(num_units[0], factors_dim_k)\n",
    "        \n",
    "        # activation to be used between hidden layers\n",
    "        self.activation = nn.ReLU(inplace = True)\n",
    "    \n",
    "    def forward(self, x_u, x_i):\n",
    "    \n",
    "        x_u = x_u.view(-1, self.input_dim_u)\n",
    "        x_i = x_i.view(-1, self.input_dim_i)\n",
    "        #layer1\n",
    "        x_u = self.layer1_u(x_u)\n",
    "        x_u = self.activation(x_u)\n",
    "        x_i = self.layer1_i(x_i)\n",
    "        x_i = self.activation(x_i)\n",
    "        #layer2\n",
    "        x_u = self.layer2_u(x_u)\n",
    "        x_u = self.activation(x_u)\n",
    "        x_i = self.layer2_i(x_i)\n",
    "        x_i = self.activation(x_i)\n",
    "        \n",
    "        output = torch.sum(torch.mul(x_u,x_i),1)#pxq\n",
    "        #output = torch.sigmoid(output)\n",
    "        return output\n",
    "    \n",
    "class NMF_Model_Wrapper:\n",
    "    def __init__(self, network, learn_rate=1e-2):\n",
    "        \n",
    "        self.learn_rate = learn_rate\n",
    "        self.network = network\n",
    "        self.network.cuda()\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr = self.learn_rate)\n",
    "        self.loss_func = nn.BCELoss()#nn.BCEWithLogitsLoss()#nn.MSELoss()\n",
    "    \n",
    "    def fit(self, x_u, x_i, y):\n",
    "        x_u = torch.from_numpy(np.array(x_u)).type(torch.FloatTensor).cuda()\n",
    "        x_i = torch.from_numpy(np.array(x_i)).type(torch.FloatTensor).cuda()\n",
    "        y = torch.from_numpy(np.array(y)).type(torch.FloatTensor).cuda()\n",
    "        # reset gradient and total loss\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.network(x_u, x_i)\n",
    "        output = torch.sigmoid(output)\n",
    "        fit_loss = self.loss_func(output, y)\n",
    "        \n",
    "        fit_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return fit_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Success!\n",
      "Data Info:\n",
      "\tUser Num: 6040\n",
      "\tItem Num: 3706\n",
      "\tData Size: 994169\n",
      "\tSparsity: 0.04441379291858915\n",
      "The length of Trainset: 1988338\n",
      "The length of Testset: 604000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:66: RuntimeWarning: covariance is not positive-semidefinite.\n",
      "/root/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:67: RuntimeWarning: covariance is not positive-semidefinite.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 994 / 995 : loss = 12.135984Sample:     1 total_loss = 11.701012\n",
      "HR@5=0.314901, NDCG@5=0.164339\n",
      " 972 / 995 : loss = 13.375147HR@10=0.385596, NDCG@10=0.201704\n",
      " 994 / 995 : loss = 12.922428Sample:     1 total_loss = 13.643918\n",
      "HR@15=0.349834, NDCG@15=0.183995\n",
      " 994 / 995 : loss = 13.423174Sample:     1 total_loss = 13.715060\n",
      "HR@20=0.370033, NDCG@20=0.194803\n"
     ]
    }
   ],
   "source": [
    "#dataset\n",
    "ds_1m = DataSet_1M(negNum=1)\n",
    "trainset = ds_1m.trainset\n",
    "trainMat = ds_1m.trainMat\n",
    "testset = ds_1m.testset\n",
    "shape = ds_1m.shape\n",
    "#training\n",
    "sample_size = 1 # samples \n",
    "num_epochs = 1\n",
    "batchSize = 2000\n",
    "for k in [5,10,15,20]:  \n",
    "    #get approximate matrice\n",
    "    bmf = BayesianMatrixFactorization()\n",
    "    bmf.fit(R=trainMat, k=k)\n",
    "    R_sample = bmf.predict(sample_size=sample_size)\n",
    "    #NMF training\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()     \n",
    "    net = NMF_Model_Wrapper(network=NeuralMatrixFactorization(input_dim_u=shape[1], input_dim_i = shape[0],factors_dim_k=k))\n",
    "    best_net, best_loss = None, float('inf')\n",
    "    for iSpl, iR in enumerate(R_sample):\n",
    "        shuffled_idx = np.random.permutation(np.arange(len(trainset)))\n",
    "        trainset = np.array(trainset)[shuffled_idx].tolist()\n",
    "        num_batches = len(trainset) // batchSize + 1 \n",
    "        total_loss = []\n",
    "        for i in range(num_batches):#batch\n",
    "            min_idx = i * batchSize\n",
    "            max_idx = np.min([len(trainset), (i+1)*batchSize])\n",
    "            train_batch = trainset[min_idx: max_idx]\n",
    "            x_u, x_i, y = [], [], []\n",
    "            for uu,ii,rr in train_batch:\n",
    "                x_u.append(iR[int(uu),:])\n",
    "                x_i.append(iR[:,int(ii)])\n",
    "                y.append(float(rr))\n",
    "            _loss = net.fit(np.array(x_u), np.array(x_i), np.array(y))\n",
    "            sys.stdout.write('\\r {} / {} : loss = {}'.format(i, num_batches, float('%0.6f'%_loss.item())))\n",
    "            sys.stdout.flush()\n",
    "            total_loss.append(_loss.item())\n",
    "        print(\"Sample: %5d total_loss = %.6f\" % (iSpl + 1, np.mean(total_loss)))\n",
    "        if np.mean(total_loss) < best_loss:\n",
    "            best_loss = np.mean(total_loss)\n",
    "            best_net = copy.deepcopy(net.network)\n",
    "    #torch.save(best_net, \"/data/tmpexec/BDMF_torch\")\n",
    "    #best_net = torch.load(\"/data/tmpexec/BDMF_torch\").eval()\n",
    "    #best_net = torch.load(\"/data/tmpexec/BDMF_torch\").to('cuda:0')\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    for c in range(0,shape[0]):#6040\n",
    "        scorelist = []\n",
    "        gtItem = -1\n",
    "        x_u, x_i, y_i = [], [], []\n",
    "        for uu,ii,rr in testset[c*100:(c+1)*100]:#604000\n",
    "            if rr == 1.0: \n",
    "                gtItem = ii\n",
    "            x_u.append(np.array(trainMat[int(uu),:]))\n",
    "            x_i.append(np.array(trainMat[:,int(ii)]))\n",
    "            y_i.append(ii)\n",
    "        x_u = torch.from_numpy(np.array(x_u)).type(torch.FloatTensor).cuda()\n",
    "        x_i = torch.from_numpy(np.array(x_i)).type(torch.FloatTensor).cuda()\n",
    "        output = best_net(x_u, x_i)\n",
    "        output = output.cpu().data.numpy().tolist()\n",
    "        for j in range(len(y_i)):\n",
    "            scorelist.append([y_i[j],output[j]])\n",
    "        map_item_score = {}\n",
    "        for item, rate in scorelist: #turn dict\n",
    "            map_item_score[item] = rate\n",
    "        ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#topn=10\n",
    "        hr = getHitRatio(ranklist, gtItem)\n",
    "        hits.append(hr)\n",
    "        ndcg = getNDCG(ranklist, gtItem)\n",
    "        ndcgs.append(ndcg)\n",
    "    hitratio,ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "    print (\"HR@%d=%.6f, NDCG@%d=%.6f\" % (k, hitratio, k, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
