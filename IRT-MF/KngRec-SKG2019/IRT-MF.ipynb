{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset shape is:2547452 rows and 3 columns\n",
      "Testset shape is:1021600 rows and 3 columns\n",
      "Dataset Statistics: Interaction = 2547452, User = 10216, Item = 96324, Sparsity = 0.0026\n",
      "  K               HR@10             NDCG@10\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.13\n",
    "@function: Implementing IRT-MF \n",
    "           Setting KnowledgeBase-CC \n",
    "           Evaluating by hitradio,ndcg\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "\n",
    "#1. loading the KnowledgeBase dataset.\n",
    "trainset = pd.read_csv(\"/data/fjsdata/ctKngBase/kbcc_trainset.csv\", sep='|', low_memory=False, dtype={'csr':int, 'ke':int, 'num':float})\n",
    "trainset['num']=trainset['num'].apply(lambda x: 1 if float(x)>0.0 else 0)\n",
    "print ('Trainset shape is:%d rows and %d columns'%(trainset.shape[0],trainset.shape[1]))\n",
    "#testset includes 100 items for every user, one item is positive and other 99 is negtive items.\n",
    "testset = pd.read_csv(\"/data/fjsdata/ctKngBase/kbcc_testset.csv\", sep='|', low_memory=False, dtype={'csr':int, 'ke':int, 'num':float})\n",
    "testset['num']=testset['num'].apply(lambda x: 1 if float(x)>0.0 else 0)\n",
    "print ('Testset shape is:%d rows and %d columns'%(testset.shape[0],testset.shape[1]))\n",
    "csrNum = trainset['csr'].max()+1\n",
    "keNum = trainset['ke'].max()+1\n",
    "print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \n",
    "      (trainset.shape[0], csrNum, keNum, trainset.shape[0]/(csrNum*keNum)) )\n",
    "\n",
    "#2. IRT-MF class\n",
    "class IRTMF():\n",
    "    \n",
    "    def __init__(self, R, num_ng=4):\n",
    "        \"\"\"\n",
    "        Perform matrix factorization to predict empty entries in a matrix.     \n",
    "        Arguments\n",
    "        - R (ndarray)   : user-item rating matrix\n",
    "        - num_ng (int)  : number of negative items\n",
    "        \"\"\"\n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.num_ng = num_ng\n",
    "        \n",
    "        # Create a list of training samples\n",
    "        self.samples = [\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] > 0\n",
    "        ]\n",
    "\n",
    "        #smapling the negative items\n",
    "        for x in self.samples:\n",
    "            u = x[0]\n",
    "            for t in range(self.num_ng):\n",
    "                j = np.random.randint(self.num_items)\n",
    "                #while (u, j) in self.R:\n",
    "                while self.R[u, j] > 0:\n",
    "                    j = np.random.randint(self.num_items)\n",
    "                self.samples.append([u, j, 0])\n",
    "\n",
    "    def train(self, K, alpha=0.001, beta=0.01, epochs=20):\n",
    "        '''\n",
    "        - alpha (float) : learning rate\n",
    "        - beta (float)  : regularization parameter\n",
    "        - K (int)       : number of latent dimensions\n",
    "        -epochs(int)    : number of iterations\n",
    "        '''\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.epochs = epochs\n",
    "        # Initialize user and item latent feature matrice\n",
    "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
    "        \n",
    "        # Initialize the biases\n",
    "        self.b_u = np.zeros(self.num_users)\n",
    "        self.b_i = np.zeros(self.num_items)\n",
    "        self.b = np.mean(self.R[np.where(self.R != 0)])\n",
    "               \n",
    "        # Perform stochastic gradient descent for number of iterations\n",
    "        training_process = []\n",
    "        for i in range(self.epochs):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            #if (i+1) % 10 == 0:\n",
    "            #    mse = self.mse()\n",
    "            #    print(\"Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "        \n",
    "        return self.full_matrix()\n",
    "\n",
    "    def mse(self):\n",
    "        \"\"\"\n",
    "        A function to compute the total mean square error\n",
    "        \"\"\"\n",
    "        xs, ys = self.R.nonzero()\n",
    "        predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        for x, y in zip(xs, ys):\n",
    "            error += pow(self.R[x, y] - predicted[x, y], 2)\n",
    "        return np.sqrt(error)\n",
    "\n",
    "    def sgd(self):\n",
    "        \"\"\"\n",
    "        Perform stochastic graident descent\n",
    "        \"\"\"\n",
    "        for i, j, r in self.samples:\n",
    "            # Computer prediction and error\n",
    "            prediction = self.get_rating(i, j)\n",
    "            prob = self.IRT_Rasch(self.a_u[i], self.d_i[j])\n",
    "            e = (r - prediction-prob)\n",
    "            \n",
    "            # Update biases\n",
    "            \n",
    "            self.a_u[i] += self.alpha * (e * prob*(prob-1) - self.beta * self.a_u[i])\n",
    "            self.d_i[j] += self.alpha * (e * prob*(1-prob) - self.beta * self.d_i[j])\n",
    "            \n",
    "            # Create copy of row of P since we need to update it but use older values for update on Q\n",
    "            P_i = self.P[i, :][:]\n",
    "            \n",
    "            # Update user and item latent feature matrices\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            self.Q[j, :] += self.alpha * (e * P_i - self.beta * self.Q[j,:])\n",
    "\n",
    "    def get_rating(self, i, j):\n",
    "        \"\"\"\n",
    "        Get the predicted rating of user i and item j\n",
    "        \"\"\"\n",
    "        prediction = self.IRT_Rasch(self.a_u[i], self.d_i[j]) + self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "    \n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        Computer the full matrix using the resultant biases, P and Q\n",
    "        \"\"\"\n",
    "        nR = np.zeros((self.num_users, self.num_items))\n",
    "        for i in range(self.num_users):\n",
    "            for j in range(self.num_items):\n",
    "                nR[i,j] = self.IRT_Rasch(self.a_u[i], self.d_i[j]) + self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return nR\n",
    "    \n",
    "    def IRT_Rasch(self, a, d):\n",
    "        \"\"\"\n",
    "        Compute the probability of respons\n",
    "        \"\"\"\n",
    "        x = a-d\n",
    "        if x < 0:\n",
    "            return 1 - 1/(1 + math.exp(x))\n",
    "        else:\n",
    "            return 1/(1 + math.exp(-x))\n",
    "        \n",
    "#3. training and evaluating \n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "\n",
    "R = np.zeros((csrNum, keNum))\n",
    "for index, row in trainset.iterrows(): \n",
    "    R[int(row['csr'])][int(row['ke'])] = float(row['num'])\n",
    "print (\"%3s%20s%20s\" % ('K','HR@10', 'NDCG@10'))\n",
    "mdl = IRTMF(R=R, num_ng=4)# K is latent factors\n",
    "for K in [8,16,32,64]:#iterations epoches\n",
    "    nR = mdl.train(K=K, alpha=0.001, beta=0.01, epochs=20)\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    list_csr = list(set(np.array(testset['csr']).tolist()))\n",
    "    for csr in list_csr:\n",
    "        csrset = testset[testset['csr']==csr]\n",
    "        scorelist = []\n",
    "        positem = 0\n",
    "        for u, i, r in np.array(csrset).tolist():   \n",
    "            if float(r)>0.0:#one positive item\n",
    "                scorelist.append([int(i),nR[int(u),int(i)]])\n",
    "                positem = int(i) \n",
    "            else:# 99 negative items\n",
    "                scorelist.append([int(i),nR[int(u),int(i)]])\n",
    "        map_item_score = {}\n",
    "        for item, rate in scorelist: #turn dict\n",
    "            map_item_score[item] = rate\n",
    "        ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "        hr = getHitRatio(ranklist, positem)\n",
    "        hits.append(hr)\n",
    "        ndcg = getNDCG(ranklist, positem)\n",
    "        ndcgs.append(ndcg)\n",
    "    hitratio,ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "    print (\"%3d%20.6f%20.6f\" % (K, hitratio, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset shape is:2547452 rows and 3 columns\n",
      "Testset shape is:1021600 rows and 3 columns\n",
      "Dataset Statistics: Interaction = 2547452, User = 10216, Item = 96324, Sparsity = 0.0026\n",
      "  K            HitRatio                NDCG\n",
      "  8            0.502643            0.502643\n",
      " 16            0.507048            0.507048\n",
      " 32            0.499315            0.499315\n",
      " 64            0.503622            0.503622\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.12\n",
    "@function: Implementing SVD(surprise) \n",
    "           Setting KnowledgeBase-cc \n",
    "           Evaluating by hitradio,ndcg\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import surprise as sp\n",
    "\n",
    "#1. loading the KnowledgeBase dataset.\n",
    "trainset = pd.read_csv(\"/data/fjsdata/ctKngBase/trainset.csv\", sep='|', low_memory=False)\n",
    "trainset['num']=trainset['num'].apply(lambda x: 1 if float(x)>0.0 else 0)\n",
    "print ('Trainset shape is:%d rows and %d columns'%(trainset.shape[0],trainset.shape[1]))\n",
    "#testset includes 100 items for every user, one item is positive and other 99 is negtive items.\n",
    "testset = pd.read_csv(\"/data/fjsdata/ctKngBase/testset.csv\", sep='|', low_memory=False)\n",
    "testset['num']=testset['num'].apply(lambda x: 1 if float(x)>0.0 else 0)\n",
    "print ('Testset shape is:%d rows and %d columns'%(testset.shape[0],testset.shape[1]))\n",
    "csrNum = trainset['csr'].max()+1\n",
    "keNum = trainset['ke'].max()+1\n",
    "print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \n",
    "      (trainset.shape[0], csrNum, keNum, trainset.shape[0]/(csrNum*keNum)) )\n",
    "\n",
    "#2. Transforming into data format of surprise and spliting the train-set and test-set\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "reader = sp.Reader(rating_scale=(0, 1))\n",
    "spdata = sp.Dataset.load_from_df(trainset,reader)\n",
    "trainset = spdata.build_full_trainset()\n",
    "testset = np.array(testset).tolist()\n",
    "\n",
    "#3.training and evaluating \n",
    "def getMSE(TRating,PRating):\n",
    "    error = TRating-PRating\n",
    "    return error*error\n",
    "\n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "print (\"%3s%20s%20s\" % ('K','HitRatio', 'NDCG'))\n",
    "for K in [8,16,32,64]:#iterations epoches\n",
    "    algo = sp.SVD(n_factors=K, n_epochs=20, lr_all=0.001, reg_all=0.01 )#NMF,SVDpp\n",
    "    algo.fit(trainset)\n",
    "    #print (algo.predict(str(1),str(1), r_ui=0, verbose=True)) \n",
    "    predictions = algo.test(testset)#testset include one positive and 99 negtive sample of every user.\n",
    "    user_iid_true_est = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        user_iid_true_est[uid].append((iid, true_r, est))\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    for uid, iid_ratings in user_iid_true_est.items():\n",
    "        # Sort user ratings by estimated value\n",
    "        #iid_ratings.sort(key=lambda x: x[2], reverse=True) #sorted by est\n",
    "        scorelist = []\n",
    "        positem = 0\n",
    "        for iid, ture_r, est in iid_ratings:\n",
    "            if (ture_r+1)>0.0:#one positive item\n",
    "                scorelist.append([iid,est])\n",
    "                positem = iid \n",
    "            else:# 99 negative items\n",
    "                scorelist.append([iid,est])\n",
    "        map_item_score = {}\n",
    "        for item, rate in scorelist: #turn dict\n",
    "            map_item_score[item] = rate\n",
    "        ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "        hr = getHitRatio(ranklist, positem)\n",
    "        hits.append(hr)\n",
    "        ndcg = getNDCG(ranklist, positem)\n",
    "        ndcgs.append(ndcg)\n",
    "    hitratio,ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "    print (\"%3d%20.6f%20.6f\" % (K, hitratio, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset shape is:2547452 rows and 3 columns\n",
      "Testset shape is:1021600 rows and 3 columns\n",
      "Dataset Statistics: Interaction = 2547452, User = 10216, Item = 96324, Sparsity = 0.0026\n",
      "  K          Iterations                RMSE            HitRatio                NDCG\n",
      "  2                   2            0.592674            0.092991            0.041560\n",
      "  2                  20            0.608159            0.133516            0.074288\n",
      "  2                  50            0.469229            0.066269            0.038255\n",
      "  2                 100            0.343965            0.033379            0.018839\n",
      " 50                   2            0.529410            0.713293            0.515024\n",
      " 50                  20            0.587791            0.412490            0.245202\n",
      " 50                  50            0.381476            0.155247            0.097560\n",
      " 50                 100            0.283121            0.092306            0.056403\n",
      "100                   2            0.529443            0.771926            0.539821\n",
      "100                  20            0.597526            0.429816            0.255935\n",
      "100                  50            0.387453            0.159749            0.099228\n",
      "100                 100            0.284994            0.098277            0.060707\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.09\n",
    "@function: Implementing IRT-MF and Setting KnowledgeBase as baseline by rmse,hitradio,ndcg\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "\n",
    "#1. loading the KnowledgeBase dataset.\n",
    "trainset = pd.read_csv(\"/data/fjsdata/ctKngBase/trainset.csv\", sep='|', low_memory=False)\n",
    "print ('Trainset shape is:%d rows and %d columns'%(trainset.shape[0],trainset.shape[1]))\n",
    "#testset includes 100 items for every user, one item is positive and other 99 is negtive items.\n",
    "testset = pd.read_csv(\"/data/fjsdata/ctKngBase/testset.csv\", sep='|', low_memory=False)\n",
    "print ('Testset shape is:%d rows and %d columns'%(testset.shape[0],testset.shape[1]))\n",
    "csrNum = trainset['csr'].max()+1\n",
    "keNum = trainset['ke'].max()+1\n",
    "print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \n",
    "      (trainset.shape[0], csrNum, keNum, trainset.shape[0]/(csrNum*keNum)) )\n",
    "\n",
    "#2. IRT-MF class\n",
    "class IRTMF():\n",
    "    \n",
    "    def __init__(self, R, K, alpha, beta, iterations):\n",
    "        \"\"\"\n",
    "        Perform matrix factorization to predict empty entries in a matrix.     \n",
    "        Arguments\n",
    "        - R (ndarray)   : user-item rating matrix\n",
    "        - K (int)       : number of latent dimensions\n",
    "        - alpha (float) : learning rate\n",
    "        - beta (float)  : regularization parameter\n",
    "        \"\"\"\n",
    "        \n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "\n",
    "    def train(self):\n",
    "        # Initialize user and item latent feature matrice\n",
    "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
    "        \n",
    "        # Initialize the biases\n",
    "        self.a_u = np.zeros(self.num_users)#users' ability\n",
    "        self.d_i = np.zeros(self.num_items)#items' difficulty\n",
    "        \n",
    "        # Create a list of training samples\n",
    "        self.samples = [\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] > 0\n",
    "        ]\n",
    "        \n",
    "        # Perform stochastic gradient descent for number of iterations\n",
    "        for i in range(self.iterations):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            #if (i+1) % 10 == 0:\n",
    "            #    mse = self.mse()\n",
    "            #    print(\"Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "        \n",
    "        return self.full_matrix()\n",
    "\n",
    "    def mse(self):\n",
    "        \"\"\"\n",
    "        A function to compute the total mean square error\n",
    "        \"\"\"\n",
    "        xs, ys = self.R.nonzero()\n",
    "        predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        for x, y in zip(xs, ys):\n",
    "            error += pow(self.R[x, y] - predicted[x, y], 2)\n",
    "        return np.sqrt(error)\n",
    "\n",
    "    def sgd(self):\n",
    "        \"\"\"\n",
    "        Perform stochastic graident descent\n",
    "        \"\"\"\n",
    "        for i, j, r in self.samples:\n",
    "            # Computer prediction and error\n",
    "            prediction = self.get_rating(i, j)\n",
    "            e = (r - prediction)\n",
    "            \n",
    "            # Update biases\n",
    "            prob = self.IRT_Rasch(self.a_u[i], self.d_i[j])\n",
    "            self.a_u[i] += self.alpha * (e * prob*(prob-1) - self.beta * self.a_u[i])\n",
    "            self.d_i[j] += self.alpha * (e * prob*(1-prob) - self.beta * self.d_i[j])\n",
    "            \n",
    "            # Create copy of row of P since we need to update it but use older values for update on Q\n",
    "            P_i = self.P[i, :][:]\n",
    "            \n",
    "            # Update user and item latent feature matrices\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            self.Q[j, :] += self.alpha * (e * P_i - self.beta * self.Q[j,:])\n",
    "\n",
    "    def get_rating(self, i, j):\n",
    "        \"\"\"\n",
    "        Get the predicted rating of user i and item j\n",
    "        \"\"\"\n",
    "        prediction = self.IRT_Rasch(self.a_u[i], self.d_i[j]) + self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "    \n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        Computer the full matrix using the resultant biases, P and Q\n",
    "        \"\"\"\n",
    "        nR = np.zeros((self.num_users, self.num_items))\n",
    "        for i in range(self.num_users):\n",
    "            for j in range(self.num_items):\n",
    "                nR[i,j] = self.IRT_Rasch(self.a_u[i], self.d_i[j]) + self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return nR\n",
    "    \n",
    "    def IRT_Rasch(self, a, d):\n",
    "        \"\"\"\n",
    "        Compute the probability of respons\n",
    "        \"\"\"\n",
    "        x = a-d\n",
    "        if x < 0:\n",
    "            return 1 - 1/(1 + math.exp(x))\n",
    "        else:\n",
    "            return 1/(1 + math.exp(-x))\n",
    "        \n",
    "#3. training and evaluating \n",
    "def getMSE(TRating,PRating):\n",
    "    error = TRating-PRating\n",
    "    return error*error\n",
    "\n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "\n",
    "R = np.zeros((csrNum, keNum))\n",
    "for index, row in trainset.iterrows(): \n",
    "    R[int(row['csr'])][int(row['ke'])] = float(row['num'])\n",
    "print (\"%3s%20s%20s%20s%20s\" % ('K','Iterations','RMSE', 'HitRatio', 'NDCG'))\n",
    "for K in [2,50,100]:#latent factors\n",
    "    for iterations in [2,20,50,100]:#iterations epoches\n",
    "        mdl = IRTMF(R=R, K=K, alpha=0.001, beta=0.01, iterations=iterations)# K is latent factors\n",
    "        nR = mdl.train()\n",
    "        mses = []\n",
    "        hits = []\n",
    "        ndcgs = []\n",
    "        list_csr = list(set(np.array(testset['csr']).tolist()))\n",
    "        for csr in list_csr:\n",
    "            csrset = testset[testset['csr']==csr]\n",
    "            scorelist = []\n",
    "            positem = 0\n",
    "            for u, i, r in np.array(csrset).tolist():   \n",
    "                if float(r)>0.0:#one positive item\n",
    "                    mses.append(getMSE(float(r),nR[int(u),int(i)]))\n",
    "                    scorelist.append([int(i),nR[int(u),int(i)]])\n",
    "                    positem = int(i) \n",
    "                else:# 99 negative items\n",
    "                    scorelist.append([int(i),nR[int(u),int(i)]])\n",
    "            map_item_score = {}\n",
    "            for item, rate in scorelist: #turn dict\n",
    "                map_item_score[item] = rate\n",
    "            ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "            hr = getHitRatio(ranklist, positem)\n",
    "            hits.append(hr)\n",
    "            ndcg = getNDCG(ranklist, positem)\n",
    "            ndcgs.append(ndcg)\n",
    "        rmse,hitratio,ndcg = math.sqrt(sum(mses) / len(mses)) ,np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "        print (\"%3d%20d%20.6f%20.6f%20.6f\" % (K, iterations, rmse, hitratio, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset shape is:2547452 rows and 3 columns\n",
      "Testset shape is:1021600 rows and 3 columns\n",
      "Dataset Statistics: Interaction = 2547452, User = 10216, Item = 96324, Sparsity = 0.0026\n",
      "  K          Iterations                RMSE            HitRatio                NDCG\n",
      "  2                   2            0.054436            0.235611            0.173626\n",
      "  2                  20            0.054858            0.313528            0.229820\n",
      "  2                  50            0.053164            0.388312            0.270784\n",
      "  2                 100            0.051611            0.450764            0.307749\n",
      " 50                   2            0.075428            0.137138            0.071148\n",
      " 50                  20            0.068491            0.149569            0.080760\n",
      " 50                  50            0.059963            0.139879            0.079384\n",
      " 50                 100            0.054146            0.139781            0.082243\n",
      "100                   2            0.091649            0.125587            0.061901\n",
      "100                  20            0.077665            0.113645            0.054241\n",
      "100                  50            0.066255            0.107478            0.050682\n",
      "100                 100            0.056881            0.098571            0.051124\n",
      "200                   2            0.117604            0.116680            0.054736\n",
      "200                  20            0.092163            0.088489            0.038359\n",
      "200                  50            0.072807            0.063919            0.026945\n",
      "200                 100            0.058756            0.052467            0.024285\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.09\n",
    "@function: Implementing SVD(surprise) and Setting KnowledgeBase as baseline by rmse,hitradio,ndcg\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import surprise as sp\n",
    "\n",
    "#1. loading the KnowledgeBase dataset.\n",
    "trainset = pd.read_csv(\"/data/fjsdata/ctKngBase/trainset.csv\", sep='|', low_memory=False)\n",
    "print ('Trainset shape is:%d rows and %d columns'%(trainset.shape[0],trainset.shape[1]))\n",
    "#testset includes 100 items for every user, one item is positive and other 99 is negtive items.\n",
    "testset = pd.read_csv(\"/data/fjsdata/ctKngBase/testset.csv\", sep='|', low_memory=False)\n",
    "print ('Testset shape is:%d rows and %d columns'%(testset.shape[0],testset.shape[1]))\n",
    "csrNum = trainset['csr'].max()+1\n",
    "keNum = trainset['ke'].max()+1\n",
    "print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \n",
    "      (trainset.shape[0], csrNum, keNum, trainset.shape[0]/(csrNum*keNum)) )\n",
    "\n",
    "#2. Transforming into data format of surprise and spliting the train-set and test-set\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "reader = sp.Reader(rating_scale=(0, 1))\n",
    "spdata = sp.Dataset.load_from_df(trainset,reader)\n",
    "trainset = spdata.build_full_trainset()\n",
    "testset = np.array(testset).tolist()\n",
    "\n",
    "#3.training and evaluating \n",
    "def getMSE(TRating,PRating):\n",
    "    error = TRating-PRating\n",
    "    return error*error\n",
    "\n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "print (\"%3s%20s%20s%20s%20s\" % ('K','Iterations','RMSE', 'HitRatio', 'NDCG'))\n",
    "for K in [2,50,100,200]:#latent factors\n",
    "    for iterations in [2,20,50,100]:#iterations epoches\n",
    "        algo = sp.SVD(n_factors=K, n_epochs=iterations, lr_all=0.001, reg_all=0.01 )#NMF,SVDpp\n",
    "        algo.fit(trainset)\n",
    "        #print (algo.predict(str(1),str(1), r_ui=0, verbose=True)) \n",
    "        predictions = algo.test(testset)#testset include one positive and 99 negtive sample of every user.\n",
    "        user_iid_true_est = defaultdict(list)\n",
    "        for uid, iid, true_r, est, _ in predictions:\n",
    "            user_iid_true_est[uid].append((iid, true_r, est))\n",
    "        mses = []\n",
    "        hits = []\n",
    "        ndcgs = []\n",
    "        for uid, iid_ratings in user_iid_true_est.items():\n",
    "            # Sort user ratings by estimated value\n",
    "            #iid_ratings.sort(key=lambda x: x[2], reverse=True) #sorted by est\n",
    "            scorelist = []\n",
    "            positem = 0\n",
    "            for iid, ture_r, est in iid_ratings:\n",
    "                if (ture_r+1)>0.0:#one positive item\n",
    "                    mses.append(getMSE((ture_r+1),est))\n",
    "                    scorelist.append([iid,est])\n",
    "                    positem = iid \n",
    "                else:# 99 negative items\n",
    "                    scorelist.append([iid,est])\n",
    "            map_item_score = {}\n",
    "            for item, rate in scorelist: #turn dict\n",
    "                map_item_score[item] = rate\n",
    "            ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "            hr = getHitRatio(ranklist, positem)\n",
    "            hits.append(hr)\n",
    "            ndcg = getNDCG(ranklist, positem)\n",
    "            ndcgs.append(ndcg)\n",
    "        rmse,hitratio,ndcg = math.sqrt(sum(mses) / len(mses)) ,np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "        print (\"%3d%20d%20.6f%20.6f%20.6f\" % (K, iterations, rmse, hitratio, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset shape is:2547452 rows and 3 columns\n",
      "Testset shape is:1021600 rows and 3 columns\n",
      "Dataset Statistics: Interaction = 2547452, User = 10216, Item = 96324, Sparsity = 0.0026\n",
      "  K          Iterations                RMSE            HitRatio                NDCG\n",
      "  2                   1            0.395768            0.082028            0.035729\n",
      "  2                  20                 nan            1.000000            0.333333\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.11\n",
    "@function: Implementing IRT-MF and Setting KnowledgeBase as baseline by rmse,hitradio,ndcg\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import scipy.stats\n",
    "\n",
    "#1. loading the KnowledgeBase dataset.\n",
    "trainset = pd.read_csv(\"/data/fjsdata/ctKngBase/trainset.csv\", sep='|', low_memory=False)\n",
    "print ('Trainset shape is:%d rows and %d columns'%(trainset.shape[0],trainset.shape[1]))\n",
    "#testset includes 100 items for every user, one item is positive and other 99 is negtive items.\n",
    "testset = pd.read_csv(\"/data/fjsdata/ctKngBase/testset.csv\", sep='|', low_memory=False)\n",
    "print ('Testset shape is:%d rows and %d columns'%(testset.shape[0],testset.shape[1]))\n",
    "csrNum = trainset['csr'].max()+1\n",
    "keNum = trainset['ke'].max()+1\n",
    "print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \n",
    "      (trainset.shape[0], csrNum, keNum, trainset.shape[0]/(csrNum*keNum)) )\n",
    "\n",
    "#2. IRT-MF class\n",
    "class IRTMF():\n",
    "    \n",
    "    def __init__(self, R, K, alpha, beta, iterations):\n",
    "        \"\"\"\n",
    "        Perform matrix factorization to predict empty entries in a matrix.     \n",
    "        Arguments\n",
    "        - R (ndarray)   : user-item rating matrix\n",
    "        - K (int)       : number of latent dimensions\n",
    "        - alpha (float) : learning rate\n",
    "        - beta (float)  : regularization parameter\n",
    "        \"\"\"\n",
    "        \n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "\n",
    "    def train(self):\n",
    "        # Initialize user and item latent feature matrice\n",
    "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
    "        \n",
    "        # Initialize the biases\n",
    "        self.a_u = np.ones(self.num_users)#users' ability\n",
    "        self.d_i = np.ones(self.num_items)#items' difficulty\n",
    "        \n",
    "        # Create a list of training samples\n",
    "        self.samples = [\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] > 0\n",
    "        ]\n",
    "        \n",
    "        # Perform stochastic gradient descent for number of iterations\n",
    "        for i in range(self.iterations):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            #if (i+1) % 10 == 0:\n",
    "            #    mse = self.mse()\n",
    "            #    print(\"Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "        \n",
    "        return self.P.dot(self.Q.T), self.a_u, self.d_i\n",
    "\n",
    "    def mse(self):\n",
    "        \"\"\"\n",
    "        A function to compute the total mean square error\n",
    "        \"\"\"\n",
    "        xs, ys = self.R.nonzero()\n",
    "        predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        for x, y in zip(xs, ys):\n",
    "            error += pow(self.R[x, y] - predicted[x, y], 2)\n",
    "        return np.sqrt(error)\n",
    "\n",
    "    def sgd(self):\n",
    "        \"\"\"\n",
    "        Perform stochastic graident descent\n",
    "        \"\"\"\n",
    "        for i, j, r in self.samples:\n",
    "            # Computer prediction and error\n",
    "            prediction = self.get_rating(i, j)\n",
    "            pdf = self.IRT_Normal(r,self.a_u[i],self.d_i[j])#calculate the probability of response.\n",
    "            e = (pdf - prediction)\n",
    "            \n",
    "            # Update biases\n",
    "            dev_au = pdf * ((r-self.d_i[j])**2/self.a_u[i]-math.sqrt(2*math.pi))*self.a_u[i]\n",
    "            dev_di = pdf * (r-self.d_i[j])/self.a_u[i]\n",
    "            self.a_u[i] += self.alpha * (e * dev_au - self.beta * self.a_u[i])\n",
    "            self.d_i[j] += self.alpha * (e * dev_di - self.beta * self.d_i[j])\n",
    "            \n",
    "            # Create copy of row of P since we need to update it but use older values for update on Q\n",
    "            P_i = self.P[i, :][:]\n",
    "            \n",
    "            # Update user and item latent feature matrices\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            self.Q[j, :] += self.alpha * (e * P_i - self.beta * self.Q[j,:])\n",
    "\n",
    "    def get_rating(self, i, j):\n",
    "        \"\"\"\n",
    "        Get the predicted rating of user i and item j\n",
    "        \"\"\"\n",
    "        prediction = self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "    \n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        Computer the full matrix using the resultant biases, P and Q\n",
    "        \"\"\"\n",
    "        nR = self.P.dot(self.Q.T)\n",
    "        return nR\n",
    "    \n",
    "    def IRT_Normal(self, r, a, d):\n",
    "        \"\"\"\n",
    "        Compute the probability of respons,Normal\n",
    "        \"\"\"\n",
    "        return scipy.stats.norm(d, a).pdf(r)\n",
    "        \n",
    "#3. training and evaluating \n",
    "def getMSE(TRating,PRating):\n",
    "    error = TRating-PRating\n",
    "    return error*error\n",
    "\n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "\n",
    "R = np.zeros((csrNum, keNum))\n",
    "for index, row in trainset.iterrows(): \n",
    "    R[int(row['csr'])][int(row['ke'])] = float(row['num'])\n",
    "print (\"%3s%20s%20s%20s%20s\" % ('K','Iterations','RMSE', 'HitRatio', 'NDCG'))\n",
    "for K in [2,50,100]:#latent factors\n",
    "    for iterations in [1,20,50,100]:#iterations epoches\n",
    "        mdl = IRTMF(R=R, K=K, alpha=0.001, beta=0.01, iterations=iterations)# K is latent factors\n",
    "        nR,au,di = mdl.train()\n",
    "        mses = []\n",
    "        hits = []\n",
    "        ndcgs = []\n",
    "        list_csr = list(set(np.array(testset['csr']).tolist()))\n",
    "        for csr in list_csr:\n",
    "            csrset = testset[testset['csr']==csr]\n",
    "            scorelist = []\n",
    "            positem = 0\n",
    "            for u, i, r in np.array(csrset).tolist():   \n",
    "                if float(r)>0.0:#one positive item\n",
    "                    true_r = mdl.IRT_Normal(float(r),au[int(u)],di[int(i)])\n",
    "                    mses.append(getMSE(true_r,nR[int(u),int(i)]))\n",
    "                    scorelist.append([int(i),nR[int(u),int(i)]])\n",
    "                    positem = int(i) \n",
    "                else:# 99 negative items\n",
    "                    scorelist.append([int(i),nR[int(u),int(i)]])\n",
    "            map_item_score = {}\n",
    "            for item, rate in scorelist: #turn dict\n",
    "                map_item_score[item] = rate\n",
    "            ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "            hr = getHitRatio(ranklist, positem)\n",
    "            hits.append(hr)\n",
    "            ndcg = getNDCG(ranklist, positem)\n",
    "            ndcgs.append(ndcg)\n",
    "        rmse,hitratio,ndcg = math.sqrt(sum(mses) / len(mses)) ,np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "        print (\"%3d%20d%20.6f%20.6f%20.6f\" % (K, iterations, rmse, hitratio, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset shape is:2547452 rows and 3 columns\n",
      "Testset shape is:254745 rows and 3 columns\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#Loading the dataset and Excluding the outliers\n",
    "kbdata = pd.read_csv(\"/data/fjsdata/ctKngBase/kb.csv\", sep='|', low_memory=False)\n",
    "kbdata = kbdata.loc[(kbdata['num']<200)]#seven months, one per day\n",
    "num_max=kbdata['num'].max()\n",
    "num_min=kbdata['num'].min()\n",
    "kbdata['num']=kbdata['num'].apply(lambda x: (x-num_min+1)*1.0/(num_max-num_min+1))\n",
    "trainset = kbdata[['csr','ke','num']]\n",
    "print ('Trainset shape is:%d rows and %d columns'%(trainset.shape[0],trainset.shape[1]))\n",
    "testset = kbdata.sample(frac=0.1)\n",
    "print ('Testset shape is:%d rows and %d columns'%(testset.shape[0],testset.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10 ; error = 1172.5074\n",
      "Iteration: 20 ; error = 958.0748\n",
      "Iteration: 30 ; error = 800.2448\n",
      "Iteration: 40 ; error = 575.6074\n",
      "Iteration: 50 ; error = 437.3844\n",
      "Iteration: 60 ; error = 366.4029\n",
      "Iteration: 70 ; error = 321.0410\n",
      "Iteration: 80 ; error = 288.5642\n",
      "Iteration: 90 ; error = 263.7157\n",
      "Iteration: 100 ; error = 243.8924\n",
      "0.15255096782684383\n"
     ]
    }
   ],
   "source": [
    "#IRT-MF for knowledgeBase dataset \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "class IRTMF():\n",
    "    \n",
    "    def __init__(self, R, K, alpha, beta, iterations):\n",
    "        \"\"\"\n",
    "        Perform matrix factorization to predict empty entries in a matrix.     \n",
    "        Arguments\n",
    "        - R (ndarray)   : user-item rating matrix\n",
    "        - K (int)       : number of latent dimensions\n",
    "        - alpha (float) : learning rate\n",
    "        - beta (float)  : regularization parameter\n",
    "        \"\"\"\n",
    "        \n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "\n",
    "    def train(self):\n",
    "        # Initialize user and item latent feature matrice\n",
    "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
    "        \n",
    "        # Initialize the biases\n",
    "        self.a_u = np.zeros(self.num_users)#users' ability\n",
    "        self.d_i = np.zeros(self.num_items)#items' difficulty\n",
    "        \n",
    "        # Create a list of training samples\n",
    "        self.samples = [\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] > 0\n",
    "        ]\n",
    "        \n",
    "        # Perform stochastic gradient descent for number of iterations\n",
    "        for i in range(self.iterations):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            if (i+1) % 10 == 0:\n",
    "                mse = self.mse()\n",
    "                print(\"Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "        \n",
    "        return self.full_matrix()\n",
    "\n",
    "    def mse(self):\n",
    "        \"\"\"\n",
    "        A function to compute the total mean square error\n",
    "        \"\"\"\n",
    "        xs, ys = self.R.nonzero()\n",
    "        predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        for x, y in zip(xs, ys):\n",
    "            error += pow(self.R[x, y] - predicted[x, y], 2)\n",
    "        return np.sqrt(error)\n",
    "\n",
    "    def sgd(self):\n",
    "        \"\"\"\n",
    "        Perform stochastic graident descent\n",
    "        \"\"\"\n",
    "        for i, j, r in self.samples:\n",
    "            # Computer prediction and error\n",
    "            prediction = self.get_rating(i, j)\n",
    "            e = (r - prediction)\n",
    "            \n",
    "            # Update biases\n",
    "            prob = self.IRT_Rasch(self.a_u[i], self.d_i[j])\n",
    "            self.a_u[i] += self.alpha * (e * prob*(prob-1) - self.beta * self.a_u[i])\n",
    "            self.d_i[j] += self.alpha * (e * prob*(1-prob) - self.beta * self.d_i[j])\n",
    "            \n",
    "            # Create copy of row of P since we need to update it but use older values for update on Q\n",
    "            P_i = self.P[i, :][:]\n",
    "            \n",
    "            # Update user and item latent feature matrices\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            self.Q[j, :] += self.alpha * (e * P_i - self.beta * self.Q[j,:])\n",
    "\n",
    "    def get_rating(self, i, j):\n",
    "        \"\"\"\n",
    "        Get the predicted rating of user i and item j\n",
    "        \"\"\"\n",
    "        prediction = self.IRT_Rasch(self.a_u[i], self.d_i[j]) + self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "    \n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        Computer the full matrix using the resultant biases, P and Q\n",
    "        \"\"\"\n",
    "        nR = np.zeros((self.num_users, self.num_items))\n",
    "        for i in range(self.num_users):\n",
    "            for j in range(self.num_items):\n",
    "                nR[i,j] = self.IRT_Rasch(self.a_u[i], self.d_i[j]) + self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return nR\n",
    "    \n",
    "    def IRT_Rasch(self, a, d):\n",
    "        \"\"\"\n",
    "        Compute the probability of respons\n",
    "        \"\"\"\n",
    "        x = a-d\n",
    "        if x < 0:\n",
    "            return 1 - 1/(1 + math.exp(x))\n",
    "        else:\n",
    "            return 1/(1 + math.exp(-x))\n",
    "\n",
    "# Construct matrix and train model\n",
    "N =len(trainset['csr'].unique())+1\n",
    "M =len(trainset['ke'].unique())+1\n",
    "R = np.zeros((N, M))\n",
    "for index, row in trainset.iterrows(): \n",
    "    R[int(row['csr'])][int(row['ke'])] = row['num']\n",
    "mf = IRTMF(R, K=100, alpha=0.001, beta=0.01, iterations=100)\n",
    "nR = mf.train() #return the predicted matrix\n",
    "#evaluate model\n",
    "squaredError = []\n",
    "for _, row in testset.iterrows(): \n",
    "    error=float(row['num'])-float(nR[int(row['csr'])][int(row['ke'])])\n",
    "    squaredError.append(error * error)\n",
    "RMSE =math.sqrt(sum(squaredError) / len(squaredError))\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9982\n"
     ]
    }
   ],
   "source": [
    "import surprise as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "#1.Transforming into data format of surprise and spliting the train-set and test-set\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "reader = sp.Reader(rating_scale=(0, 1))\n",
    "spdata = sp.Dataset.load_from_df(trainset,reader)\n",
    "# sampling random trainset and testset, and test set is made of 10% of the ratings.\n",
    "#trainset, testset = sp.model_selection.train_test_split(spdata, test_size=.1)\n",
    "trainset_sp = spdata.build_full_trainset()\n",
    "testset_sp = np.array(testset).tolist()\n",
    "\n",
    "#2.Training the model and predicting ratings for the testset\n",
    "algo = sp.SVD()#NMF,SVDpp,n_factors=100\n",
    "algo.fit(trainset_sp)\n",
    "predictions = algo.test(testset_sp)#testset include positive and negtive sample.\n",
    "\n",
    "#3.measuring the performance of SVD by precision, recall and  NDCG\n",
    "#print ('RMSE of testset is:%.8f'%(sp.accuracy.rmse(predictions)))\n",
    "RMSE = sp.accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10 ; error = 82.9026\n",
      "Iteration: 20 ; error = 82.1781\n",
      "Iteration: 30 ; error = 81.9153\n",
      "Iteration: 40 ; error = 81.7769\n",
      "Iteration: 50 ; error = 81.6866\n",
      "Iteration: 60 ; error = 81.6230\n",
      "Iteration: 70 ; error = 81.5738\n",
      "Iteration: 80 ; error = 81.5325\n",
      "Iteration: 90 ; error = 81.4958\n",
      "Iteration: 100 ; error = 81.4597\n",
      "0.05180268517188995\n"
     ]
    }
   ],
   "source": [
    "#MF for knowledgeBase dataset \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "class IRTMF():\n",
    "    \n",
    "    def __init__(self, R, K, alpha, beta, iterations):\n",
    "        \"\"\"\n",
    "        Perform matrix factorization to predict empty entries in a matrix.     \n",
    "        Arguments\n",
    "        - R (ndarray)   : user-item rating matrix\n",
    "        - K (int)       : number of latent dimensions\n",
    "        - alpha (float) : learning rate\n",
    "        - beta (float)  : regularization parameter\n",
    "        \"\"\"\n",
    "        \n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "\n",
    "    def train(self):\n",
    "        # Initialize user and item latent feature matrice\n",
    "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
    "        \n",
    "        # Initialize the biases\n",
    "        self.b_u = np.zeros(self.num_users)\n",
    "        self.b_i = np.zeros(self.num_items)\n",
    "        self.b = np.mean(self.R[np.where(self.R != 0)])\n",
    "        \n",
    "        # Create a list of training samples\n",
    "        self.samples = [\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] > 0\n",
    "        ]\n",
    "        \n",
    "        # Perform stochastic gradient descent for number of iterations\n",
    "        for i in range(self.iterations):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            if (i+1) % 10 == 0:\n",
    "                mse = self.mse()\n",
    "                print(\"Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "        \n",
    "        return self.full_matrix()\n",
    "\n",
    "    def mse(self):\n",
    "        \"\"\"\n",
    "        A function to compute the total mean square error\n",
    "        \"\"\"\n",
    "        xs, ys = self.R.nonzero()\n",
    "        predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        for x, y in zip(xs, ys):\n",
    "            error += pow(self.R[x, y] - predicted[x, y], 2)\n",
    "        return np.sqrt(error)\n",
    "\n",
    "    def sgd(self):\n",
    "        \"\"\"\n",
    "        Perform stochastic graident descent\n",
    "        \"\"\"\n",
    "        for i, j, r in self.samples:\n",
    "            # Computer prediction and error\n",
    "            prediction = self.get_rating(i, j)\n",
    "            e = (r - prediction)\n",
    "            \n",
    "            # Update biases\n",
    "            self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i])\n",
    "            self.b_i[j] += self.alpha * (e - self.beta * self.b_i[j])\n",
    "            \n",
    "            # Create copy of row of P since we need to update it but use older values for update on Q\n",
    "            P_i = self.P[i, :][:]\n",
    "            \n",
    "            # Update user and item latent feature matrices\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            self.Q[j, :] += self.alpha * (e * P_i - self.beta * self.Q[j,:])\n",
    "\n",
    "    def get_rating(self, i, j):\n",
    "        \"\"\"\n",
    "        Get the predicted rating of user i and item j\n",
    "        \"\"\"\n",
    "        prediction = self.b + self.b_u[i] + self.b_i[j] + self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "    \n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        Computer the full matrix using the resultant biases, P and Q\n",
    "        \"\"\"\n",
    "        return self.b + self.b_u[:,np.newaxis] + self.b_i[np.newaxis:,] + self.P.dot(self.Q.T)\n",
    "\n",
    "# Construct matrix and train model\n",
    "N =len(trainset['csr'].unique())+1\n",
    "M =len(trainset['ke'].unique())+1\n",
    "R = np.zeros((N, M))\n",
    "for index, row in trainset.iterrows(): \n",
    "    R[int(row['csr'])][int(row['ke'])] = row['num']\n",
    "mf = IRTMF(R, K=100, alpha=0.001, beta=0.01, iterations=100)\n",
    "nR = mf.train() #return the predicted matrix\n",
    "#evaluate model\n",
    "squaredError = []\n",
    "for _, row in testset.iterrows(): \n",
    "    error=float(row['num'])-float(nR[int(row['csr'])][int(row['ke'])])\n",
    "    squaredError.append(error * error)\n",
    "RMSE =math.sqrt(sum(squaredError) / len(squaredError))\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape is:2547452 rows and 3 columns\n",
      "  K          Precisions             Recalls                NDCG\n",
      "  5          0.46459149          0.55998973          0.60620035\n",
      " 10          0.44782239          0.61048597          0.65218110\n",
      " 15          0.44397245          0.62228409          0.67561003\n",
      " 20          0.44310821          0.62424625          0.69222003\n",
      "RMSE: 1.2989\n"
     ]
    }
   ],
   "source": [
    "#Baseline for knowledgeBase dataset \n",
    "import surprise as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "#1.Loading the dataset and Excluding the outliers\n",
    "kbdata = pd.read_csv(\"/data/fjsdata/ctKngBase/kb.csv\", sep='|', low_memory=False)\n",
    "kbdata = kbdata.loc[(kbdata['num']<200)]#seven months, one per day\n",
    "num_max=kbdata['num'].max()\n",
    "num_min=kbdata['num'].min()\n",
    "kbdata['num']=kbdata['num'].apply(lambda x: (x-num_min+1)*1.0/(num_max-num_min+1) )\n",
    "print ('Dataset shape is:%d rows and %d columns'%(kbdata.shape[0],kbdata.shape[1]))\n",
    "#2.Transforming into data format of surprise and spliting the train-set and test-set\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "reader = sp.Reader(rating_scale=(0, 1))\n",
    "spdata = sp.Dataset.load_from_df(kbdata[['csr', 'ke', 'num']],reader)\n",
    "# sampling random trainset and testset, and test set is made of 10% of the ratings.\n",
    "#trainset, testset = sp.model_selection.train_test_split(spdata, test_size=.1)\n",
    "trainset = spdata.build_full_trainset()\n",
    "testset = trainset.build_testset()\n",
    "\n",
    "#3.Training the model and predicting ratings for the testset\n",
    "algo = sp.SVD()#NMF,SVDpp,n_factors=100\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)#testset include positive and negtive sample.\n",
    "\n",
    "#4.measuring the performance of SVD by precision, recall and  NDCG\n",
    "#print ('RMSE of testset is:%.8f'%(sp.accuracy.rmse(predictions)))\n",
    "def calc_dcg(items):\n",
    "    dcg = 0\n",
    "    i = 0\n",
    "    for item in items:\n",
    "        i += 1\n",
    "        dcg += (math.pow(2, item) - 1)/ math.log(1 + i, 2)\n",
    "    return dcg\n",
    "def index_at_k(predictions, k, threshold=0.1):\n",
    "   #Return precision and recall at k metrics for each user.\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    ndcgs =dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r > threshold) for (_, true_r) in user_ratings)\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est > threshold) for (est, _) in user_ratings[:k])\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r > threshold) and (est > threshold)) for (est, true_r) in user_ratings[:k])\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "        #true ratings of recommended items in top k\n",
    "        l_rec_k = [true_r for (_,true_r) in user_ratings[:k]]\n",
    "        dcg = calc_dcg(l_rec_k)\n",
    "        #l_rec_k.sort(reverse=True)\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        l_rel_k = [true_r for (_,true_r) in user_ratings[:k]]\n",
    "        idcg = calc_dcg(l_rel_k)\n",
    "        ndcgs[uid]=dcg*1.0/idcg \n",
    "    return precisions, recalls, ndcgs\n",
    "\n",
    "print (\"%3s%20s%20s%20s\" % ('K','Precisions','Recalls','NDCG'))\n",
    "for k in [5,10,15,20]:#latent factor\n",
    "    precisions, recalls, ndcgs = index_at_k(predictions, k=k)\n",
    "    # Precision and recall can then be averaged over all users\n",
    "    precision = sum(prec for prec in precisions.values()) / len(precisions)\n",
    "    recall = sum(rec for rec in recalls.values()) / len(recalls)\n",
    "    ndcg = sum(ndcg for ndcg in ndcgs.values()) / len(ndcgs)\n",
    "    print (\"%3s%20.8f%20.8f%20.8f\" % (k, precision, recall, ndcg))\n",
    "RMSE = sp.accuracy.rmse(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
