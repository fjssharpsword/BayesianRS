{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: overflow encountered in square\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: RuntimeWarning: overflow encountered in multiply\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: overflow encountered in double_scalars\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: RuntimeWarning: overflow encountered in multiply\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in add\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "nan\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pylab import *\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "def SGD(train,test,N,M,eta,K,lambda_1,lambda_2,Step):\n",
    "    # train: train data\n",
    "    # test: test data\n",
    "    # N:the number of user\n",
    "    # M:the number of item\n",
    "    # eta: the learning rata\n",
    "    # K: the number of latent factor\n",
    "    # lambda_1,lambda_2: regularization parameters\n",
    "    # Step: the max iteration\n",
    "    U = np.random.normal(0, 0.1, (N, K))\n",
    "    V = np.random.normal(0, 0.1, (M, K))\n",
    "    L=1000.0\n",
    "    rmse=[]\n",
    "    loss=[]\n",
    "    for ste in range(Step):\n",
    "        los=0.0\n",
    "        for data in train:\n",
    "            u=data[0]\n",
    "            i=data[1]\n",
    "            r=data[2]\n",
    "\n",
    "            e=r-np.dot(U[u],V[i].T)            \n",
    "            U[u]=U[u]+eta*(e*V[i]-lambda_1*U[u])\n",
    "            V[i]=V[i]+eta*(e*U[u]-lambda_2*V[i])\n",
    "\n",
    "            los=los+0.5*(e**2+lambda_1*np.square(U[u]).sum()+lambda_2*np.square(V[i]).sum())\n",
    "        loss.append(los)\n",
    "        rms=RMSE(U,V,test)\n",
    "        rmse.append(rms)\n",
    "        if los<L:\n",
    "            break\n",
    "        if ste%10==0:\n",
    "            print (ste/10)\n",
    "    return loss,rmse,U,V\n",
    "\n",
    "           \n",
    "def RMSE(U,V,test):\n",
    "    count=len(test)\n",
    "    sum_rmse=0.0\n",
    "    for t in test:\n",
    "        u=t[0]\n",
    "        i=t[1]\n",
    "        r=t[2]\n",
    "        pr=np.dot(U[u],V[i].T)\n",
    "        sum_rmse+=np.square(r-pr)\n",
    "    rmse=np.sqrt(sum_rmse/count)\n",
    "    return rmse\n",
    "\n",
    "\n",
    "def Load_data(filedir,ratio):\n",
    "    user_set={}\n",
    "    item_set={}\n",
    "    N=0;#the number of user\n",
    "    M=0;#the number of item\n",
    "    u_idx=0\n",
    "    i_idx=0\n",
    "    data=[]\n",
    "    f = open(filedir)\n",
    "    line_num = 0\n",
    "    for line in f.readlines():\n",
    "        line_num += 1\n",
    "        if (line_num==1):continue\n",
    "        u,i,r=line.split('|')\n",
    "        if int(u) not in user_set:\n",
    "            user_set[int(u)]=u_idx\n",
    "            u_idx+=1\n",
    "        if int(i) not in item_set:\n",
    "            item_set[int(i)]=i_idx\n",
    "            i_idx+=1\n",
    "        data.append([user_set[int(u)],item_set[int(i)],int(r)])\n",
    "    f.close()\n",
    "    N=u_idx;\n",
    "    M=i_idx;\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "    train=data[0:int(len(data)*ratio)]\n",
    "    test=data[int(len(data)*ratio):]\n",
    "    return N,M,train,test\n",
    "\n",
    "\n",
    "def Figure(loss,rmse):\n",
    "    fig1=plt.figure('LOSS')\n",
    "    x = range(len(loss))\n",
    "    plot(x, loss, color='g',linewidth=3)\n",
    "    plt.title('Convergence curve')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    fig2=plt.figure('RMSE')\n",
    "    x = range(len(rmse))\n",
    "    plot(x, rmse, color='r',linewidth=3)\n",
    "    plt.title('Convergence curve')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('RMSE')\n",
    "    show()\n",
    "\n",
    "#----------------------------SELF TEST----------------------------#\n",
    " \n",
    "def main():\n",
    "    dir_data=\"/data/fjsdata/ctKngBase/kb.csv\"\n",
    "    ratio=0.8\n",
    "    N,M,train,test=Load_data(dir_data,ratio)\n",
    "        \n",
    "    eta=0.005\n",
    "    K=10\n",
    "    lambda_1=0.1\n",
    "    lambda_2=0.1\n",
    "    Step=50\n",
    "    loss,rmse,U,V=SGD(train,test,N,M,eta,K,lambda_1,lambda_2,Step)\n",
    "    print (rmse[-1]);\n",
    "    Figure(loss,rmse)\n",
    "    \n",
    "         \n",
    "if __name__ == '__main__': \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "import pickle\n",
    "import os\n",
    "import copy\n",
    "class PMF():\n",
    "    '''\n",
    "    a class for this Double Co-occurence Factorization model\n",
    "    '''\n",
    "    # initialize some paprameters\n",
    "    def __init__(self, R, lambda_alpha=1e-2, lambda_beta=1e-2, latent_size=50, momuntum=0.8,\n",
    "                 lr=0.001, iters=200, seed=None):\n",
    "        self.lambda_alpha = lambda_alpha\n",
    "        self.lambda_beta = lambda_beta\n",
    "        self.momuntum = momuntum\n",
    "        self.R = R\n",
    "        self.random_state = RandomState(seed)\n",
    "        self.iterations = iters\n",
    "        self.lr = lr\n",
    "        self.I = copy.deepcopy(self.R)\n",
    "        self.I[self.I != 0] = 1\n",
    "\n",
    "        self.U = 0.1*self.random_state.rand(np.size(R, 0), latent_size)\n",
    "        self.V = 0.1*self.random_state.rand(np.size(R, 1), latent_size)\n",
    "\n",
    "\n",
    "    def loss(self):\n",
    "        # the loss function of the model\n",
    "        loss = np.sum(self.I*(self.R-np.dot(self.U, self.V.T))**2) + self.lambda_alpha*np.sum(np.square(self.U)) + self.lambda_beta*np.sum(np.square(self.V))\n",
    "        return loss\n",
    "    def predict(self, data):\n",
    "        index_data = np.array([[int(ele[0]), int(ele[1])] for index,ele in data.iterrows()], dtype=int)\n",
    "        u_features = self.U.take(index_data.take(0, axis=1), axis=0)\n",
    "        v_features = self.V.take(index_data.take(1, axis=1), axis=0)\n",
    "        preds_value_array = np.sum(u_features*v_features, 1)\n",
    "        return preds_value_array\n",
    "\n",
    "    def train(self, train_data=None, vali_data=None):\n",
    "        '''\n",
    "        # training process\n",
    "        :param train_data: train data with [[i,j],...] and this indacates that K[i,j]=rating\n",
    "        :param lr: learning rate\n",
    "        :param iterations: number of iterations\n",
    "        :return: learned V, T and loss_list during iterations\n",
    "        '''\n",
    "        train_loss_list = []\n",
    "        vali_rmse_list = []\n",
    "        last_vali_rmse = None\n",
    "\n",
    "        # monemtum\n",
    "        momuntum_u = np.zeros(self.U.shape)\n",
    "        momuntum_v = np.zeros(self.V.shape)\n",
    "\n",
    "        for it in range(self.iterations):\n",
    "            # derivate of Vi\n",
    "            grads_u = np.dot(self.I*(self.R-np.dot(self.U, self.V.T)), -self.V) + self.lambda_alpha*self.U\n",
    "\n",
    "            # derivate of Tj\n",
    "            grads_v = np.dot((self.I*(self.R-np.dot(self.U, self.V.T))).T, -self.U) + self.lambda_beta*self.V\n",
    "\n",
    "            # update the parameters\n",
    "            momuntum_u = (self.momuntum * momuntum_u) + self.lr * grads_u\n",
    "            momuntum_v = (self.momuntum * momuntum_v) + self.lr * grads_v\n",
    "            self.U = self.U - momuntum_u\n",
    "            self.V = self.V - momuntum_v\n",
    "\n",
    "            # training evaluation\n",
    "            train_loss = self.loss()\n",
    "            train_loss_list.append(train_loss)\n",
    "\n",
    "            vali_preds = self.predict(vali_data)\n",
    "            vali_rmse = RMSE(vali_preds,np.array(vali_preds, vali_data['num']).tolist())\n",
    "            vali_rmse_list.append(vali_rmse)\n",
    "\n",
    "            print('traning iteration:{: d} ,loss:{: f}, vali_rmse:{: f}'.format(it, train_loss, vali_rmse))\n",
    "\n",
    "            if last_vali_rmse and (last_vali_rmse - vali_rmse) <= 0:\n",
    "                print('convergence at iterations:{: d}'.format(it))\n",
    "                break\n",
    "            else:\n",
    "                last_vali_rmse = vali_rmse\n",
    "\n",
    "        return self.U, self.V, train_loss_list, vali_rmse_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape is:2547452 rows and 3 columns\n",
      "dataset density:0.002589\n",
      "training model.......\n",
      "parameters are:ratio=0.800000, reg_u=0.010000, reg_v=0.010000, latent_size=20, lr=0.000030, iters=1000\n",
      "traning iteration: 0 ,loss: 10160.997764, vali_rmse: 0.059907\n",
      "traning iteration: 1 ,loss: 10144.829910, vali_rmse: 0.059876\n",
      "traning iteration: 2 ,loss: 10121.960476, vali_rmse: 0.059833\n",
      "traning iteration: 3 ,loss: 10093.259835, vali_rmse: 0.059779\n",
      "traning iteration: 4 ,loss: 10059.544999, vali_rmse: 0.059716\n",
      "traning iteration: 5 ,loss: 10021.571722, vali_rmse: 0.059644\n",
      "traning iteration: 6 ,loss: 9980.029782, vali_rmse: 0.059566\n",
      "traning iteration: 7 ,loss: 9935.541100, vali_rmse: 0.059482\n",
      "traning iteration: 8 ,loss: 9888.660294, vali_rmse: 0.059394\n",
      "traning iteration: 9 ,loss: 9839.877178, vali_rmse: 0.059303\n",
      "traning iteration: 10 ,loss: 9789.620743, vali_rmse: 0.059210\n",
      "traning iteration: 11 ,loss: 9738.264127, vali_rmse: 0.059115\n",
      "traning iteration: 12 ,loss: 9686.130153, vali_rmse: 0.059019\n",
      "traning iteration: 13 ,loss: 9633.497053, vali_rmse: 0.058923\n",
      "traning iteration: 14 ,loss: 9580.604076, vali_rmse: 0.058828\n",
      "traning iteration: 15 ,loss: 9527.656742, vali_rmse: 0.058734\n",
      "traning iteration: 16 ,loss: 9474.831590, vali_rmse: 0.058641\n",
      "traning iteration: 17 ,loss: 9422.280330, vali_rmse: 0.058550\n",
      "traning iteration: 18 ,loss: 9370.133354, vali_rmse: 0.058461\n",
      "traning iteration: 19 ,loss: 9318.502653, vali_rmse: 0.058374\n",
      "traning iteration: 20 ,loss: 9267.484152, vali_rmse: 0.058290\n",
      "traning iteration: 21 ,loss: 9217.159581, vali_rmse: 0.058209\n",
      "traning iteration: 22 ,loss: 9167.597935, vali_rmse: 0.058131\n",
      "traning iteration: 23 ,loss: 9118.856639, vali_rmse: 0.058056\n",
      "traning iteration: 24 ,loss: 9070.982494, vali_rmse: 0.057984\n",
      "traning iteration: 25 ,loss: 9024.012479, vali_rmse: 0.057916\n",
      "traning iteration: 26 ,loss: 8977.974483, vali_rmse: 0.057850\n",
      "traning iteration: 27 ,loss: 8932.887997, vali_rmse: 0.057788\n",
      "traning iteration: 28 ,loss: 8888.764814, vali_rmse: 0.057728\n",
      "traning iteration: 29 ,loss: 8845.609747, vali_rmse: 0.057672\n",
      "traning iteration: 30 ,loss: 8803.421369, vali_rmse: 0.057620\n",
      "traning iteration: 31 ,loss: 8762.192771, vali_rmse: 0.057570\n",
      "traning iteration: 32 ,loss: 8721.912336, vali_rmse: 0.057523\n",
      "traning iteration: 33 ,loss: 8682.564489, vali_rmse: 0.057479\n",
      "traning iteration: 34 ,loss: 8644.130443, vali_rmse: 0.057439\n",
      "traning iteration: 35 ,loss: 8606.588884, vali_rmse: 0.057401\n",
      "traning iteration: 36 ,loss: 8569.916613, vali_rmse: 0.057365\n",
      "traning iteration: 37 ,loss: 8534.089115, vali_rmse: 0.057333\n",
      "traning iteration: 38 ,loss: 8499.081056, vali_rmse: 0.057303\n",
      "traning iteration: 39 ,loss: 8464.866696, vali_rmse: 0.057275\n",
      "traning iteration: 40 ,loss: 8431.420234, vali_rmse: 0.057251\n",
      "traning iteration: 41 ,loss: 8398.716071, vali_rmse: 0.057228\n",
      "traning iteration: 42 ,loss: 8366.729011, vali_rmse: 0.057208\n",
      "traning iteration: 43 ,loss: 8335.434396, vali_rmse: 0.057189\n",
      "traning iteration: 44 ,loss: 8304.808200, vali_rmse: 0.057173\n",
      "traning iteration: 45 ,loss: 8274.827078, vali_rmse: 0.057159\n",
      "traning iteration: 46 ,loss: 8245.468385, vali_rmse: 0.057147\n",
      "traning iteration: 47 ,loss: 8216.710178, vali_rmse: 0.057137\n",
      "traning iteration: 48 ,loss: 8188.531197, vali_rmse: 0.057129\n",
      "traning iteration: 49 ,loss: 8160.910849, vali_rmse: 0.057122\n",
      "traning iteration: 50 ,loss: 8133.829182, vali_rmse: 0.057117\n",
      "traning iteration: 51 ,loss: 8107.266867, vali_rmse: 0.057114\n",
      "traning iteration: 52 ,loss: 8081.205174, vali_rmse: 0.057112\n",
      "traning iteration: 53 ,loss: 8055.625968, vali_rmse: 0.057112\n",
      "traning iteration: 54 ,loss: 8030.511695, vali_rmse: 0.057113\n",
      "convergence at iterations: 54\n",
      "testing model.......\n",
      "test rmse:0.056470\n"
     ]
    }
   ],
   "source": [
    "import surprise as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from scipy import stats\n",
    "import re\n",
    "\n",
    "def RMSE(preds, truth):\n",
    "    return np.sqrt(np.mean(np.square(preds-truth)))\n",
    "\n",
    "#1.Loading the dataset and Excluding the outliers\n",
    "datatype = {'csr':int,'ke':int,'num':float}\n",
    "kbdata = pd.read_csv(\"/data/fjsdata/ctKngBase/kb.csv\", sep='|', low_memory=False,dtype=datatype)\n",
    "kbdata = kbdata.loc[(kbdata['num']<200)]#seven months, one per day\n",
    "num_max=kbdata['num'].max()\n",
    "num_min=kbdata['num'].min()\n",
    "kbdata['num']=kbdata['num'].apply(lambda x: (x-num_min+1)*1.0/(num_max-num_min+1) )\n",
    "print ('Dataset shape is:%d rows and %d columns'%(kbdata.shape[0],kbdata.shape[1]))\n",
    "# set split ratio\n",
    "ratio = 0.8\n",
    "train_data = data[:int(ratio*data.shape[0])]\n",
    "vali_data = data[int(ratio*data.shape[0]):int((ratio+(1-ratio)/2)*data.shape[0])]\n",
    "test_data = data[int((ratio+(1-ratio)/2)*data.shape[0]):]\n",
    "\n",
    "NUM_USERS = kbdata['csr'].max() + 1\n",
    "NUM_ITEMS = kbdata['ke'].max() + 1\n",
    "print('dataset density:{:f}'.format(len(data)*1.0/(NUM_USERS*NUM_ITEMS)))\n",
    "\n",
    "R = np.zeros([NUM_USERS, NUM_ITEMS])\n",
    "for index,ele in train_data.iterrows():\n",
    "    R[int(ele[0]), int(ele[1])] = float(ele[2])\n",
    "\n",
    "# construct model\n",
    "print('training model.......')\n",
    "lambda_alpha = 0.01\n",
    "lambda_beta = 0.01\n",
    "latent_size = 20\n",
    "lr = 3e-5\n",
    "iters = 1000\n",
    "model = PMF(R=R, lambda_alpha=lambda_alpha, lambda_beta=lambda_beta, latent_size=latent_size, momuntum=0.9, lr=lr, iters=iters, seed=1)\n",
    "print('parameters are:ratio={:f}, reg_u={:f}, reg_v={:f}, latent_size={:d}, lr={:f}, iters={:d}'.format(ratio, lambda_alpha, lambda_beta, latent_size,lr, iters))\n",
    "U, V, train_loss_list, vali_rmse_list = model.train(train_data=train_data, vali_data=vali_data)\n",
    "\n",
    "print('testing model.......')\n",
    "preds = model.predict(data=test_data)\n",
    "test_rmse = RMSE(preds, np.array(test_data['num']).tolist())\n",
    "\n",
    "print('test rmse:{:f}'.format(test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape is:2547452 rows and 3 columns\n",
      "dataset density:0.002589\n",
      "training model.......\n",
      "parameters are:ratio=0.800000, reg_u=0.010000, reg_v=0.010000, latent_size=20, lr=0.000030, iters=1000\n",
      "traning iteration: 0 ,loss: 5710.950688, vali_rmse: 0.000000\n",
      "traning iteration: 1 ,loss: 5697.482041, vali_rmse: 0.000000\n",
      "traning iteration: 2 ,loss: 5678.368057, vali_rmse: 0.000000\n",
      "traning iteration: 3 ,loss: 5654.276976, vali_rmse: 0.000000\n",
      "traning iteration: 4 ,loss: 5625.831290, vali_rmse: 0.000000\n",
      "traning iteration: 5 ,loss: 5593.607225, vali_rmse: 0.000000\n",
      "traning iteration: 6 ,loss: 5558.135107, vali_rmse: 0.000000\n",
      "traning iteration: 7 ,loss: 5519.900403, vali_rmse: 0.000000\n",
      "traning iteration: 8 ,loss: 5479.345289, vali_rmse: 0.000000\n",
      "traning iteration: 9 ,loss: 5436.870572, vali_rmse: 0.000000\n",
      "traning iteration: 10 ,loss: 5392.837883, vali_rmse: 0.000000\n",
      "traning iteration: 11 ,loss: 5347.572025, vali_rmse: 0.000000\n",
      "traning iteration: 12 ,loss: 5301.363411, vali_rmse: 0.000000\n",
      "traning iteration: 13 ,loss: 5254.470529, vali_rmse: 0.000000\n",
      "traning iteration: 14 ,loss: 5207.122383, vali_rmse: 0.000000\n",
      "traning iteration: 15 ,loss: 5159.520880, vali_rmse: 0.000000\n",
      "traning iteration: 16 ,loss: 5111.843125, vali_rmse: 0.000000\n",
      "traning iteration: 17 ,loss: 5064.243619, vali_rmse: 0.000000\n",
      "traning iteration: 18 ,loss: 5016.856330, vali_rmse: 0.000000\n",
      "traning iteration: 19 ,loss: 4969.796648, vali_rmse: 0.000000\n",
      "traning iteration: 20 ,loss: 4923.163202, vali_rmse: 0.000000\n",
      "traning iteration: 21 ,loss: 4877.039552, vali_rmse: 0.000000\n",
      "traning iteration: 22 ,loss: 4831.495751, vali_rmse: 0.000000\n",
      "traning iteration: 23 ,loss: 4786.589790, vali_rmse: 0.000000\n",
      "traning iteration: 24 ,loss: 4742.368916, vali_rmse: 0.000000\n",
      "traning iteration: 25 ,loss: 4698.870845, vali_rmse: 0.000000\n",
      "traning iteration: 26 ,loss: 4656.124872, vali_rmse: 0.000000\n",
      "traning iteration: 27 ,loss: 4614.152870, vali_rmse: 0.000000\n",
      "traning iteration: 28 ,loss: 4572.970216, vali_rmse: 0.000000\n",
      "traning iteration: 29 ,loss: 4532.586614, vali_rmse: 0.000000\n",
      "traning iteration: 30 ,loss: 4493.006850, vali_rmse: 0.000000\n",
      "traning iteration: 31 ,loss: 4454.231471, vali_rmse: 0.000000\n",
      "traning iteration: 32 ,loss: 4416.257388, vali_rmse: 0.000000\n",
      "traning iteration: 33 ,loss: 4379.078432, vali_rmse: 0.000000\n",
      "traning iteration: 34 ,loss: 4342.685835, vali_rmse: 0.000000\n",
      "traning iteration: 35 ,loss: 4307.068665, vali_rmse: 0.000000\n",
      "traning iteration: 36 ,loss: 4272.214217, vali_rmse: 0.000000\n",
      "traning iteration: 37 ,loss: 4238.108346, vali_rmse: 0.000000\n",
      "traning iteration: 38 ,loss: 4204.735769, vali_rmse: 0.000000\n",
      "traning iteration: 39 ,loss: 4172.080325, vali_rmse: 0.000000\n",
      "traning iteration: 40 ,loss: 4140.125202, vali_rmse: 0.000000\n",
      "traning iteration: 41 ,loss: 4108.853132, vali_rmse: 0.000000\n",
      "traning iteration: 42 ,loss: 4078.246557, vali_rmse: 0.000000\n",
      "traning iteration: 43 ,loss: 4048.287777, vali_rmse: 0.000000\n",
      "traning iteration: 44 ,loss: 4018.959064, vali_rmse: 0.000000\n",
      "traning iteration: 45 ,loss: 3990.242771, vali_rmse: 0.000000\n",
      "traning iteration: 46 ,loss: 3962.121412, vali_rmse: 0.000000\n",
      "traning iteration: 47 ,loss: 3934.577735, vali_rmse: 0.000000\n",
      "traning iteration: 48 ,loss: 3907.594775, vali_rmse: 0.000000\n",
      "traning iteration: 49 ,loss: 3881.155905, vali_rmse: 0.000000\n",
      "traning iteration: 50 ,loss: 3855.244867, vali_rmse: 0.000000\n",
      "traning iteration: 51 ,loss: 3829.845804, vali_rmse: 0.000000\n",
      "traning iteration: 52 ,loss: 3804.943281, vali_rmse: 0.000000\n",
      "traning iteration: 53 ,loss: 3780.522295, vali_rmse: 0.000000\n",
      "traning iteration: 54 ,loss: 3756.568290, vali_rmse: 0.000000\n",
      "traning iteration: 55 ,loss: 3733.067163, vali_rmse: 0.000000\n",
      "traning iteration: 56 ,loss: 3710.005261, vali_rmse: 0.000000\n",
      "traning iteration: 57 ,loss: 3687.369383, vali_rmse: 0.000000\n",
      "traning iteration: 58 ,loss: 3665.146775, vali_rmse: 0.000000\n",
      "traning iteration: 59 ,loss: 3643.325127, vali_rmse: 0.000000\n",
      "traning iteration: 60 ,loss: 3621.892559, vali_rmse: 0.000000\n",
      "traning iteration: 61 ,loss: 3600.837618, vali_rmse: 0.000000\n",
      "traning iteration: 62 ,loss: 3580.149263, vali_rmse: 0.000000\n",
      "traning iteration: 63 ,loss: 3559.816856, vali_rmse: 0.000000\n",
      "traning iteration: 64 ,loss: 3539.830149, vali_rmse: 0.000000\n",
      "traning iteration: 65 ,loss: 3520.179272, vali_rmse: 0.000000\n",
      "traning iteration: 66 ,loss: 3500.854719, vali_rmse: 0.000000\n",
      "traning iteration: 67 ,loss: 3481.847335, vali_rmse: 0.000000\n",
      "traning iteration: 68 ,loss: 3463.148304, vali_rmse: 0.000000\n",
      "traning iteration: 69 ,loss: 3444.749136, vali_rmse: 0.000000\n",
      "traning iteration: 70 ,loss: 3426.641650, vali_rmse: 0.000000\n",
      "traning iteration: 71 ,loss: 3408.817969, vali_rmse: 0.000000\n",
      "traning iteration: 72 ,loss: 3391.270501, vali_rmse: 0.000000\n",
      "traning iteration: 73 ,loss: 3373.991930, vali_rmse: 0.000000\n",
      "traning iteration: 74 ,loss: 3356.975205, vali_rmse: 0.000000\n",
      "traning iteration: 75 ,loss: 3340.213526, vali_rmse: 0.000000\n",
      "traning iteration: 76 ,loss: 3323.700338, vali_rmse: 0.000000\n",
      "traning iteration: 77 ,loss: 3307.429314, vali_rmse: 0.000000\n",
      "traning iteration: 78 ,loss: 3291.394354, vali_rmse: 0.000000\n",
      "traning iteration: 79 ,loss: 3275.589568, vali_rmse: 0.000000\n",
      "traning iteration: 80 ,loss: 3260.009270, vali_rmse: 0.000000\n",
      "traning iteration: 81 ,loss: 3244.647970, vali_rmse: 0.000000\n",
      "traning iteration: 82 ,loss: 3229.500366, vali_rmse: 0.000000\n",
      "traning iteration: 83 ,loss: 3214.561334, vali_rmse: 0.000000\n",
      "traning iteration: 84 ,loss: 3199.825923, vali_rmse: 0.000000\n",
      "traning iteration: 85 ,loss: 3185.289346, vali_rmse: 0.000000\n",
      "traning iteration: 86 ,loss: 3170.946975, vali_rmse: 0.000000\n",
      "traning iteration: 87 ,loss: 3156.794334, vali_rmse: 0.000000\n",
      "traning iteration: 88 ,loss: 3142.827091, vali_rmse: 0.000000\n",
      "traning iteration: 89 ,loss: 3129.041055, vali_rmse: 0.000000\n",
      "traning iteration: 90 ,loss: 3115.432167, vali_rmse: 0.000000\n",
      "traning iteration: 91 ,loss: 3101.996501, vali_rmse: 0.000000\n",
      "traning iteration: 92 ,loss: 3088.730249, vali_rmse: 0.000000\n",
      "traning iteration: 93 ,loss: 3075.629726, vali_rmse: 0.000000\n",
      "traning iteration: 94 ,loss: 3062.691359, vali_rmse: 0.000000\n",
      "traning iteration: 95 ,loss: 3049.911686, vali_rmse: 0.000000\n",
      "traning iteration: 96 ,loss: 3037.287348, vali_rmse: 0.000000\n",
      "traning iteration: 97 ,loss: 3024.815090, vali_rmse: 0.000000\n",
      "traning iteration: 98 ,loss: 3012.491753, vali_rmse: 0.000000\n",
      "traning iteration: 99 ,loss: 3000.314274, vali_rmse: 0.000000\n",
      "traning iteration: 100 ,loss: 2988.279678, vali_rmse: 0.000000\n",
      "traning iteration: 101 ,loss: 2976.385077, vali_rmse: 0.000000\n",
      "traning iteration: 102 ,loss: 2964.627670, vali_rmse: 0.000000\n",
      "traning iteration: 103 ,loss: 2953.004734, vali_rmse: 0.000000\n",
      "traning iteration: 104 ,loss: 2941.513626, vali_rmse: 0.000000\n",
      "traning iteration: 105 ,loss: 2930.151775, vali_rmse: 0.000000\n",
      "traning iteration: 106 ,loss: 2918.916686, vali_rmse: 0.000000\n",
      "traning iteration: 107 ,loss: 2907.805932, vali_rmse: 0.000000\n",
      "traning iteration: 108 ,loss: 2896.817154, vali_rmse: 0.000000\n",
      "traning iteration: 109 ,loss: 2885.948058, vali_rmse: 0.000000\n",
      "traning iteration: 110 ,loss: 2875.196414, vali_rmse: 0.000000\n",
      "traning iteration: 111 ,loss: 2864.560050, vali_rmse: 0.000000\n",
      "traning iteration: 112 ,loss: 2854.036856, vali_rmse: 0.000000\n",
      "traning iteration: 113 ,loss: 2843.624777, vali_rmse: 0.000000\n",
      "traning iteration: 114 ,loss: 2833.321811, vali_rmse: 0.000000\n",
      "traning iteration: 115 ,loss: 2823.126013, vali_rmse: 0.000000\n",
      "traning iteration: 116 ,loss: 2813.035485, vali_rmse: 0.000000\n",
      "traning iteration: 117 ,loss: 2803.048380, vali_rmse: 0.000000\n",
      "traning iteration: 118 ,loss: 2793.162900, vali_rmse: 0.000000\n",
      "traning iteration: 119 ,loss: 2783.377292, vali_rmse: 0.000000\n",
      "traning iteration: 120 ,loss: 2773.689847, vali_rmse: 0.000000\n",
      "traning iteration: 121 ,loss: 2764.098901, vali_rmse: 0.000000\n",
      "traning iteration: 122 ,loss: 2754.602831, vali_rmse: 0.000000\n",
      "traning iteration: 123 ,loss: 2745.200053, vali_rmse: 0.000000\n",
      "traning iteration: 124 ,loss: 2735.889026, vali_rmse: 0.000000\n",
      "traning iteration: 125 ,loss: 2726.668242, vali_rmse: 0.000000\n",
      "traning iteration: 126 ,loss: 2717.536234, vali_rmse: 0.000000\n",
      "traning iteration: 127 ,loss: 2708.491569, vali_rmse: 0.000000\n",
      "traning iteration: 128 ,loss: 2699.532847, vali_rmse: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning iteration: 129 ,loss: 2690.658705, vali_rmse: 0.000000\n",
      "traning iteration: 130 ,loss: 2681.867809, vali_rmse: 0.000000\n",
      "traning iteration: 131 ,loss: 2673.158858, vali_rmse: 0.000000\n",
      "traning iteration: 132 ,loss: 2664.530581, vali_rmse: 0.000000\n",
      "traning iteration: 133 ,loss: 2655.981737, vali_rmse: 0.000000\n",
      "traning iteration: 134 ,loss: 2647.511112, vali_rmse: 0.000000\n",
      "traning iteration: 135 ,loss: 2639.117523, vali_rmse: 0.000000\n",
      "traning iteration: 136 ,loss: 2630.799811, vali_rmse: 0.000000\n",
      "traning iteration: 137 ,loss: 2622.556843, vali_rmse: 0.000000\n",
      "traning iteration: 138 ,loss: 2614.387514, vali_rmse: 0.000000\n",
      "traning iteration: 139 ,loss: 2606.290741, vali_rmse: 0.000000\n",
      "traning iteration: 140 ,loss: 2598.265466, vali_rmse: 0.000000\n",
      "traning iteration: 141 ,loss: 2590.310654, vali_rmse: 0.000000\n",
      "traning iteration: 142 ,loss: 2582.425294, vali_rmse: 0.000000\n",
      "traning iteration: 143 ,loss: 2574.608393, vali_rmse: 0.000000\n",
      "traning iteration: 144 ,loss: 2566.858983, vali_rmse: 0.000000\n",
      "traning iteration: 145 ,loss: 2559.176115, vali_rmse: 0.000000\n",
      "traning iteration: 146 ,loss: 2551.558861, vali_rmse: 0.000000\n",
      "traning iteration: 147 ,loss: 2544.006312, vali_rmse: 0.000000\n",
      "traning iteration: 148 ,loss: 2536.517576, vali_rmse: 0.000000\n",
      "traning iteration: 149 ,loss: 2529.091783, vali_rmse: 0.000000\n",
      "traning iteration: 150 ,loss: 2521.728078, vali_rmse: 0.000000\n",
      "traning iteration: 151 ,loss: 2514.425625, vali_rmse: 0.000000\n",
      "traning iteration: 152 ,loss: 2507.183604, vali_rmse: 0.000000\n",
      "traning iteration: 153 ,loss: 2500.001213, vali_rmse: 0.000000\n",
      "traning iteration: 154 ,loss: 2492.877663, vali_rmse: 0.000000\n",
      "traning iteration: 155 ,loss: 2485.812185, vali_rmse: 0.000000\n",
      "traning iteration: 156 ,loss: 2478.804021, vali_rmse: 0.000000\n",
      "traning iteration: 157 ,loss: 2471.852431, vali_rmse: 0.000000\n",
      "traning iteration: 158 ,loss: 2464.956686, vali_rmse: 0.000000\n",
      "traning iteration: 159 ,loss: 2458.116074, vali_rmse: 0.000000\n",
      "traning iteration: 160 ,loss: 2451.329895, vali_rmse: 0.000000\n",
      "traning iteration: 161 ,loss: 2444.597464, vali_rmse: 0.000000\n",
      "traning iteration: 162 ,loss: 2437.918107, vali_rmse: 0.000000\n",
      "traning iteration: 163 ,loss: 2431.291163, vali_rmse: 0.000000\n",
      "traning iteration: 164 ,loss: 2424.715985, vali_rmse: 0.000000\n",
      "traning iteration: 165 ,loss: 2418.191935, vali_rmse: 0.000000\n",
      "traning iteration: 166 ,loss: 2411.718390, vali_rmse: 0.000000\n",
      "traning iteration: 167 ,loss: 2405.294736, vali_rmse: 0.000000\n",
      "traning iteration: 168 ,loss: 2398.920371, vali_rmse: 0.000000\n",
      "traning iteration: 169 ,loss: 2392.594704, vali_rmse: 0.000000\n",
      "traning iteration: 170 ,loss: 2386.317154, vali_rmse: 0.000000\n",
      "traning iteration: 171 ,loss: 2380.087150, vali_rmse: 0.000000\n",
      "traning iteration: 172 ,loss: 2373.904133, vali_rmse: 0.000000\n",
      "traning iteration: 173 ,loss: 2367.767551, vali_rmse: 0.000000\n",
      "traning iteration: 174 ,loss: 2361.676864, vali_rmse: 0.000000\n",
      "traning iteration: 175 ,loss: 2355.631541, vali_rmse: 0.000000\n",
      "traning iteration: 176 ,loss: 2349.631058, vali_rmse: 0.000000\n",
      "traning iteration: 177 ,loss: 2343.674903, vali_rmse: 0.000000\n",
      "traning iteration: 178 ,loss: 2337.762570, vali_rmse: 0.000000\n",
      "traning iteration: 179 ,loss: 2331.893563, vali_rmse: 0.000000\n",
      "traning iteration: 180 ,loss: 2326.067393, vali_rmse: 0.000000\n",
      "traning iteration: 181 ,loss: 2320.283583, vali_rmse: 0.000000\n",
      "traning iteration: 182 ,loss: 2314.541658, vali_rmse: 0.000000\n",
      "traning iteration: 183 ,loss: 2308.841155, vali_rmse: 0.000000\n",
      "traning iteration: 184 ,loss: 2303.181617, vali_rmse: 0.000000\n",
      "traning iteration: 185 ,loss: 2297.562596, vali_rmse: 0.000000\n",
      "traning iteration: 186 ,loss: 2291.983648, vali_rmse: 0.000000\n",
      "traning iteration: 187 ,loss: 2286.444340, vali_rmse: 0.000000\n",
      "traning iteration: 188 ,loss: 2280.944243, vali_rmse: 0.000000\n",
      "traning iteration: 189 ,loss: 2275.482935, vali_rmse: 0.000000\n",
      "traning iteration: 190 ,loss: 2270.060003, vali_rmse: 0.000000\n",
      "traning iteration: 191 ,loss: 2264.675038, vali_rmse: 0.000000\n",
      "traning iteration: 192 ,loss: 2259.327638, vali_rmse: 0.000000\n",
      "traning iteration: 193 ,loss: 2254.017408, vali_rmse: 0.000000\n",
      "traning iteration: 194 ,loss: 2248.743958, vali_rmse: 0.000000\n",
      "traning iteration: 195 ,loss: 2243.506905, vali_rmse: 0.000000\n",
      "traning iteration: 196 ,loss: 2238.305871, vali_rmse: 0.000000\n",
      "traning iteration: 197 ,loss: 2233.140483, vali_rmse: 0.000000\n",
      "traning iteration: 198 ,loss: 2228.010376, vali_rmse: 0.000000\n",
      "traning iteration: 199 ,loss: 2222.915189, vali_rmse: 0.000000\n",
      "traning iteration: 200 ,loss: 2217.854565, vali_rmse: 0.000000\n",
      "traning iteration: 201 ,loss: 2212.828154, vali_rmse: 0.000000\n",
      "traning iteration: 202 ,loss: 2207.835611, vali_rmse: 0.000000\n",
      "traning iteration: 203 ,loss: 2202.876596, vali_rmse: 0.000000\n",
      "traning iteration: 204 ,loss: 2197.950773, vali_rmse: 0.000000\n",
      "traning iteration: 205 ,loss: 2193.057813, vali_rmse: 0.000000\n",
      "traning iteration: 206 ,loss: 2188.197388, vali_rmse: 0.000000\n",
      "traning iteration: 207 ,loss: 2183.369179, vali_rmse: 0.000000\n",
      "traning iteration: 208 ,loss: 2178.572868, vali_rmse: 0.000000\n",
      "traning iteration: 209 ,loss: 2173.808143, vali_rmse: 0.000000\n",
      "traning iteration: 210 ,loss: 2169.074697, vali_rmse: 0.000000\n",
      "traning iteration: 211 ,loss: 2164.372226, vali_rmse: 0.000000\n",
      "traning iteration: 212 ,loss: 2159.700430, vali_rmse: 0.000000\n",
      "traning iteration: 213 ,loss: 2155.059016, vali_rmse: 0.000000\n",
      "traning iteration: 214 ,loss: 2150.447692, vali_rmse: 0.000000\n",
      "traning iteration: 215 ,loss: 2145.866170, vali_rmse: 0.000000\n",
      "traning iteration: 216 ,loss: 2141.314167, vali_rmse: 0.000000\n",
      "traning iteration: 217 ,loss: 2136.791405, vali_rmse: 0.000000\n",
      "traning iteration: 218 ,loss: 2132.297607, vali_rmse: 0.000000\n",
      "traning iteration: 219 ,loss: 2127.832502, vali_rmse: 0.000000\n",
      "traning iteration: 220 ,loss: 2123.395820, vali_rmse: 0.000000\n",
      "traning iteration: 221 ,loss: 2118.987299, vali_rmse: 0.000000\n",
      "traning iteration: 222 ,loss: 2114.606675, vali_rmse: 0.000000\n",
      "traning iteration: 223 ,loss: 2110.253692, vali_rmse: 0.000000\n",
      "traning iteration: 224 ,loss: 2105.928094, vali_rmse: 0.000000\n",
      "traning iteration: 225 ,loss: 2101.629631, vali_rmse: 0.000000\n",
      "traning iteration: 226 ,loss: 2097.358055, vali_rmse: 0.000000\n",
      "traning iteration: 227 ,loss: 2093.113120, vali_rmse: 0.000000\n",
      "traning iteration: 228 ,loss: 2088.894586, vali_rmse: 0.000000\n",
      "traning iteration: 229 ,loss: 2084.702214, vali_rmse: 0.000000\n",
      "traning iteration: 230 ,loss: 2080.535767, vali_rmse: 0.000000\n",
      "traning iteration: 231 ,loss: 2076.395014, vali_rmse: 0.000000\n",
      "traning iteration: 232 ,loss: 2072.279725, vali_rmse: 0.000000\n",
      "traning iteration: 233 ,loss: 2068.189672, vali_rmse: 0.000000\n",
      "traning iteration: 234 ,loss: 2064.124633, vali_rmse: 0.000000\n",
      "traning iteration: 235 ,loss: 2060.084385, vali_rmse: 0.000000\n",
      "traning iteration: 236 ,loss: 2056.068711, vali_rmse: 0.000000\n",
      "traning iteration: 237 ,loss: 2052.077394, vali_rmse: 0.000000\n",
      "traning iteration: 238 ,loss: 2048.110222, vali_rmse: 0.000000\n",
      "traning iteration: 239 ,loss: 2044.166983, vali_rmse: 0.000000\n",
      "traning iteration: 240 ,loss: 2040.247469, vali_rmse: 0.000000\n",
      "traning iteration: 241 ,loss: 2036.351476, vali_rmse: 0.000000\n",
      "traning iteration: 242 ,loss: 2032.478800, vali_rmse: 0.000000\n",
      "traning iteration: 243 ,loss: 2028.629239, vali_rmse: 0.000000\n",
      "traning iteration: 244 ,loss: 2024.802597, vali_rmse: 0.000000\n",
      "traning iteration: 245 ,loss: 2020.998677, vali_rmse: 0.000000\n",
      "traning iteration: 246 ,loss: 2017.217285, vali_rmse: 0.000000\n",
      "traning iteration: 247 ,loss: 2013.458230, vali_rmse: 0.000000\n",
      "traning iteration: 248 ,loss: 2009.721323, vali_rmse: 0.000000\n",
      "traning iteration: 249 ,loss: 2006.006376, vali_rmse: 0.000000\n",
      "traning iteration: 250 ,loss: 2002.313206, vali_rmse: 0.000000\n",
      "traning iteration: 251 ,loss: 1998.641629, vali_rmse: 0.000000\n",
      "traning iteration: 252 ,loss: 1994.991465, vali_rmse: 0.000000\n",
      "traning iteration: 253 ,loss: 1991.362535, vali_rmse: 0.000000\n",
      "traning iteration: 254 ,loss: 1987.754664, vali_rmse: 0.000000\n",
      "traning iteration: 255 ,loss: 1984.167675, vali_rmse: 0.000000\n",
      "traning iteration: 256 ,loss: 1980.601398, vali_rmse: 0.000000\n",
      "traning iteration: 257 ,loss: 1977.055661, vali_rmse: 0.000000\n",
      "traning iteration: 258 ,loss: 1973.530295, vali_rmse: 0.000000\n",
      "traning iteration: 259 ,loss: 1970.025135, vali_rmse: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning iteration: 260 ,loss: 1966.540014, vali_rmse: 0.000000\n",
      "traning iteration: 261 ,loss: 1963.074770, vali_rmse: 0.000000\n",
      "traning iteration: 262 ,loss: 1959.629242, vali_rmse: 0.000000\n",
      "traning iteration: 263 ,loss: 1956.203270, vali_rmse: 0.000000\n",
      "traning iteration: 264 ,loss: 1952.796696, vali_rmse: 0.000000\n",
      "traning iteration: 265 ,loss: 1949.409365, vali_rmse: 0.000000\n",
      "traning iteration: 266 ,loss: 1946.041121, vali_rmse: 0.000000\n",
      "traning iteration: 267 ,loss: 1942.691812, vali_rmse: 0.000000\n",
      "traning iteration: 268 ,loss: 1939.361288, vali_rmse: 0.000000\n",
      "traning iteration: 269 ,loss: 1936.049398, vali_rmse: 0.000000\n",
      "traning iteration: 270 ,loss: 1932.755995, vali_rmse: 0.000000\n",
      "traning iteration: 271 ,loss: 1929.480932, vali_rmse: 0.000000\n",
      "traning iteration: 272 ,loss: 1926.224065, vali_rmse: 0.000000\n",
      "traning iteration: 273 ,loss: 1922.985251, vali_rmse: 0.000000\n",
      "traning iteration: 274 ,loss: 1919.764347, vali_rmse: 0.000000\n",
      "traning iteration: 275 ,loss: 1916.561215, vali_rmse: 0.000000\n",
      "traning iteration: 276 ,loss: 1913.375714, vali_rmse: 0.000000\n",
      "traning iteration: 277 ,loss: 1910.207708, vali_rmse: 0.000000\n",
      "traning iteration: 278 ,loss: 1907.057061, vali_rmse: 0.000000\n",
      "traning iteration: 279 ,loss: 1903.923639, vali_rmse: 0.000000\n",
      "traning iteration: 280 ,loss: 1900.807307, vali_rmse: 0.000000\n",
      "traning iteration: 281 ,loss: 1897.707936, vali_rmse: 0.000000\n",
      "traning iteration: 282 ,loss: 1894.625394, vali_rmse: 0.000000\n",
      "traning iteration: 283 ,loss: 1891.559552, vali_rmse: 0.000000\n",
      "traning iteration: 284 ,loss: 1888.510283, vali_rmse: 0.000000\n",
      "traning iteration: 285 ,loss: 1885.477460, vali_rmse: 0.000000\n",
      "traning iteration: 286 ,loss: 1882.460958, vali_rmse: 0.000000\n",
      "traning iteration: 287 ,loss: 1879.460653, vali_rmse: 0.000000\n",
      "traning iteration: 288 ,loss: 1876.476423, vali_rmse: 0.000000\n",
      "traning iteration: 289 ,loss: 1873.508146, vali_rmse: 0.000000\n",
      "traning iteration: 290 ,loss: 1870.555703, vali_rmse: 0.000000\n",
      "traning iteration: 291 ,loss: 1867.618973, vali_rmse: 0.000000\n",
      "traning iteration: 292 ,loss: 1864.697840, vali_rmse: 0.000000\n",
      "traning iteration: 293 ,loss: 1861.792186, vali_rmse: 0.000000\n",
      "traning iteration: 294 ,loss: 1858.901896, vali_rmse: 0.000000\n",
      "traning iteration: 295 ,loss: 1856.026856, vali_rmse: 0.000000\n",
      "traning iteration: 296 ,loss: 1853.166953, vali_rmse: 0.000000\n",
      "traning iteration: 297 ,loss: 1850.322074, vali_rmse: 0.000000\n",
      "traning iteration: 298 ,loss: 1847.492108, vali_rmse: 0.000000\n",
      "traning iteration: 299 ,loss: 1844.676946, vali_rmse: 0.000000\n",
      "traning iteration: 300 ,loss: 1841.876478, vali_rmse: 0.000000\n",
      "traning iteration: 301 ,loss: 1839.090597, vali_rmse: 0.000000\n",
      "traning iteration: 302 ,loss: 1836.319195, vali_rmse: 0.000000\n",
      "traning iteration: 303 ,loss: 1833.562168, vali_rmse: 0.000000\n",
      "traning iteration: 304 ,loss: 1830.819409, vali_rmse: 0.000000\n",
      "traning iteration: 305 ,loss: 1828.090817, vali_rmse: 0.000000\n",
      "traning iteration: 306 ,loss: 1825.376286, vali_rmse: 0.000000\n",
      "traning iteration: 307 ,loss: 1822.675717, vali_rmse: 0.000000\n",
      "traning iteration: 308 ,loss: 1819.989008, vali_rmse: 0.000000\n",
      "traning iteration: 309 ,loss: 1817.316058, vali_rmse: 0.000000\n",
      "traning iteration: 310 ,loss: 1814.656770, vali_rmse: 0.000000\n",
      "traning iteration: 311 ,loss: 1812.011044, vali_rmse: 0.000000\n",
      "traning iteration: 312 ,loss: 1809.378785, vali_rmse: 0.000000\n",
      "traning iteration: 313 ,loss: 1806.759895, vali_rmse: 0.000000\n",
      "traning iteration: 314 ,loss: 1804.154279, vali_rmse: 0.000000\n",
      "traning iteration: 315 ,loss: 1801.561844, vali_rmse: 0.000000\n",
      "traning iteration: 316 ,loss: 1798.982494, vali_rmse: 0.000000\n",
      "traning iteration: 317 ,loss: 1796.416139, vali_rmse: 0.000000\n",
      "traning iteration: 318 ,loss: 1793.862685, vali_rmse: 0.000000\n",
      "traning iteration: 319 ,loss: 1791.322041, vali_rmse: 0.000000\n",
      "traning iteration: 320 ,loss: 1788.794118, vali_rmse: 0.000000\n",
      "traning iteration: 321 ,loss: 1786.278827, vali_rmse: 0.000000\n",
      "traning iteration: 322 ,loss: 1783.776077, vali_rmse: 0.000000\n",
      "traning iteration: 323 ,loss: 1781.285783, vali_rmse: 0.000000\n",
      "traning iteration: 324 ,loss: 1778.807856, vali_rmse: 0.000000\n",
      "traning iteration: 325 ,loss: 1776.342212, vali_rmse: 0.000000\n",
      "traning iteration: 326 ,loss: 1773.888763, vali_rmse: 0.000000\n",
      "traning iteration: 327 ,loss: 1771.447426, vali_rmse: 0.000000\n",
      "traning iteration: 328 ,loss: 1769.018117, vali_rmse: 0.000000\n",
      "traning iteration: 329 ,loss: 1766.600753, vali_rmse: 0.000000\n",
      "traning iteration: 330 ,loss: 1764.195250, vali_rmse: 0.000000\n",
      "traning iteration: 331 ,loss: 1761.801529, vali_rmse: 0.000000\n",
      "traning iteration: 332 ,loss: 1759.419507, vali_rmse: 0.000000\n",
      "traning iteration: 333 ,loss: 1757.049104, vali_rmse: 0.000000\n",
      "traning iteration: 334 ,loss: 1754.690242, vali_rmse: 0.000000\n",
      "traning iteration: 335 ,loss: 1752.342841, vali_rmse: 0.000000\n",
      "traning iteration: 336 ,loss: 1750.006822, vali_rmse: 0.000000\n",
      "traning iteration: 337 ,loss: 1747.682110, vali_rmse: 0.000000\n",
      "traning iteration: 338 ,loss: 1745.368626, vali_rmse: 0.000000\n",
      "traning iteration: 339 ,loss: 1743.066295, vali_rmse: 0.000000\n",
      "traning iteration: 340 ,loss: 1740.775041, vali_rmse: 0.000000\n",
      "traning iteration: 341 ,loss: 1738.494790, vali_rmse: 0.000000\n",
      "traning iteration: 342 ,loss: 1736.225467, vali_rmse: 0.000000\n",
      "traning iteration: 343 ,loss: 1733.966999, vali_rmse: 0.000000\n",
      "traning iteration: 344 ,loss: 1731.719313, vali_rmse: 0.000000\n",
      "traning iteration: 345 ,loss: 1729.482338, vali_rmse: 0.000000\n",
      "traning iteration: 346 ,loss: 1727.256000, vali_rmse: 0.000000\n",
      "traning iteration: 347 ,loss: 1725.040230, vali_rmse: 0.000000\n",
      "traning iteration: 348 ,loss: 1722.834956, vali_rmse: 0.000000\n",
      "traning iteration: 349 ,loss: 1720.640110, vali_rmse: 0.000000\n",
      "traning iteration: 350 ,loss: 1718.455621, vali_rmse: 0.000000\n",
      "traning iteration: 351 ,loss: 1716.281422, vali_rmse: 0.000000\n",
      "traning iteration: 352 ,loss: 1714.117444, vali_rmse: 0.000000\n",
      "traning iteration: 353 ,loss: 1711.963620, vali_rmse: 0.000000\n",
      "traning iteration: 354 ,loss: 1709.819883, vali_rmse: 0.000000\n",
      "traning iteration: 355 ,loss: 1707.686167, vali_rmse: 0.000000\n",
      "traning iteration: 356 ,loss: 1705.562405, vali_rmse: 0.000000\n",
      "traning iteration: 357 ,loss: 1703.448533, vali_rmse: 0.000000\n",
      "traning iteration: 358 ,loss: 1701.344486, vali_rmse: 0.000000\n",
      "traning iteration: 359 ,loss: 1699.250200, vali_rmse: 0.000000\n",
      "traning iteration: 360 ,loss: 1697.165611, vali_rmse: 0.000000\n",
      "traning iteration: 361 ,loss: 1695.090656, vali_rmse: 0.000000\n",
      "traning iteration: 362 ,loss: 1693.025273, vali_rmse: 0.000000\n",
      "traning iteration: 363 ,loss: 1690.969399, vali_rmse: 0.000000\n",
      "traning iteration: 364 ,loss: 1688.922974, vali_rmse: 0.000000\n",
      "traning iteration: 365 ,loss: 1686.885935, vali_rmse: 0.000000\n",
      "traning iteration: 366 ,loss: 1684.858223, vali_rmse: 0.000000\n",
      "traning iteration: 367 ,loss: 1682.839777, vali_rmse: 0.000000\n",
      "traning iteration: 368 ,loss: 1680.830539, vali_rmse: 0.000000\n",
      "traning iteration: 369 ,loss: 1678.830448, vali_rmse: 0.000000\n",
      "traning iteration: 370 ,loss: 1676.839447, vali_rmse: 0.000000\n",
      "traning iteration: 371 ,loss: 1674.857477, vali_rmse: 0.000000\n",
      "traning iteration: 372 ,loss: 1672.884481, vali_rmse: 0.000000\n",
      "traning iteration: 373 ,loss: 1670.920402, vali_rmse: 0.000000\n",
      "traning iteration: 374 ,loss: 1668.965182, vali_rmse: 0.000000\n",
      "traning iteration: 375 ,loss: 1667.018766, vali_rmse: 0.000000\n",
      "traning iteration: 376 ,loss: 1665.081098, vali_rmse: 0.000000\n",
      "traning iteration: 377 ,loss: 1663.152122, vali_rmse: 0.000000\n",
      "traning iteration: 378 ,loss: 1661.231784, vali_rmse: 0.000000\n",
      "traning iteration: 379 ,loss: 1659.320030, vali_rmse: 0.000000\n",
      "traning iteration: 380 ,loss: 1657.416804, vali_rmse: 0.000000\n",
      "traning iteration: 381 ,loss: 1655.522054, vali_rmse: 0.000000\n",
      "traning iteration: 382 ,loss: 1653.635727, vali_rmse: 0.000000\n",
      "traning iteration: 383 ,loss: 1651.757770, vali_rmse: 0.000000\n",
      "traning iteration: 384 ,loss: 1649.888130, vali_rmse: 0.000000\n",
      "traning iteration: 385 ,loss: 1648.026756, vali_rmse: 0.000000\n",
      "traning iteration: 386 ,loss: 1646.173595, vali_rmse: 0.000000\n",
      "traning iteration: 387 ,loss: 1644.328598, vali_rmse: 0.000000\n",
      "traning iteration: 388 ,loss: 1642.491714, vali_rmse: 0.000000\n",
      "traning iteration: 389 ,loss: 1640.662891, vali_rmse: 0.000000\n",
      "traning iteration: 390 ,loss: 1638.842081, vali_rmse: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning iteration: 391 ,loss: 1637.029233, vali_rmse: 0.000000\n",
      "traning iteration: 392 ,loss: 1635.224299, vali_rmse: 0.000000\n",
      "traning iteration: 393 ,loss: 1633.427230, vali_rmse: 0.000000\n",
      "traning iteration: 394 ,loss: 1631.637977, vali_rmse: 0.000000\n",
      "traning iteration: 395 ,loss: 1629.856493, vali_rmse: 0.000000\n",
      "traning iteration: 396 ,loss: 1628.082731, vali_rmse: 0.000000\n",
      "traning iteration: 397 ,loss: 1626.316642, vali_rmse: 0.000000\n",
      "traning iteration: 398 ,loss: 1624.558179, vali_rmse: 0.000000\n",
      "traning iteration: 399 ,loss: 1622.807298, vali_rmse: 0.000000\n",
      "traning iteration: 400 ,loss: 1621.063950, vali_rmse: 0.000000\n",
      "traning iteration: 401 ,loss: 1619.328092, vali_rmse: 0.000000\n",
      "traning iteration: 402 ,loss: 1617.599676, vali_rmse: 0.000000\n",
      "traning iteration: 403 ,loss: 1615.878658, vali_rmse: 0.000000\n",
      "traning iteration: 404 ,loss: 1614.164994, vali_rmse: 0.000000\n",
      "traning iteration: 405 ,loss: 1612.458638, vali_rmse: 0.000000\n",
      "traning iteration: 406 ,loss: 1610.759548, vali_rmse: 0.000000\n",
      "traning iteration: 407 ,loss: 1609.067678, vali_rmse: 0.000000\n",
      "traning iteration: 408 ,loss: 1607.382987, vali_rmse: 0.000000\n",
      "traning iteration: 409 ,loss: 1605.705430, vali_rmse: 0.000000\n",
      "traning iteration: 410 ,loss: 1604.034964, vali_rmse: 0.000000\n",
      "traning iteration: 411 ,loss: 1602.371549, vali_rmse: 0.000000\n",
      "traning iteration: 412 ,loss: 1600.715141, vali_rmse: 0.000000\n",
      "traning iteration: 413 ,loss: 1599.065698, vali_rmse: 0.000000\n",
      "traning iteration: 414 ,loss: 1597.423180, vali_rmse: 0.000000\n",
      "traning iteration: 415 ,loss: 1595.787545, vali_rmse: 0.000000\n",
      "traning iteration: 416 ,loss: 1594.158752, vali_rmse: 0.000000\n",
      "traning iteration: 417 ,loss: 1592.536761, vali_rmse: 0.000000\n",
      "traning iteration: 418 ,loss: 1590.921532, vali_rmse: 0.000000\n",
      "traning iteration: 419 ,loss: 1589.313024, vali_rmse: 0.000000\n",
      "traning iteration: 420 ,loss: 1587.711198, vali_rmse: 0.000000\n",
      "traning iteration: 421 ,loss: 1586.116014, vali_rmse: 0.000000\n",
      "traning iteration: 422 ,loss: 1584.527434, vali_rmse: 0.000000\n",
      "traning iteration: 423 ,loss: 1582.945419, vali_rmse: 0.000000\n",
      "traning iteration: 424 ,loss: 1581.369931, vali_rmse: 0.000000\n",
      "traning iteration: 425 ,loss: 1579.800930, vali_rmse: 0.000000\n",
      "traning iteration: 426 ,loss: 1578.238380, vali_rmse: 0.000000\n",
      "traning iteration: 427 ,loss: 1576.682243, vali_rmse: 0.000000\n",
      "traning iteration: 428 ,loss: 1575.132480, vali_rmse: 0.000000\n",
      "traning iteration: 429 ,loss: 1573.589057, vali_rmse: 0.000000\n",
      "traning iteration: 430 ,loss: 1572.051934, vali_rmse: 0.000000\n",
      "traning iteration: 431 ,loss: 1570.521077, vali_rmse: 0.000000\n",
      "traning iteration: 432 ,loss: 1568.996448, vali_rmse: 0.000000\n",
      "traning iteration: 433 ,loss: 1567.478012, vali_rmse: 0.000000\n",
      "traning iteration: 434 ,loss: 1565.965733, vali_rmse: 0.000000\n",
      "traning iteration: 435 ,loss: 1564.459576, vali_rmse: 0.000000\n",
      "traning iteration: 436 ,loss: 1562.959505, vali_rmse: 0.000000\n",
      "traning iteration: 437 ,loss: 1561.465485, vali_rmse: 0.000000\n",
      "traning iteration: 438 ,loss: 1559.977482, vali_rmse: 0.000000\n",
      "traning iteration: 439 ,loss: 1558.495461, vali_rmse: 0.000000\n",
      "traning iteration: 440 ,loss: 1557.019388, vali_rmse: 0.000000\n",
      "traning iteration: 441 ,loss: 1555.549230, vali_rmse: 0.000000\n",
      "traning iteration: 442 ,loss: 1554.084951, vali_rmse: 0.000000\n",
      "traning iteration: 443 ,loss: 1552.626520, vali_rmse: 0.000000\n",
      "traning iteration: 444 ,loss: 1551.173902, vali_rmse: 0.000000\n",
      "traning iteration: 445 ,loss: 1549.727065, vali_rmse: 0.000000\n",
      "traning iteration: 446 ,loss: 1548.285975, vali_rmse: 0.000000\n",
      "traning iteration: 447 ,loss: 1546.850601, vali_rmse: 0.000000\n",
      "traning iteration: 448 ,loss: 1545.420911, vali_rmse: 0.000000\n",
      "traning iteration: 449 ,loss: 1543.996871, vali_rmse: 0.000000\n",
      "traning iteration: 450 ,loss: 1542.578451, vali_rmse: 0.000000\n",
      "traning iteration: 451 ,loss: 1541.165618, vali_rmse: 0.000000\n",
      "traning iteration: 452 ,loss: 1539.758341, vali_rmse: 0.000000\n",
      "traning iteration: 453 ,loss: 1538.356590, vali_rmse: 0.000000\n",
      "traning iteration: 454 ,loss: 1536.960333, vali_rmse: 0.000000\n",
      "traning iteration: 455 ,loss: 1535.569539, vali_rmse: 0.000000\n",
      "traning iteration: 456 ,loss: 1534.184179, vali_rmse: 0.000000\n",
      "traning iteration: 457 ,loss: 1532.804221, vali_rmse: 0.000000\n",
      "traning iteration: 458 ,loss: 1531.429636, vali_rmse: 0.000000\n",
      "traning iteration: 459 ,loss: 1530.060393, vali_rmse: 0.000000\n",
      "traning iteration: 460 ,loss: 1528.696464, vali_rmse: 0.000000\n",
      "traning iteration: 461 ,loss: 1527.337818, vali_rmse: 0.000000\n",
      "traning iteration: 462 ,loss: 1525.984427, vali_rmse: 0.000000\n",
      "traning iteration: 463 ,loss: 1524.636261, vali_rmse: 0.000000\n",
      "traning iteration: 464 ,loss: 1523.293292, vali_rmse: 0.000000\n",
      "traning iteration: 465 ,loss: 1521.955491, vali_rmse: 0.000000\n",
      "traning iteration: 466 ,loss: 1520.622830, vali_rmse: 0.000000\n",
      "traning iteration: 467 ,loss: 1519.295280, vali_rmse: 0.000000\n",
      "traning iteration: 468 ,loss: 1517.972813, vali_rmse: 0.000000\n",
      "traning iteration: 469 ,loss: 1516.655401, vali_rmse: 0.000000\n",
      "traning iteration: 470 ,loss: 1515.343018, vali_rmse: 0.000000\n",
      "traning iteration: 471 ,loss: 1514.035635, vali_rmse: 0.000000\n",
      "traning iteration: 472 ,loss: 1512.733225, vali_rmse: 0.000000\n",
      "traning iteration: 473 ,loss: 1511.435761, vali_rmse: 0.000000\n",
      "traning iteration: 474 ,loss: 1510.143217, vali_rmse: 0.000000\n",
      "traning iteration: 475 ,loss: 1508.855564, vali_rmse: 0.000000\n",
      "traning iteration: 476 ,loss: 1507.572778, vali_rmse: 0.000000\n",
      "traning iteration: 477 ,loss: 1506.294831, vali_rmse: 0.000000\n",
      "traning iteration: 478 ,loss: 1505.021698, vali_rmse: 0.000000\n",
      "traning iteration: 479 ,loss: 1503.753353, vali_rmse: 0.000000\n",
      "traning iteration: 480 ,loss: 1502.489768, vali_rmse: 0.000000\n",
      "traning iteration: 481 ,loss: 1501.230921, vali_rmse: 0.000000\n",
      "traning iteration: 482 ,loss: 1499.976783, vali_rmse: 0.000000\n",
      "traning iteration: 483 ,loss: 1498.727331, vali_rmse: 0.000000\n",
      "traning iteration: 484 ,loss: 1497.482539, vali_rmse: 0.000000\n",
      "traning iteration: 485 ,loss: 1496.242383, vali_rmse: 0.000000\n",
      "traning iteration: 486 ,loss: 1495.006837, vali_rmse: 0.000000\n",
      "traning iteration: 487 ,loss: 1493.775877, vali_rmse: 0.000000\n",
      "traning iteration: 488 ,loss: 1492.549478, vali_rmse: 0.000000\n",
      "traning iteration: 489 ,loss: 1491.327617, vali_rmse: 0.000000\n",
      "traning iteration: 490 ,loss: 1490.110268, vali_rmse: 0.000000\n",
      "traning iteration: 491 ,loss: 1488.897409, vali_rmse: 0.000000\n",
      "traning iteration: 492 ,loss: 1487.689016, vali_rmse: 0.000000\n",
      "traning iteration: 493 ,loss: 1486.485064, vali_rmse: 0.000000\n",
      "traning iteration: 494 ,loss: 1485.285530, vali_rmse: 0.000000\n",
      "traning iteration: 495 ,loss: 1484.090391, vali_rmse: 0.000000\n",
      "traning iteration: 496 ,loss: 1482.899625, vali_rmse: 0.000000\n",
      "traning iteration: 497 ,loss: 1481.713207, vali_rmse: 0.000000\n",
      "traning iteration: 498 ,loss: 1480.531115, vali_rmse: 0.000000\n",
      "traning iteration: 499 ,loss: 1479.353326, vali_rmse: 0.000000\n",
      "traning iteration: 500 ,loss: 1478.179819, vali_rmse: 0.000000\n",
      "traning iteration: 501 ,loss: 1477.010569, vali_rmse: 0.000000\n",
      "traning iteration: 502 ,loss: 1475.845556, vali_rmse: 0.000000\n",
      "traning iteration: 503 ,loss: 1474.684757, vali_rmse: 0.000000\n",
      "traning iteration: 504 ,loss: 1473.528150, vali_rmse: 0.000000\n",
      "traning iteration: 505 ,loss: 1472.375713, vali_rmse: 0.000000\n",
      "traning iteration: 506 ,loss: 1471.227425, vali_rmse: 0.000000\n",
      "traning iteration: 507 ,loss: 1470.083264, vali_rmse: 0.000000\n",
      "traning iteration: 508 ,loss: 1468.943209, vali_rmse: 0.000000\n",
      "traning iteration: 509 ,loss: 1467.807238, vali_rmse: 0.000000\n",
      "traning iteration: 510 ,loss: 1466.675331, vali_rmse: 0.000000\n",
      "traning iteration: 511 ,loss: 1465.547467, vali_rmse: 0.000000\n",
      "traning iteration: 512 ,loss: 1464.423623, vali_rmse: 0.000000\n",
      "traning iteration: 513 ,loss: 1463.303781, vali_rmse: 0.000000\n",
      "traning iteration: 514 ,loss: 1462.187920, vali_rmse: 0.000000\n",
      "traning iteration: 515 ,loss: 1461.076018, vali_rmse: 0.000000\n",
      "traning iteration: 516 ,loss: 1459.968056, vali_rmse: 0.000000\n",
      "traning iteration: 517 ,loss: 1458.864014, vali_rmse: 0.000000\n",
      "traning iteration: 518 ,loss: 1457.763871, vali_rmse: 0.000000\n",
      "traning iteration: 519 ,loss: 1456.667608, vali_rmse: 0.000000\n",
      "traning iteration: 520 ,loss: 1455.575205, vali_rmse: 0.000000\n",
      "traning iteration: 521 ,loss: 1454.486642, vali_rmse: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning iteration: 522 ,loss: 1453.401900, vali_rmse: 0.000000\n",
      "traning iteration: 523 ,loss: 1452.320960, vali_rmse: 0.000000\n",
      "traning iteration: 524 ,loss: 1451.243801, vali_rmse: 0.000000\n",
      "traning iteration: 525 ,loss: 1450.170406, vali_rmse: 0.000000\n",
      "traning iteration: 526 ,loss: 1449.100755, vali_rmse: 0.000000\n",
      "traning iteration: 527 ,loss: 1448.034828, vali_rmse: 0.000000\n",
      "traning iteration: 528 ,loss: 1446.972608, vali_rmse: 0.000000\n",
      "traning iteration: 529 ,loss: 1445.914076, vali_rmse: 0.000000\n",
      "traning iteration: 530 ,loss: 1444.859213, vali_rmse: 0.000000\n",
      "traning iteration: 531 ,loss: 1443.808001, vali_rmse: 0.000000\n",
      "traning iteration: 532 ,loss: 1442.760421, vali_rmse: 0.000000\n",
      "traning iteration: 533 ,loss: 1441.716456, vali_rmse: 0.000000\n",
      "traning iteration: 534 ,loss: 1440.676087, vali_rmse: 0.000000\n",
      "traning iteration: 535 ,loss: 1439.639296, vali_rmse: 0.000000\n",
      "traning iteration: 536 ,loss: 1438.606066, vali_rmse: 0.000000\n",
      "traning iteration: 537 ,loss: 1437.576379, vali_rmse: 0.000000\n",
      "traning iteration: 538 ,loss: 1436.550217, vali_rmse: 0.000000\n",
      "traning iteration: 539 ,loss: 1435.527562, vali_rmse: 0.000000\n",
      "traning iteration: 540 ,loss: 1434.508398, vali_rmse: 0.000000\n",
      "traning iteration: 541 ,loss: 1433.492707, vali_rmse: 0.000000\n",
      "traning iteration: 542 ,loss: 1432.480472, vali_rmse: 0.000000\n",
      "traning iteration: 543 ,loss: 1431.471675, vali_rmse: 0.000000\n",
      "traning iteration: 544 ,loss: 1430.466301, vali_rmse: 0.000000\n",
      "traning iteration: 545 ,loss: 1429.464331, vali_rmse: 0.000000\n",
      "traning iteration: 546 ,loss: 1428.465750, vali_rmse: 0.000000\n",
      "traning iteration: 547 ,loss: 1427.470541, vali_rmse: 0.000000\n",
      "traning iteration: 548 ,loss: 1426.478687, vali_rmse: 0.000000\n",
      "traning iteration: 549 ,loss: 1425.490171, vali_rmse: 0.000000\n",
      "traning iteration: 550 ,loss: 1424.504978, vali_rmse: 0.000000\n",
      "traning iteration: 551 ,loss: 1423.523091, vali_rmse: 0.000000\n",
      "traning iteration: 552 ,loss: 1422.544495, vali_rmse: 0.000000\n",
      "traning iteration: 553 ,loss: 1421.569172, vali_rmse: 0.000000\n",
      "traning iteration: 554 ,loss: 1420.597107, vali_rmse: 0.000000\n",
      "traning iteration: 555 ,loss: 1419.628285, vali_rmse: 0.000000\n",
      "traning iteration: 556 ,loss: 1418.662690, vali_rmse: 0.000000\n",
      "traning iteration: 557 ,loss: 1417.700305, vali_rmse: 0.000000\n",
      "traning iteration: 558 ,loss: 1416.741116, vali_rmse: 0.000000\n",
      "traning iteration: 559 ,loss: 1415.785107, vali_rmse: 0.000000\n",
      "traning iteration: 560 ,loss: 1414.832263, vali_rmse: 0.000000\n",
      "traning iteration: 561 ,loss: 1413.882568, vali_rmse: 0.000000\n",
      "traning iteration: 562 ,loss: 1412.936007, vali_rmse: 0.000000\n",
      "traning iteration: 563 ,loss: 1411.992566, vali_rmse: 0.000000\n",
      "traning iteration: 564 ,loss: 1411.052229, vali_rmse: 0.000000\n",
      "traning iteration: 565 ,loss: 1410.114981, vali_rmse: 0.000000\n",
      "traning iteration: 566 ,loss: 1409.180808, vali_rmse: 0.000000\n",
      "traning iteration: 567 ,loss: 1408.249695, vali_rmse: 0.000000\n",
      "traning iteration: 568 ,loss: 1407.321627, vali_rmse: 0.000000\n",
      "traning iteration: 569 ,loss: 1406.396590, vali_rmse: 0.000000\n",
      "traning iteration: 570 ,loss: 1405.474570, vali_rmse: 0.000000\n",
      "traning iteration: 571 ,loss: 1404.555552, vali_rmse: 0.000000\n",
      "traning iteration: 572 ,loss: 1403.639521, vali_rmse: 0.000000\n",
      "traning iteration: 573 ,loss: 1402.726464, vali_rmse: 0.000000\n",
      "traning iteration: 574 ,loss: 1401.816367, vali_rmse: 0.000000\n",
      "traning iteration: 575 ,loss: 1400.909215, vali_rmse: 0.000000\n",
      "traning iteration: 576 ,loss: 1400.004996, vali_rmse: 0.000000\n",
      "traning iteration: 577 ,loss: 1399.103693, vali_rmse: 0.000000\n",
      "traning iteration: 578 ,loss: 1398.205296, vali_rmse: 0.000000\n",
      "traning iteration: 579 ,loss: 1397.309788, vali_rmse: 0.000000\n",
      "traning iteration: 580 ,loss: 1396.417158, vali_rmse: 0.000000\n",
      "traning iteration: 581 ,loss: 1395.527391, vali_rmse: 0.000000\n",
      "traning iteration: 582 ,loss: 1394.640474, vali_rmse: 0.000000\n",
      "traning iteration: 583 ,loss: 1393.756393, vali_rmse: 0.000000\n",
      "traning iteration: 584 ,loss: 1392.875136, vali_rmse: 0.000000\n",
      "traning iteration: 585 ,loss: 1391.996690, vali_rmse: 0.000000\n",
      "traning iteration: 586 ,loss: 1391.121040, vali_rmse: 0.000000\n",
      "traning iteration: 587 ,loss: 1390.248175, vali_rmse: 0.000000\n",
      "traning iteration: 588 ,loss: 1389.378081, vali_rmse: 0.000000\n",
      "traning iteration: 589 ,loss: 1388.510745, vali_rmse: 0.000000\n",
      "traning iteration: 590 ,loss: 1387.646155, vali_rmse: 0.000000\n",
      "traning iteration: 591 ,loss: 1386.784298, vali_rmse: 0.000000\n",
      "traning iteration: 592 ,loss: 1385.925160, vali_rmse: 0.000000\n",
      "traning iteration: 593 ,loss: 1385.068731, vali_rmse: 0.000000\n",
      "traning iteration: 594 ,loss: 1384.214997, vali_rmse: 0.000000\n",
      "traning iteration: 595 ,loss: 1383.363945, vali_rmse: 0.000000\n",
      "traning iteration: 596 ,loss: 1382.515564, vali_rmse: 0.000000\n",
      "traning iteration: 597 ,loss: 1381.669842, vali_rmse: 0.000000\n",
      "traning iteration: 598 ,loss: 1380.826765, vali_rmse: 0.000000\n",
      "traning iteration: 599 ,loss: 1379.986322, vali_rmse: 0.000000\n",
      "traning iteration: 600 ,loss: 1379.148501, vali_rmse: 0.000000\n",
      "traning iteration: 601 ,loss: 1378.313289, vali_rmse: 0.000000\n",
      "traning iteration: 602 ,loss: 1377.480676, vali_rmse: 0.000000\n",
      "traning iteration: 603 ,loss: 1376.650649, vali_rmse: 0.000000\n",
      "traning iteration: 604 ,loss: 1375.823196, vali_rmse: 0.000000\n",
      "traning iteration: 605 ,loss: 1374.998306, vali_rmse: 0.000000\n",
      "traning iteration: 606 ,loss: 1374.175966, vali_rmse: 0.000000\n",
      "traning iteration: 607 ,loss: 1373.356166, vali_rmse: 0.000000\n",
      "traning iteration: 608 ,loss: 1372.538894, vali_rmse: 0.000000\n",
      "traning iteration: 609 ,loss: 1371.724139, vali_rmse: 0.000000\n",
      "traning iteration: 610 ,loss: 1370.911889, vali_rmse: 0.000000\n",
      "traning iteration: 611 ,loss: 1370.102132, vali_rmse: 0.000000\n",
      "traning iteration: 612 ,loss: 1369.294859, vali_rmse: 0.000000\n",
      "traning iteration: 613 ,loss: 1368.490056, vali_rmse: 0.000000\n",
      "traning iteration: 614 ,loss: 1367.687714, vali_rmse: 0.000000\n",
      "traning iteration: 615 ,loss: 1366.887821, vali_rmse: 0.000000\n",
      "traning iteration: 616 ,loss: 1366.090367, vali_rmse: 0.000000\n",
      "traning iteration: 617 ,loss: 1365.295340, vali_rmse: 0.000000\n",
      "traning iteration: 618 ,loss: 1364.502730, vali_rmse: 0.000000\n",
      "traning iteration: 619 ,loss: 1363.712525, vali_rmse: 0.000000\n",
      "traning iteration: 620 ,loss: 1362.924716, vali_rmse: 0.000000\n",
      "traning iteration: 621 ,loss: 1362.139290, vali_rmse: 0.000000\n",
      "traning iteration: 622 ,loss: 1361.356239, vali_rmse: 0.000000\n",
      "traning iteration: 623 ,loss: 1360.575551, vali_rmse: 0.000000\n",
      "traning iteration: 624 ,loss: 1359.797215, vali_rmse: 0.000000\n",
      "traning iteration: 625 ,loss: 1359.021222, vali_rmse: 0.000000\n",
      "traning iteration: 626 ,loss: 1358.247561, vali_rmse: 0.000000\n",
      "traning iteration: 627 ,loss: 1357.476222, vali_rmse: 0.000000\n",
      "traning iteration: 628 ,loss: 1356.707194, vali_rmse: 0.000000\n",
      "traning iteration: 629 ,loss: 1355.940467, vali_rmse: 0.000000\n",
      "traning iteration: 630 ,loss: 1355.176032, vali_rmse: 0.000000\n",
      "traning iteration: 631 ,loss: 1354.413877, vali_rmse: 0.000000\n",
      "traning iteration: 632 ,loss: 1353.653994, vali_rmse: 0.000000\n",
      "traning iteration: 633 ,loss: 1352.896371, vali_rmse: 0.000000\n",
      "traning iteration: 634 ,loss: 1352.141000, vali_rmse: 0.000000\n",
      "traning iteration: 635 ,loss: 1351.387870, vali_rmse: 0.000000\n",
      "traning iteration: 636 ,loss: 1350.636972, vali_rmse: 0.000000\n",
      "traning iteration: 637 ,loss: 1349.888295, vali_rmse: 0.000000\n",
      "traning iteration: 638 ,loss: 1349.141831, vali_rmse: 0.000000\n",
      "traning iteration: 639 ,loss: 1348.397569, vali_rmse: 0.000000\n",
      "traning iteration: 640 ,loss: 1347.655500, vali_rmse: 0.000000\n",
      "traning iteration: 641 ,loss: 1346.915614, vali_rmse: 0.000000\n",
      "traning iteration: 642 ,loss: 1346.177902, vali_rmse: 0.000000\n",
      "traning iteration: 643 ,loss: 1345.442355, vali_rmse: 0.000000\n",
      "traning iteration: 644 ,loss: 1344.708963, vali_rmse: 0.000000\n",
      "traning iteration: 645 ,loss: 1343.977717, vali_rmse: 0.000000\n",
      "traning iteration: 646 ,loss: 1343.248607, vali_rmse: 0.000000\n",
      "traning iteration: 647 ,loss: 1342.521625, vali_rmse: 0.000000\n",
      "traning iteration: 648 ,loss: 1341.796761, vali_rmse: 0.000000\n",
      "traning iteration: 649 ,loss: 1341.074006, vali_rmse: 0.000000\n",
      "traning iteration: 650 ,loss: 1340.353351, vali_rmse: 0.000000\n",
      "traning iteration: 651 ,loss: 1339.634787, vali_rmse: 0.000000\n",
      "traning iteration: 652 ,loss: 1338.918305, vali_rmse: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning iteration: 653 ,loss: 1338.203897, vali_rmse: 0.000000\n",
      "traning iteration: 654 ,loss: 1337.491553, vali_rmse: 0.000000\n",
      "traning iteration: 655 ,loss: 1336.781264, vali_rmse: 0.000000\n",
      "traning iteration: 656 ,loss: 1336.073023, vali_rmse: 0.000000\n",
      "traning iteration: 657 ,loss: 1335.366819, vali_rmse: 0.000000\n",
      "traning iteration: 658 ,loss: 1334.662645, vali_rmse: 0.000000\n",
      "traning iteration: 659 ,loss: 1333.960491, vali_rmse: 0.000000\n",
      "traning iteration: 660 ,loss: 1333.260350, vali_rmse: 0.000000\n",
      "traning iteration: 661 ,loss: 1332.562213, vali_rmse: 0.000000\n",
      "traning iteration: 662 ,loss: 1331.866071, vali_rmse: 0.000000\n",
      "traning iteration: 663 ,loss: 1331.171915, vali_rmse: 0.000000\n",
      "traning iteration: 664 ,loss: 1330.479738, vali_rmse: 0.000000\n",
      "traning iteration: 665 ,loss: 1329.789532, vali_rmse: 0.000000\n",
      "traning iteration: 666 ,loss: 1329.101287, vali_rmse: 0.000000\n",
      "traning iteration: 667 ,loss: 1328.414995, vali_rmse: 0.000000\n",
      "traning iteration: 668 ,loss: 1327.730649, vali_rmse: 0.000000\n",
      "traning iteration: 669 ,loss: 1327.048240, vali_rmse: 0.000000\n",
      "traning iteration: 670 ,loss: 1326.367760, vali_rmse: 0.000000\n",
      "traning iteration: 671 ,loss: 1325.689202, vali_rmse: 0.000000\n",
      "traning iteration: 672 ,loss: 1325.012556, vali_rmse: 0.000000\n",
      "traning iteration: 673 ,loss: 1324.337815, vali_rmse: 0.000000\n",
      "traning iteration: 674 ,loss: 1323.664972, vali_rmse: 0.000000\n",
      "traning iteration: 675 ,loss: 1322.994017, vali_rmse: 0.000000\n",
      "traning iteration: 676 ,loss: 1322.324944, vali_rmse: 0.000000\n",
      "traning iteration: 677 ,loss: 1321.657745, vali_rmse: 0.000000\n",
      "traning iteration: 678 ,loss: 1320.992411, vali_rmse: 0.000000\n",
      "traning iteration: 679 ,loss: 1320.328935, vali_rmse: 0.000000\n",
      "traning iteration: 680 ,loss: 1319.667310, vali_rmse: 0.000000\n",
      "traning iteration: 681 ,loss: 1319.007527, vali_rmse: 0.000000\n",
      "traning iteration: 682 ,loss: 1318.349580, vali_rmse: 0.000000\n",
      "traning iteration: 683 ,loss: 1317.693459, vali_rmse: 0.000000\n",
      "traning iteration: 684 ,loss: 1317.039159, vali_rmse: 0.000000\n",
      "traning iteration: 685 ,loss: 1316.386672, vali_rmse: 0.000000\n",
      "traning iteration: 686 ,loss: 1315.735989, vali_rmse: 0.000000\n",
      "traning iteration: 687 ,loss: 1315.087105, vali_rmse: 0.000000\n",
      "traning iteration: 688 ,loss: 1314.440010, vali_rmse: 0.000000\n",
      "traning iteration: 689 ,loss: 1313.794699, vali_rmse: 0.000000\n",
      "traning iteration: 690 ,loss: 1313.151163, vali_rmse: 0.000000\n",
      "traning iteration: 691 ,loss: 1312.509396, vali_rmse: 0.000000\n",
      "traning iteration: 692 ,loss: 1311.869390, vali_rmse: 0.000000\n",
      "traning iteration: 693 ,loss: 1311.231138, vali_rmse: 0.000000\n",
      "traning iteration: 694 ,loss: 1310.594633, vali_rmse: 0.000000\n",
      "traning iteration: 695 ,loss: 1309.959867, vali_rmse: 0.000000\n",
      "traning iteration: 696 ,loss: 1309.326835, vali_rmse: 0.000000\n",
      "traning iteration: 697 ,loss: 1308.695528, vali_rmse: 0.000000\n",
      "traning iteration: 698 ,loss: 1308.065940, vali_rmse: 0.000000\n",
      "traning iteration: 699 ,loss: 1307.438064, vali_rmse: 0.000000\n",
      "traning iteration: 700 ,loss: 1306.811893, vali_rmse: 0.000000\n",
      "traning iteration: 701 ,loss: 1306.187420, vali_rmse: 0.000000\n",
      "traning iteration: 702 ,loss: 1305.564638, vali_rmse: 0.000000\n",
      "traning iteration: 703 ,loss: 1304.943540, vali_rmse: 0.000000\n",
      "traning iteration: 704 ,loss: 1304.324121, vali_rmse: 0.000000\n",
      "traning iteration: 705 ,loss: 1303.706371, vali_rmse: 0.000000\n",
      "traning iteration: 706 ,loss: 1303.090287, vali_rmse: 0.000000\n",
      "traning iteration: 707 ,loss: 1302.475859, vali_rmse: 0.000000\n",
      "traning iteration: 708 ,loss: 1301.863083, vali_rmse: 0.000000\n",
      "traning iteration: 709 ,loss: 1301.251951, vali_rmse: 0.000000\n",
      "traning iteration: 710 ,loss: 1300.642456, vali_rmse: 0.000000\n",
      "traning iteration: 711 ,loss: 1300.034593, vali_rmse: 0.000000\n",
      "traning iteration: 712 ,loss: 1299.428354, vali_rmse: 0.000000\n",
      "traning iteration: 713 ,loss: 1298.823734, vali_rmse: 0.000000\n",
      "traning iteration: 714 ,loss: 1298.220726, vali_rmse: 0.000000\n",
      "traning iteration: 715 ,loss: 1297.619323, vali_rmse: 0.000000\n",
      "traning iteration: 716 ,loss: 1297.019519, vali_rmse: 0.000000\n",
      "traning iteration: 717 ,loss: 1296.421307, vali_rmse: 0.000000\n",
      "traning iteration: 718 ,loss: 1295.824682, vali_rmse: 0.000000\n",
      "traning iteration: 719 ,loss: 1295.229638, vali_rmse: 0.000000\n",
      "traning iteration: 720 ,loss: 1294.636167, vali_rmse: 0.000000\n",
      "traning iteration: 721 ,loss: 1294.044264, vali_rmse: 0.000000\n",
      "traning iteration: 722 ,loss: 1293.453923, vali_rmse: 0.000000\n",
      "traning iteration: 723 ,loss: 1292.865137, vali_rmse: 0.000000\n",
      "traning iteration: 724 ,loss: 1292.277900, vali_rmse: 0.000000\n",
      "traning iteration: 725 ,loss: 1291.692207, vali_rmse: 0.000000\n",
      "traning iteration: 726 ,loss: 1291.108051, vali_rmse: 0.000000\n",
      "traning iteration: 727 ,loss: 1290.525426, vali_rmse: 0.000000\n",
      "traning iteration: 728 ,loss: 1289.944326, vali_rmse: 0.000000\n",
      "traning iteration: 729 ,loss: 1289.364746, vali_rmse: 0.000000\n",
      "traning iteration: 730 ,loss: 1288.786679, vali_rmse: 0.000000\n",
      "traning iteration: 731 ,loss: 1288.210119, vali_rmse: 0.000000\n",
      "traning iteration: 732 ,loss: 1287.635061, vali_rmse: 0.000000\n",
      "traning iteration: 733 ,loss: 1287.061499, vali_rmse: 0.000000\n",
      "traning iteration: 734 ,loss: 1286.489426, vali_rmse: 0.000000\n",
      "traning iteration: 735 ,loss: 1285.918838, vali_rmse: 0.000000\n",
      "traning iteration: 736 ,loss: 1285.349728, vali_rmse: 0.000000\n",
      "traning iteration: 737 ,loss: 1284.782091, vali_rmse: 0.000000\n",
      "traning iteration: 738 ,loss: 1284.215921, vali_rmse: 0.000000\n",
      "traning iteration: 739 ,loss: 1283.651213, vali_rmse: 0.000000\n",
      "traning iteration: 740 ,loss: 1283.087960, vali_rmse: 0.000000\n",
      "traning iteration: 741 ,loss: 1282.526157, vali_rmse: 0.000000\n",
      "traning iteration: 742 ,loss: 1281.965799, vali_rmse: 0.000000\n",
      "traning iteration: 743 ,loss: 1281.406880, vali_rmse: 0.000000\n",
      "traning iteration: 744 ,loss: 1280.849394, vali_rmse: 0.000000\n",
      "traning iteration: 745 ,loss: 1280.293336, vali_rmse: 0.000000\n",
      "traning iteration: 746 ,loss: 1279.738701, vali_rmse: 0.000000\n",
      "traning iteration: 747 ,loss: 1279.185483, vali_rmse: 0.000000\n",
      "traning iteration: 748 ,loss: 1278.633677, vali_rmse: 0.000000\n",
      "traning iteration: 749 ,loss: 1278.083276, vali_rmse: 0.000000\n",
      "traning iteration: 750 ,loss: 1277.534277, vali_rmse: 0.000000\n",
      "traning iteration: 751 ,loss: 1276.986674, vali_rmse: 0.000000\n",
      "traning iteration: 752 ,loss: 1276.440460, vali_rmse: 0.000000\n",
      "traning iteration: 753 ,loss: 1275.895632, vali_rmse: 0.000000\n",
      "traning iteration: 754 ,loss: 1275.352184, vali_rmse: 0.000000\n",
      "traning iteration: 755 ,loss: 1274.810110, vali_rmse: 0.000000\n",
      "traning iteration: 756 ,loss: 1274.269405, vali_rmse: 0.000000\n",
      "traning iteration: 757 ,loss: 1273.730065, vali_rmse: 0.000000\n",
      "traning iteration: 758 ,loss: 1273.192083, vali_rmse: 0.000000\n",
      "traning iteration: 759 ,loss: 1272.655456, vali_rmse: 0.000000\n",
      "traning iteration: 760 ,loss: 1272.120177, vali_rmse: 0.000000\n",
      "traning iteration: 761 ,loss: 1271.586242, vali_rmse: 0.000000\n",
      "traning iteration: 762 ,loss: 1271.053646, vali_rmse: 0.000000\n",
      "traning iteration: 763 ,loss: 1270.522383, vali_rmse: 0.000000\n",
      "traning iteration: 764 ,loss: 1269.992449, vali_rmse: 0.000000\n",
      "traning iteration: 765 ,loss: 1269.463839, vali_rmse: 0.000000\n",
      "traning iteration: 766 ,loss: 1268.936547, vali_rmse: 0.000000\n",
      "traning iteration: 767 ,loss: 1268.410569, vali_rmse: 0.000000\n",
      "traning iteration: 768 ,loss: 1267.885901, vali_rmse: 0.000000\n",
      "traning iteration: 769 ,loss: 1267.362536, vali_rmse: 0.000000\n",
      "traning iteration: 770 ,loss: 1266.840470, vali_rmse: 0.000000\n",
      "traning iteration: 771 ,loss: 1266.319698, vali_rmse: 0.000000\n",
      "traning iteration: 772 ,loss: 1265.800216, vali_rmse: 0.000000\n",
      "traning iteration: 773 ,loss: 1265.282019, vali_rmse: 0.000000\n",
      "traning iteration: 774 ,loss: 1264.765102, vali_rmse: 0.000000\n",
      "traning iteration: 775 ,loss: 1264.249459, vali_rmse: 0.000000\n",
      "traning iteration: 776 ,loss: 1263.735088, vali_rmse: 0.000000\n",
      "traning iteration: 777 ,loss: 1263.221982, vali_rmse: 0.000000\n",
      "traning iteration: 778 ,loss: 1262.710137, vali_rmse: 0.000000\n",
      "traning iteration: 779 ,loss: 1262.199549, vali_rmse: 0.000000\n",
      "traning iteration: 780 ,loss: 1261.690213, vali_rmse: 0.000000\n",
      "traning iteration: 781 ,loss: 1261.182123, vali_rmse: 0.000000\n",
      "traning iteration: 782 ,loss: 1260.675277, vali_rmse: 0.000000\n",
      "traning iteration: 783 ,loss: 1260.169668, vali_rmse: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning iteration: 784 ,loss: 1259.665294, vali_rmse: 0.000000\n",
      "traning iteration: 785 ,loss: 1259.162148, vali_rmse: 0.000000\n",
      "traning iteration: 786 ,loss: 1258.660226, vali_rmse: 0.000000\n",
      "traning iteration: 787 ,loss: 1258.159525, vali_rmse: 0.000000\n",
      "traning iteration: 788 ,loss: 1257.660039, vali_rmse: 0.000000\n",
      "traning iteration: 789 ,loss: 1257.161765, vali_rmse: 0.000000\n",
      "traning iteration: 790 ,loss: 1256.664697, vali_rmse: 0.000000\n",
      "traning iteration: 791 ,loss: 1256.168832, vali_rmse: 0.000000\n",
      "traning iteration: 792 ,loss: 1255.674164, vali_rmse: 0.000000\n",
      "traning iteration: 793 ,loss: 1255.180690, vali_rmse: 0.000000\n",
      "traning iteration: 794 ,loss: 1254.688406, vali_rmse: 0.000000\n",
      "traning iteration: 795 ,loss: 1254.197306, vali_rmse: 0.000000\n",
      "traning iteration: 796 ,loss: 1253.707388, vali_rmse: 0.000000\n",
      "traning iteration: 797 ,loss: 1253.218645, vali_rmse: 0.000000\n",
      "traning iteration: 798 ,loss: 1252.731075, vali_rmse: 0.000000\n",
      "traning iteration: 799 ,loss: 1252.244673, vali_rmse: 0.000000\n",
      "traning iteration: 800 ,loss: 1251.759435, vali_rmse: 0.000000\n",
      "traning iteration: 801 ,loss: 1251.275356, vali_rmse: 0.000000\n",
      "traning iteration: 802 ,loss: 1250.792432, vali_rmse: 0.000000\n",
      "traning iteration: 803 ,loss: 1250.310660, vali_rmse: 0.000000\n",
      "traning iteration: 804 ,loss: 1249.830035, vali_rmse: 0.000000\n",
      "traning iteration: 805 ,loss: 1249.350553, vali_rmse: 0.000000\n",
      "traning iteration: 806 ,loss: 1248.872210, vali_rmse: 0.000000\n",
      "traning iteration: 807 ,loss: 1248.395001, vali_rmse: 0.000000\n",
      "traning iteration: 808 ,loss: 1247.918923, vali_rmse: 0.000000\n",
      "traning iteration: 809 ,loss: 1247.443973, vali_rmse: 0.000000\n",
      "traning iteration: 810 ,loss: 1246.970144, vali_rmse: 0.000000\n",
      "traning iteration: 811 ,loss: 1246.497435, vali_rmse: 0.000000\n",
      "traning iteration: 812 ,loss: 1246.025840, vali_rmse: 0.000000\n",
      "traning iteration: 813 ,loss: 1245.555356, vali_rmse: 0.000000\n",
      "traning iteration: 814 ,loss: 1245.085979, vali_rmse: 0.000000\n",
      "traning iteration: 815 ,loss: 1244.617705, vali_rmse: 0.000000\n",
      "traning iteration: 816 ,loss: 1244.150530, vali_rmse: 0.000000\n",
      "traning iteration: 817 ,loss: 1243.684450, vali_rmse: 0.000000\n",
      "traning iteration: 818 ,loss: 1243.219461, vali_rmse: 0.000000\n",
      "traning iteration: 819 ,loss: 1242.755560, vali_rmse: 0.000000\n",
      "traning iteration: 820 ,loss: 1242.292742, vali_rmse: 0.000000\n",
      "traning iteration: 821 ,loss: 1241.831003, vali_rmse: 0.000000\n",
      "traning iteration: 822 ,loss: 1241.370341, vali_rmse: 0.000000\n",
      "traning iteration: 823 ,loss: 1240.910751, vali_rmse: 0.000000\n",
      "traning iteration: 824 ,loss: 1240.452229, vali_rmse: 0.000000\n",
      "traning iteration: 825 ,loss: 1239.994772, vali_rmse: 0.000000\n",
      "traning iteration: 826 ,loss: 1239.538376, vali_rmse: 0.000000\n",
      "traning iteration: 827 ,loss: 1239.083037, vali_rmse: 0.000000\n",
      "traning iteration: 828 ,loss: 1238.628752, vali_rmse: 0.000000\n",
      "traning iteration: 829 ,loss: 1238.175516, vali_rmse: 0.000000\n",
      "traning iteration: 830 ,loss: 1237.723326, vali_rmse: 0.000000\n",
      "traning iteration: 831 ,loss: 1237.272179, vali_rmse: 0.000000\n",
      "traning iteration: 832 ,loss: 1236.822071, vali_rmse: 0.000000\n",
      "traning iteration: 833 ,loss: 1236.372998, vali_rmse: 0.000000\n",
      "traning iteration: 834 ,loss: 1235.924957, vali_rmse: 0.000000\n",
      "traning iteration: 835 ,loss: 1235.477943, vali_rmse: 0.000000\n",
      "traning iteration: 836 ,loss: 1235.031954, vali_rmse: 0.000000\n",
      "traning iteration: 837 ,loss: 1234.586986, vali_rmse: 0.000000\n",
      "traning iteration: 838 ,loss: 1234.143036, vali_rmse: 0.000000\n",
      "traning iteration: 839 ,loss: 1233.700099, vali_rmse: 0.000000\n",
      "traning iteration: 840 ,loss: 1233.258173, vali_rmse: 0.000000\n",
      "traning iteration: 841 ,loss: 1232.817254, vali_rmse: 0.000000\n",
      "traning iteration: 842 ,loss: 1232.377338, vali_rmse: 0.000000\n",
      "traning iteration: 843 ,loss: 1231.938422, vali_rmse: 0.000000\n",
      "traning iteration: 844 ,loss: 1231.500503, vali_rmse: 0.000000\n",
      "traning iteration: 845 ,loss: 1231.063576, vali_rmse: 0.000000\n",
      "traning iteration: 846 ,loss: 1230.627640, vali_rmse: 0.000000\n",
      "traning iteration: 847 ,loss: 1230.192690, vali_rmse: 0.000000\n",
      "traning iteration: 848 ,loss: 1229.758722, vali_rmse: 0.000000\n",
      "traning iteration: 849 ,loss: 1229.325734, vali_rmse: 0.000000\n",
      "traning iteration: 850 ,loss: 1228.893723, vali_rmse: 0.000000\n",
      "traning iteration: 851 ,loss: 1228.462684, vali_rmse: 0.000000\n",
      "traning iteration: 852 ,loss: 1228.032615, vali_rmse: 0.000000\n",
      "traning iteration: 853 ,loss: 1227.603512, vali_rmse: 0.000000\n",
      "traning iteration: 854 ,loss: 1227.175372, vali_rmse: 0.000000\n",
      "traning iteration: 855 ,loss: 1226.748191, vali_rmse: 0.000000\n",
      "traning iteration: 856 ,loss: 1226.321967, vali_rmse: 0.000000\n",
      "traning iteration: 857 ,loss: 1225.896697, vali_rmse: 0.000000\n",
      "traning iteration: 858 ,loss: 1225.472376, vali_rmse: 0.000000\n",
      "traning iteration: 859 ,loss: 1225.049002, vali_rmse: 0.000000\n",
      "traning iteration: 860 ,loss: 1224.626571, vali_rmse: 0.000000\n",
      "traning iteration: 861 ,loss: 1224.205081, vali_rmse: 0.000000\n",
      "traning iteration: 862 ,loss: 1223.784527, vali_rmse: 0.000000\n",
      "traning iteration: 863 ,loss: 1223.364908, vali_rmse: 0.000000\n",
      "traning iteration: 864 ,loss: 1222.946220, vali_rmse: 0.000000\n",
      "traning iteration: 865 ,loss: 1222.528459, vali_rmse: 0.000000\n",
      "traning iteration: 866 ,loss: 1222.111623, vali_rmse: 0.000000\n",
      "traning iteration: 867 ,loss: 1221.695708, vali_rmse: 0.000000\n",
      "traning iteration: 868 ,loss: 1221.280712, vali_rmse: 0.000000\n",
      "traning iteration: 869 ,loss: 1220.866631, vali_rmse: 0.000000\n",
      "traning iteration: 870 ,loss: 1220.453462, vali_rmse: 0.000000\n",
      "traning iteration: 871 ,loss: 1220.041203, vali_rmse: 0.000000\n",
      "traning iteration: 872 ,loss: 1219.629849, vali_rmse: 0.000000\n",
      "traning iteration: 873 ,loss: 1219.219399, vali_rmse: 0.000000\n",
      "traning iteration: 874 ,loss: 1218.809848, vali_rmse: 0.000000\n",
      "traning iteration: 875 ,loss: 1218.401195, vali_rmse: 0.000000\n",
      "traning iteration: 876 ,loss: 1217.993436, vali_rmse: 0.000000\n",
      "traning iteration: 877 ,loss: 1217.586568, vali_rmse: 0.000000\n",
      "traning iteration: 878 ,loss: 1217.180588, vali_rmse: 0.000000\n",
      "traning iteration: 879 ,loss: 1216.775493, vali_rmse: 0.000000\n",
      "traning iteration: 880 ,loss: 1216.371281, vali_rmse: 0.000000\n",
      "traning iteration: 881 ,loss: 1215.967948, vali_rmse: 0.000000\n",
      "traning iteration: 882 ,loss: 1215.565491, vali_rmse: 0.000000\n",
      "traning iteration: 883 ,loss: 1215.163908, vali_rmse: 0.000000\n",
      "traning iteration: 884 ,loss: 1214.763195, vali_rmse: 0.000000\n",
      "traning iteration: 885 ,loss: 1214.363351, vali_rmse: 0.000000\n",
      "traning iteration: 886 ,loss: 1213.964371, vali_rmse: 0.000000\n",
      "traning iteration: 887 ,loss: 1213.566253, vali_rmse: 0.000000\n",
      "traning iteration: 888 ,loss: 1213.168994, vali_rmse: 0.000000\n",
      "traning iteration: 889 ,loss: 1212.772592, vali_rmse: 0.000000\n",
      "traning iteration: 890 ,loss: 1212.377043, vali_rmse: 0.000000\n",
      "traning iteration: 891 ,loss: 1211.982345, vali_rmse: 0.000000\n",
      "traning iteration: 892 ,loss: 1211.588496, vali_rmse: 0.000000\n",
      "traning iteration: 893 ,loss: 1211.195491, vali_rmse: 0.000000\n",
      "traning iteration: 894 ,loss: 1210.803329, vali_rmse: 0.000000\n",
      "traning iteration: 895 ,loss: 1210.412006, vali_rmse: 0.000000\n",
      "traning iteration: 896 ,loss: 1210.021520, vali_rmse: 0.000000\n",
      "traning iteration: 897 ,loss: 1209.631869, vali_rmse: 0.000000\n",
      "traning iteration: 898 ,loss: 1209.243049, vali_rmse: 0.000000\n",
      "traning iteration: 899 ,loss: 1208.855057, vali_rmse: 0.000000\n",
      "traning iteration: 900 ,loss: 1208.467892, vali_rmse: 0.000000\n",
      "traning iteration: 901 ,loss: 1208.081550, vali_rmse: 0.000000\n",
      "traning iteration: 902 ,loss: 1207.696029, vali_rmse: 0.000000\n",
      "traning iteration: 903 ,loss: 1207.311326, vali_rmse: 0.000000\n",
      "traning iteration: 904 ,loss: 1206.927439, vali_rmse: 0.000000\n",
      "traning iteration: 905 ,loss: 1206.544364, vali_rmse: 0.000000\n",
      "traning iteration: 906 ,loss: 1206.162099, vali_rmse: 0.000000\n",
      "traning iteration: 907 ,loss: 1205.780642, vali_rmse: 0.000000\n",
      "traning iteration: 908 ,loss: 1205.399989, vali_rmse: 0.000000\n",
      "traning iteration: 909 ,loss: 1205.020139, vali_rmse: 0.000000\n",
      "traning iteration: 910 ,loss: 1204.641089, vali_rmse: 0.000000\n",
      "traning iteration: 911 ,loss: 1204.262836, vali_rmse: 0.000000\n",
      "traning iteration: 912 ,loss: 1203.885377, vali_rmse: 0.000000\n",
      "traning iteration: 913 ,loss: 1203.508710, vali_rmse: 0.000000\n",
      "traning iteration: 914 ,loss: 1203.132833, vali_rmse: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning iteration: 915 ,loss: 1202.757743, vali_rmse: 0.000000\n",
      "traning iteration: 916 ,loss: 1202.383437, vali_rmse: 0.000000\n",
      "traning iteration: 917 ,loss: 1202.009913, vali_rmse: 0.000000\n",
      "traning iteration: 918 ,loss: 1201.637169, vali_rmse: 0.000000\n",
      "traning iteration: 919 ,loss: 1201.265201, vali_rmse: 0.000000\n",
      "traning iteration: 920 ,loss: 1200.894008, vali_rmse: 0.000000\n",
      "traning iteration: 921 ,loss: 1200.523587, vali_rmse: 0.000000\n",
      "traning iteration: 922 ,loss: 1200.153935, vali_rmse: 0.000000\n",
      "traning iteration: 923 ,loss: 1199.785050, vali_rmse: 0.000000\n",
      "traning iteration: 924 ,loss: 1199.416930, vali_rmse: 0.000000\n",
      "traning iteration: 925 ,loss: 1199.049572, vali_rmse: 0.000000\n",
      "traning iteration: 926 ,loss: 1198.682974, vali_rmse: 0.000000\n",
      "traning iteration: 927 ,loss: 1198.317133, vali_rmse: 0.000000\n",
      "traning iteration: 928 ,loss: 1197.952047, vali_rmse: 0.000000\n",
      "traning iteration: 929 ,loss: 1197.587714, vali_rmse: 0.000000\n",
      "traning iteration: 930 ,loss: 1197.224131, vali_rmse: 0.000000\n",
      "traning iteration: 931 ,loss: 1196.861295, vali_rmse: 0.000000\n",
      "traning iteration: 932 ,loss: 1196.499206, vali_rmse: 0.000000\n",
      "traning iteration: 933 ,loss: 1196.137859, vali_rmse: 0.000000\n",
      "traning iteration: 934 ,loss: 1195.777253, vali_rmse: 0.000000\n",
      "traning iteration: 935 ,loss: 1195.417385, vali_rmse: 0.000000\n",
      "traning iteration: 936 ,loss: 1195.058253, vali_rmse: 0.000000\n",
      "traning iteration: 937 ,loss: 1194.699855, vali_rmse: 0.000000\n",
      "traning iteration: 938 ,loss: 1194.342189, vali_rmse: 0.000000\n",
      "traning iteration: 939 ,loss: 1193.985252, vali_rmse: 0.000000\n",
      "traning iteration: 940 ,loss: 1193.629041, vali_rmse: 0.000000\n",
      "traning iteration: 941 ,loss: 1193.273556, vali_rmse: 0.000000\n",
      "traning iteration: 942 ,loss: 1192.918792, vali_rmse: 0.000000\n",
      "traning iteration: 943 ,loss: 1192.564749, vali_rmse: 0.000000\n",
      "traning iteration: 944 ,loss: 1192.211423, vali_rmse: 0.000000\n",
      "traning iteration: 945 ,loss: 1191.858813, vali_rmse: 0.000000\n",
      "traning iteration: 946 ,loss: 1191.506917, vali_rmse: 0.000000\n",
      "traning iteration: 947 ,loss: 1191.155731, vali_rmse: 0.000000\n",
      "traning iteration: 948 ,loss: 1190.805255, vali_rmse: 0.000000\n",
      "traning iteration: 949 ,loss: 1190.455485, vali_rmse: 0.000000\n",
      "traning iteration: 950 ,loss: 1190.106420, vali_rmse: 0.000000\n",
      "traning iteration: 951 ,loss: 1189.758057, vali_rmse: 0.000000\n",
      "traning iteration: 952 ,loss: 1189.410394, vali_rmse: 0.000000\n",
      "traning iteration: 953 ,loss: 1189.063429, vali_rmse: 0.000000\n",
      "traning iteration: 954 ,loss: 1188.717160, vali_rmse: 0.000000\n",
      "traning iteration: 955 ,loss: 1188.371585, vali_rmse: 0.000000\n",
      "traning iteration: 956 ,loss: 1188.026701, vali_rmse: 0.000000\n",
      "traning iteration: 957 ,loss: 1187.682506, vali_rmse: 0.000000\n",
      "traning iteration: 958 ,loss: 1187.338999, vali_rmse: 0.000000\n",
      "traning iteration: 959 ,loss: 1186.996177, vali_rmse: 0.000000\n",
      "traning iteration: 960 ,loss: 1186.654038, vali_rmse: 0.000000\n",
      "traning iteration: 961 ,loss: 1186.312580, vali_rmse: 0.000000\n",
      "traning iteration: 962 ,loss: 1185.971800, vali_rmse: 0.000000\n",
      "traning iteration: 963 ,loss: 1185.631698, vali_rmse: 0.000000\n",
      "traning iteration: 964 ,loss: 1185.292270, vali_rmse: 0.000000\n",
      "traning iteration: 965 ,loss: 1184.953515, vali_rmse: 0.000000\n",
      "traning iteration: 966 ,loss: 1184.615430, vali_rmse: 0.000000\n",
      "traning iteration: 967 ,loss: 1184.278014, vali_rmse: 0.000000\n",
      "traning iteration: 968 ,loss: 1183.941264, vali_rmse: 0.000000\n",
      "traning iteration: 969 ,loss: 1183.605179, vali_rmse: 0.000000\n",
      "traning iteration: 970 ,loss: 1183.269756, vali_rmse: 0.000000\n",
      "traning iteration: 971 ,loss: 1182.934993, vali_rmse: 0.000000\n",
      "traning iteration: 972 ,loss: 1182.600889, vali_rmse: 0.000000\n",
      "traning iteration: 973 ,loss: 1182.267441, vali_rmse: 0.000000\n",
      "traning iteration: 974 ,loss: 1181.934647, vali_rmse: 0.000000\n",
      "traning iteration: 975 ,loss: 1181.602506, vali_rmse: 0.000000\n",
      "traning iteration: 976 ,loss: 1181.271015, vali_rmse: 0.000000\n",
      "traning iteration: 977 ,loss: 1180.940172, vali_rmse: 0.000000\n",
      "traning iteration: 978 ,loss: 1180.609976, vali_rmse: 0.000000\n",
      "traning iteration: 979 ,loss: 1180.280424, vali_rmse: 0.000000\n",
      "traning iteration: 980 ,loss: 1179.951515, vali_rmse: 0.000000\n",
      "traning iteration: 981 ,loss: 1179.623247, vali_rmse: 0.000000\n",
      "traning iteration: 982 ,loss: 1179.295617, vali_rmse: 0.000000\n",
      "traning iteration: 983 ,loss: 1178.968624, vali_rmse: 0.000000\n",
      "traning iteration: 984 ,loss: 1178.642265, vali_rmse: 0.000000\n",
      "traning iteration: 985 ,loss: 1178.316540, vali_rmse: 0.000000\n",
      "traning iteration: 986 ,loss: 1177.991445, vali_rmse: 0.000000\n",
      "traning iteration: 987 ,loss: 1177.666980, vali_rmse: 0.000000\n",
      "traning iteration: 988 ,loss: 1177.343141, vali_rmse: 0.000000\n",
      "traning iteration: 989 ,loss: 1177.019928, vali_rmse: 0.000000\n",
      "traning iteration: 990 ,loss: 1176.697339, vali_rmse: 0.000000\n",
      "traning iteration: 991 ,loss: 1176.375371, vali_rmse: 0.000000\n",
      "traning iteration: 992 ,loss: 1176.054022, vali_rmse: 0.000000\n",
      "traning iteration: 993 ,loss: 1175.733292, vali_rmse: 0.000000\n",
      "traning iteration: 994 ,loss: 1175.413177, vali_rmse: 0.000000\n",
      "traning iteration: 995 ,loss: 1175.093677, vali_rmse: 0.000000\n",
      "traning iteration: 996 ,loss: 1174.774789, vali_rmse: 0.000000\n",
      "traning iteration: 997 ,loss: 1174.456511, vali_rmse: 0.000000\n",
      "traning iteration: 998 ,loss: 1174.138842, vali_rmse: 0.000000\n",
      "traning iteration: 999 ,loss: 1173.821780, vali_rmse: 0.000000\n",
      "testing model.......\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'subtract' did not contain a loop with signature matching types dtype('<U32') dtype('<U32') dtype('<U32')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-b08ab588b195>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'testing model.......'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mtest_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'irt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test rmse:{:f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_rmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-8c3d14c92367>\u001b[0m in \u001b[0;36mRMSE\u001b[0;34m(preds, truth)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mRMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtruth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#1.Loading the dataset and Excluding the outliers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ufunc 'subtract' did not contain a loop with signature matching types dtype('<U32') dtype('<U32') dtype('<U32')"
     ]
    }
   ],
   "source": [
    "import surprise as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from scipy import stats\n",
    "\n",
    "#1.Loading the dataset and Excluding the outliers\n",
    "kbdata = pd.read_csv(\"/data/fjsdata/ctKngBase/kb.csv\", sep='|', low_memory=False)\n",
    "kbdata = kbdata.loc[(kbdata['num']<200)]#seven months, one per day\n",
    "print ('Dataset shape is:%d rows and %d columns'%(kbdata.shape[0],kbdata.shape[1]))\n",
    "\n",
    "#2.Calculating the mean of CSRs and KEs.\n",
    "Ab_csr = kbdata['num'].groupby(kbdata['csr']).mean()#the type of groupby is Series\n",
    "Di_ke = kbdata['num'].groupby(kbdata['ke']).mean()\n",
    "#3.Calculating the IRT of every pair<csr,ke> which is the pdf of norm\n",
    "#mu=Di_ke,sigma=Ab_csr\n",
    "#x[2]=num,x[1]=ke,x[0]=csr\n",
    "kbdata['irt'] = kbdata.apply(lambda x: \"{:.8f}\".format(stats.norm.pdf(int(x[2]), Di_ke[int(x[1])], Ab_csr[int(x[0])])),axis=1)\n",
    "data = kbdata[['csr', 'ke', 'irt']]\n",
    "# set split ratio\n",
    "ratio = 0.8\n",
    "train_data = data[:int(ratio*data.shape[0])]\n",
    "vali_data = data[int(ratio*data.shape[0]):int((ratio+(1-ratio)/2)*data.shape[0])]\n",
    "test_data = data[int((ratio+(1-ratio)/2)*data.shape[0]):]\n",
    "\n",
    "NUM_USERS = kbdata['csr'].max() + 1\n",
    "NUM_ITEMS = kbdata['ke'].max() + 1\n",
    "print('dataset density:{:f}'.format(len(data)*1.0/(NUM_USERS*NUM_ITEMS)))\n",
    "\n",
    "R = np.zeros([NUM_USERS, NUM_ITEMS])\n",
    "for index,ele in train_data.iterrows():\n",
    "    R[int(ele[0]), int(ele[1])] = float(ele[2])\n",
    "\n",
    "# construct model\n",
    "print('training model.......')\n",
    "lambda_alpha = 0.01\n",
    "lambda_beta = 0.01\n",
    "latent_size = 20\n",
    "lr = 3e-5\n",
    "iters = 1000\n",
    "model = PMF(R=R, lambda_alpha=lambda_alpha, lambda_beta=lambda_beta, latent_size=latent_size, momuntum=0.9, lr=lr, iters=iters, seed=1)\n",
    "print('parameters are:ratio={:f}, reg_u={:f}, reg_v={:f}, latent_size={:d}, lr={:f}, iters={:d}'.format(ratio, lambda_alpha, lambda_beta, latent_size,lr, iters))\n",
    "U, V, train_loss_list, vali_rmse_list = model.train(train_data=train_data, vali_data=vali_data)\n",
    "\n",
    "print('testing model.......')\n",
    "preds = model.predict(data=test_data)\n",
    "test_rmse = RMSE(preds, np.array(test_data['irt']).tolist())\n",
    "\n",
    "print('test rmse:{:f}'.format(test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
