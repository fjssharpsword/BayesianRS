{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape is:2547452 rows and 3 columns\n",
      "  K          Precisions             Recalls                NDCG\n",
      "  5          0.46939605          0.56137439          0.60653518\n",
      " 10          0.45245813          0.61192698          0.65193114\n",
      " 15          0.44844596          0.62378710          0.67542627\n",
      " 20          0.44742766          0.62568932          0.69187443\n"
     ]
    }
   ],
   "source": [
    "#Baseline:SVD，Surprise，testset only have positive sample.\n",
    "import surprise as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "#1.Loading the dataset and Excluding the outliers\n",
    "kbdata = pd.read_csv(\"/data/fjsdata/ctKngBase/kb.csv\", sep='|', low_memory=False)\n",
    "kbdata = kbdata.loc[(kbdata['num']<200)]#seven months, one per day\n",
    "num_max=kbdata['num'].max()\n",
    "num_min=kbdata['num'].min()\n",
    "kbdata['num']=kbdata['num'].apply(lambda x: (x-num_min+1)*1.0/(num_max-num_min+1) )\n",
    "print ('Dataset shape is:%d rows and %d columns'%(kbdata.shape[0],kbdata.shape[1]))\n",
    "\n",
    "#2.Transforming into data format of surprise and spliting the train-set and test-set\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "reader = sp.Reader(rating_scale=(0, 1))\n",
    "spdata = sp.Dataset.load_from_df(kbdata[['csr', 'ke', 'num']],reader)\n",
    "# sampling random trainset and testset, and test set is made of 10% of the ratings.\n",
    "#trainset, testset = sp.model_selection.train_test_split(spdata, test_size=.1)\n",
    "trainset = spdata.build_full_trainset()\n",
    "testset = trainset.build_testset()\n",
    "\n",
    "#3.Training the model and predicting ratings for the testset\n",
    "algo = sp.SVD()\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)#testset include positive and negtive sample.\n",
    "\n",
    "#4.measuring the performance of SVD by precision, recall and  NDCG\n",
    "#print ('RMSE of testset is:%.8f'%(sp.accuracy.rmse(predictions)))\n",
    "def calc_dcg(items):\n",
    "    dcg = 0\n",
    "    i = 0\n",
    "    for item in items:\n",
    "        i += 1\n",
    "        dcg += (math.pow(2, item) - 1)/ math.log(1 + i, 2)\n",
    "    return dcg\n",
    "def index_at_k(predictions, k, threshold=0.1):\n",
    "   #Return precision and recall at k metrics for each user.\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    ndcgs =dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r > threshold) for (_, true_r) in user_ratings)\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est > threshold) for (est, _) in user_ratings[:k])\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r > threshold) and (est > threshold)) for (est, true_r) in user_ratings[:k])\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "        #true ratings of recommended items in top k\n",
    "        l_rec_k = [true_r for (_,true_r) in user_ratings[:k]]\n",
    "        dcg = calc_dcg(l_rec_k)\n",
    "        #l_rec_k.sort(reverse=True)\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        l_rel_k = [true_r for (_,true_r) in user_ratings[:k]]\n",
    "        idcg = calc_dcg(l_rel_k)\n",
    "        ndcgs[uid]=dcg*1.0/idcg \n",
    "    return precisions, recalls, ndcgs\n",
    "\n",
    "print (\"%3s%20s%20s%20s\" % ('K','Precisions','Recalls','NDCG'))\n",
    "for k in [5,10,15,20]:#latent factor\n",
    "    precisions, recalls, ndcgs = index_at_k(predictions, k=k)\n",
    "    # Precision and recall can then be averaged over all users\n",
    "    precision = sum(prec for prec in precisions.values()) / len(precisions)\n",
    "    recall = sum(rec for rec in recalls.values()) / len(recalls)\n",
    "    ndcg = sum(ndcg for ndcg in ndcgs.values()) / len(ndcgs)\n",
    "    print (\"%3s%20.8f%20.8f%20.8f\" % (k, precision, recall, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape is:2547452 rows and 3 columns\n",
      "  K          Precisions             Recalls                NDCG\n",
      "  5          0.73666295          0.66078699          0.72512805\n",
      " 10          0.73650303          0.66144953          0.77198244\n",
      " 15          0.73644888          0.66151743          0.79402777\n",
      " 20          0.73642598          0.66153395          0.80740954\n"
     ]
    }
   ],
   "source": [
    "#Baseline:SVD，Surprise，testset only have positive sample.\n",
    "import surprise as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "#1.Loading the dataset and Excluding the outliers\n",
    "kbdata = pd.read_csv(\"/data/fjsdata/ctKngBase/kb.csv\", sep='|', low_memory=False)\n",
    "kbdata = kbdata.loc[(kbdata['num']<200)]#seven months, one per day\n",
    "num_max=kbdata['num'].max()\n",
    "num_min=kbdata['num'].min()\n",
    "kbdata['num']=kbdata['num'].apply(lambda x: (x-num_min+1)*1.0/(num_max-num_min+1) )\n",
    "print ('Dataset shape is:%d rows and %d columns'%(kbdata.shape[0],kbdata.shape[1]))\n",
    "\n",
    "#2.Transforming into data format of surprise and spliting the train-set and test-set\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "reader = sp.Reader(rating_scale=(0, 1))\n",
    "spdata = sp.Dataset.load_from_df(kbdata[['csr', 'ke', 'num']],reader)\n",
    "# sampling random trainset and testset, and test set is made of 10% of the ratings.\n",
    "trainset, testset = sp.model_selection.train_test_split(spdata, test_size=.1)\n",
    "\n",
    "#3.Training the model and predicting ratings for the testset\n",
    "algo = sp.SVD()\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)#testset include positive and negtive sample.\n",
    "# Than predict ratings for all pairs (u, i) that are NOT in the training set.\n",
    "'''\n",
    "predPos = algo.test(testset)\n",
    "testsetNeg = trainset.build_anti_testset(fill=0)    \n",
    "predNeg = algo.test(testsetNeg)\n",
    "predictions=predPos.extend(predNeg)#concanate  the pos and the neg\n",
    "'''\n",
    "\n",
    "#4.measuring the performance of SVD by precision, recall and  NDCG\n",
    "#print ('RMSE of testset is:%.8f'%(sp.accuracy.rmse(predictions)))\n",
    "def calc_dcg(items):\n",
    "    dcg = 0\n",
    "    i = 0\n",
    "    for item in items:\n",
    "        i += 1\n",
    "        dcg += (math.pow(2, item) - 1)/ math.log(1 + i, 2)\n",
    "    return dcg\n",
    "def index_at_k(predictions, k, threshold=0.1):\n",
    "   #Return precision and recall at k metrics for each user.\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    ndcgs =dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r > threshold) for (_, true_r) in user_ratings)\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est > threshold) for (est, _) in user_ratings[:k])\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r > threshold) and (est > threshold)) for (est, true_r) in user_ratings[:k])\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "        #true ratings of recommended items in top k\n",
    "        l_rec_k = [true_r for (_,true_r) in user_ratings[:k]]\n",
    "        dcg = calc_dcg(l_rec_k)\n",
    "        #l_rec_k.sort(reverse=True)\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        l_rel_k = [true_r for (_,true_r) in user_ratings[:k]]\n",
    "        idcg = calc_dcg(l_rel_k)\n",
    "        ndcgs[uid]=dcg*1.0/idcg \n",
    "    return precisions, recalls, ndcgs\n",
    "\n",
    "print (\"%3s%20s%20s%20s\" % ('K','Precisions','Recalls','NDCG'))\n",
    "for k in [5,10,15,20]:#latent factor\n",
    "    precisions, recalls, ndcgs = index_at_k(predictions, k=k)\n",
    "    # Precision and recall can then be averaged over all users\n",
    "    precision = sum(prec for prec in precisions.values()) / len(precisions)\n",
    "    recall = sum(rec for rec in recalls.values()) / len(recalls)\n",
    "    ndcg = sum(ndcg for ndcg in ndcgs.values()) / len(ndcgs)\n",
    "    print (\"%3s%20.8f%20.8f%20.8f\" % (k, precision, recall, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset shape is:2292706 rows and 3 columns\n",
      "Testset shape is:254746 rows and 3 columns\n",
      "  K          Precisions             Recalls                NDCG\n",
      "  5          0.59434487          1.00000000          1.02955465\n",
      " 10          0.59434487          1.00000000          1.01712075\n",
      " 15          0.59434487          1.00000000          1.01254969\n",
      " 20          0.59434487          1.00000000          1.01004584\n"
     ]
    }
   ],
   "source": [
    "#Baseline:SVD，Surprise，trainset and testset are not splitted samply.\n",
    "import surprise as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "#1.Loading the dataset\n",
    "trainset = pd.read_csv(\"/data/fjsdata/ctKngBase/trainset.csv\", sep='|', low_memory=False) \n",
    "testset = pd.read_csv(\"/data/fjsdata/ctKngBase/testset.csv\", sep='|', low_memory=False)\n",
    "num_max=max(trainset['num'].max(),testset['num'].max())\n",
    "num_min=min(trainset['num'].min(),testset['num'].min())\n",
    "trainset['num']=trainset['num'].apply(lambda x: (x-num_min+1)*1.0/(num_max-num_min+1) )\n",
    "print ('Trainset shape is:%d rows and %d columns'%(trainset.shape[0],trainset.shape[1]))\n",
    "testset['num']=testset['num'].apply(lambda x: (x-num_min+1)*1.0/(num_max-num_min+1) )\n",
    "print ('Testset shape is:%d rows and %d columns'%(testset.shape[0],testset.shape[1]))\n",
    "\n",
    "#2.Transforming into data format of surprise and spliting the train-set and test-set\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "reader = sp.Reader(rating_scale=(0, 1))\n",
    "trainset = sp.Dataset.load_from_df(trainset[['csr', 'ke', 'num']],reader)\n",
    "trainset = trainset.build_full_trainset()\n",
    "#testset = sp.Dataset.load_from_df(testset[['csr', 'ke', 'num']],reader)\n",
    "testset = np.array(testset).tolist()\n",
    "#3.Training the model and predicting ratings for the testset\n",
    "algo = sp.SVD()\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)#testset include positive and negtive sample.\n",
    "\n",
    "#4.measuring the performance of SVD by precision, recall and  NDCG\n",
    "#print ('RMSE of testset is:%.8f'%(sp.accuracy.rmse(predictions)))\n",
    "def calc_dcg(items):\n",
    "    dcg = 0\n",
    "    i = 0\n",
    "    for item in items:\n",
    "        i += 1\n",
    "        dcg += (math.pow(2, item) - 1)/ math.log(1 + i, 2)\n",
    "    return dcg\n",
    "def index_at_k(predictions, k, threshold=0.1):\n",
    "   #Return precision and recall at k metrics for each user.\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    ndcgs =dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r > threshold) for (_, true_r) in user_ratings)\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est > threshold) for (est, _) in user_ratings[:k])\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r > threshold) and (est > threshold)) for (est, true_r) in user_ratings[:k])\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "        #true ratings of recommended items in top k\n",
    "        l_rec_k = [true_r for (_,true_r) in user_ratings[:k]]\n",
    "        dcg = calc_dcg(l_rec_k)\n",
    "        #l_rec_k.sort(reverse=True)\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        l_rel_k = [true_r for (_,true_r) in user_ratings[:k]]\n",
    "        idcg = calc_dcg(l_rel_k)\n",
    "        ndcgs[uid]=dcg*1.0/idcg \n",
    "    return precisions, recalls, ndcgs\n",
    "\n",
    "print (\"%3s%20s%20s%20s\" % ('K','Precisions','Recalls','NDCG'))\n",
    "for k in [5,10,15,20]:#latent factor\n",
    "    precisions, recalls, ndcgs = index_at_k(predictions, k=k)\n",
    "    # Precision and recall can then be averaged over all users\n",
    "    precision = sum(prec for prec in precisions.values()) / len(precisions)\n",
    "    recall = sum(rec for rec in recalls.values()) / len(recalls)\n",
    "    ndcg = sum(ndcg for ndcg in ndcgs.values()) / len(ndcgs)\n",
    "    print (\"%3s%20.8f%20.8f%20.8f\" % (k, precision, recall, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape is:2547452 rows and 3 columns\n",
      "254746\n",
      "0.7348169845161151\n",
      "0.6729520766740884\n"
     ]
    }
   ],
   "source": [
    "#Baseline:SVD，Surprise\n",
    "#Measures: Hit Ratio and NDCG\n",
    "import surprise as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "#1.Loading the dataset and Excluding the outliers\n",
    "kbdata = pd.read_csv(\"/data/fjsdata/ctKngBase/kb.csv\", sep='|', low_memory=False)\n",
    "kbdata = kbdata.loc[(kbdata['num']<200)]#7个月，一天一次估算\n",
    "num_max=kbdata['num'].max()\n",
    "num_min=kbdata['num'].min()\n",
    "kbdata['num']=kbdata['num'].apply(lambda x: (x-num_min+1)*1.0/(num_max-num_min+1) )\n",
    "print ('Dataset shape is:%d rows and %d columns'%(kbdata.shape[0],kbdata.shape[1]))\n",
    "\n",
    "#2.Transforming into data format of surprise and spliting the train-set and test-set\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "reader = sp.Reader(rating_scale=(0, 1))\n",
    "spdata = sp.Dataset.load_from_df(kbdata[['csr', 'ke', 'num']],reader)\n",
    "# sampling random trainset and testset, and test set is made of 10% of the ratings.\n",
    "trainset, testset = sp.model_selection.train_test_split(spdata, test_size=.1)\n",
    "\n",
    "#3.Training the model and predicting ratings for the testset\n",
    "algo = SVD()\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "#4.measuring the performance by precision and recalls\n",
    "def precision_recall_at_k(predictions, k=10, threshold=0.1):\n",
    "   #Return precision and recall at k metrics for each user.\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold)) for (est, true_r) in user_ratings[:k])\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "    return precisions, recalls\n",
    "precisions, recalls = precision_recall_at_k(predictions, k=10, threshold=0.1)\n",
    "\n",
    "# Precision and recall can then be averaged over all users\n",
    "print(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "print(sum(rec for rec in recalls.values()) / len(recalls))\n",
    "\n",
    "#5.measuring the performance by NDCG and RMSE\n",
    "RMSE = sp.accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape is:2547452 rows and 3 columns\n",
      "RMSE: 0.0573\n",
      "0.057304167884117255\n"
     ]
    }
   ],
   "source": [
    "#Baseline:SVD，Surprise\n",
    "#Measures: Hit Ratio and NDCG\n",
    "import surprise as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#1.Loading the dataset and Excluding the outliers\n",
    "kbdata = pd.read_csv(\"/data/fjsdata/ctKngBase/kb.csv\", sep='|', low_memory=False)\n",
    "kbdata = kbdata.loc[(kbdata['num']<200)]#7个月，一天一次估算\n",
    "num_max=kbdata['num'].max()\n",
    "num_min=kbdata['num'].min()\n",
    "kbdata['num']=kbdata['num'].apply(lambda x: (x-num_min+1)*1.0/(num_max-num_min+1) )\n",
    "print ('Dataset shape is:%d rows and %d columns'%(kbdata.shape[0],kbdata.shape[1]))\n",
    "\n",
    "#2.Transforming into data format of surprise and spliting the train-set and test-set\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "reader = sp.Reader(rating_scale=(0, 1))\n",
    "spdata = sp.Dataset.load_from_df(kbdata[['csr', 'ke', 'num']],reader)\n",
    "# sampling random trainset and testset, and test set is made of 10% of the ratings.\n",
    "trainset, testset = sp.model_selection.train_test_split(spdata, test_size=.1)\n",
    "\n",
    "#3.Training the model\n",
    "algo = SVD()\n",
    "algo.fit(trainset)\n",
    "\n",
    "#4.predicting ratings for the testset\n",
    "predictions = algo.test(testset)\n",
    "print (sp.accuracy.rmse(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape is:2547452 rows and 3 columns\n",
      "-0.9849246231155779\n",
      "RMSE: 0.9981\n",
      "0.998147312473551\n"
     ]
    }
   ],
   "source": [
    "#SVD，Surprise\n",
    "import surprise as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#1.Loading the dataset and Excluding the outliers\n",
    "kbdata = pd.read_csv(\"/data/fjsdata/ctKngBase/kb.csv\", sep='|', low_memory=False)\n",
    "kbdata = kbdata.loc[(kbdata['num']<200)]#7个月，一天一次估算\n",
    "num_max=kbdata['num'].max()\n",
    "num_min=kbdata['num'].min()\n",
    "kbdata['num']=kbdata['num'].apply(lambda x: (x-num_min+1)*1.0/(num_max-num_min+1) )\n",
    "print ('Dataset shape is:%d rows and %d columns'%(kbdata.shape[0],kbdata.shape[1]))\n",
    "testset = kbdata.sample(frac=0.1)\n",
    "#2.Transforming into data format of surprise and spliting the train-set and test-set\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "reader = sp.Reader(rating_scale=(0, 1))\n",
    "spdata = sp.Dataset.load_from_df(kbdata[['csr', 'ke', 'num']],reader)\n",
    "trainset = spdata.build_full_trainset()\n",
    "testset = np.array(testset).tolist()\n",
    "# sampling random trainset and testset, and test set is made of 10% of the ratings.\n",
    "#trainset, testset = sp.model_selection.train_test_split(spdata, test_size=.1)\n",
    "#print(type(testset))\n",
    "\n",
    "#3.Training the model\n",
    "algo = sp.SVD()\n",
    "algo.fit(trainset)\n",
    "\n",
    "#4.predicting ratings for the testset\n",
    "predictions = algo.test(testset)\n",
    "for uid, iid, true_r, est, _ in predictions:\n",
    "    print (true_r)\n",
    "    break;\n",
    "print (sp.accuracy.rmse(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  K               steps                RMSE\n",
      " 50                   2          448.907549\n"
     ]
    }
   ],
   "source": [
    "#Baseline:SVD\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#@INPUT:\n",
    "# R     : a matrix to be factorized, dimension N x M\n",
    "# P     : an initial matrix of dimension N x K\n",
    "# Q     : an initial matrix of dimension M x K\n",
    "# K     : the number of latent features\n",
    "# steps : the maximum number of steps to perform the optimisation\n",
    "# alpha : the learning rate\n",
    "#  beta  : the regularization parameter\n",
    "# @OUTPUT:the final matrices P and Q\n",
    "def matrix_factorization(R, P, Q, K, steps, alpha=0.0002, beta=0.002):\n",
    "    Q = Q.T\n",
    "    for step in range(steps):\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i][j] > 0:\n",
    "                    eij = R[i][j] - np.dot(P[i,:],Q[:,j])\n",
    "                    for k in range(K):\n",
    "                        P[i][k] = P[i][k] + alpha * (2 * eij * Q[k][j] - beta * P[i][k])\n",
    "                        Q[k][j] = Q[k][j] + alpha * (2 * eij * P[i][k] - beta * Q[k][j])\n",
    "        eR = np.dot(P,Q)\n",
    "        e = 0\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i][j] > 0:\n",
    "                    e = e + pow(R[i][j] - np.dot(P[i,:],Q[:,j]), 2)\n",
    "                    for k in range(K):\n",
    "                        e = e + (beta/2) * ( pow(P[i][k],2) + pow(Q[k][j],2) )\n",
    "        if e < 0.001:\n",
    "            break\n",
    "    return P, Q.T\n",
    "#1.加载训练数据并标准化\n",
    "kbdata = traindata = pd.read_csv(\"/data/fjsdata/ctKngBase/kb.csv\", sep='|', low_memory=False)\n",
    "le = LabelEncoder()\n",
    "kbdata = kbdata.apply(le.fit_transform)\n",
    "test = kbdata.sample(frac=0.1)#抽样10%比例测试\n",
    "#2.构建系数矩阵R\n",
    "nCsr = len(kbdata['csr'].unique()) #用户数\n",
    "nKe = len(kbdata['ke'].unique()) #知识点数\n",
    "R = np.zeros((nCsr, nKe))#初始化零矩阵为nCSR*nKe大小\n",
    "for index, row in kbdata.iterrows(): # 获取每行的值\n",
    "    R[int(row['csr'])][int(row['ke'])] = 1.0 #正样本\n",
    "#3.SVD训练和评估\n",
    "print (\"%3s%20s%20s\" % ('K','steps','RMSE'))\n",
    "for K in [50,100,200]:#隐因子\n",
    "    for steps in [2,500,1000]:#迭代次数\n",
    "        P = np.random.rand(nCsr,K)\n",
    "        Q = np.random.rand(nKe,K)\n",
    "        nP, nQ = matrix_factorization(R, P, Q, K,steps)#SVD分解\n",
    "        nR = np.dot(nP, nQ.T) #近似矩阵  \n",
    "        #RMSE评估   \n",
    "        Y_test = []\n",
    "        for index, row in test.iterrows(): # 获取每行的值\n",
    "            nRating=nR[int(row['csr'])][int(row['ke'])] #获取预测值\n",
    "            Y_test.append(nRating)\n",
    "        RMSE = mean_squared_error(test['num'],Y_test)\n",
    "        print (\"%3d%20d%20.6f\" % (K,steps,RMSE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
